# Generated by Django 5.2.1 on 2025-05-13 17:37

from datetime import datetime, timedelta
from typing import Any
import uuid
from collections.abc import Sequence
from itertools import chain
from dataclasses import dataclass, field
from typing import TypedDict

from django.conf import settings
from django.db import migrations
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps

from sentry.new_migrations.migrations import CheckedMigration
from sentry.utils import json, redis
from sentry.utils.query import RangeQuerySetWrapper

MAX_ERRORS_PER_SET = 10
MONITOR_ERRORS_LIFETIME = timedelta(days=7)


@dataclass
class CheckinItem:
    """
    Represents a check-in to be processed
    """

    ts: datetime
    """
    The timestamp the check-in was produced into the kafka topic. This differs
    from the start_time that is part of the CheckIn
    """

    partition: int
    """
    The kafka partition id the check-in was produced into.
    """

    message: Any
    """
    The original unpacked check-in message contents.
    """

    payload: Any
    """
    The json-decoded check-in payload contained within the message. Includes
    the full check-in details.
    """

    @property
    def processing_key(self):
        """
        This key is used to uniquely identify the check-in group this check-in
        belongs to. Check-ins grouped together will never be processed in
        parallel with other check-ins belonging to the same group
        """
        project_id = self.message["project_id"]
        env = self.payload.get("environment")
        return f"{project_id}:{self.valid_monitor_slug}:{env}"

    def to_dict(self) -> Any:
        return {
            "id": self.id.hex,
            "checkin": self.checkin.to_dict(),
            "errors": self.errors,
        }

    @classmethod
    def from_dict(cls, data: Any) -> "CheckinItem":
        return cls(
            datetime.fromisoformat(data["ts"]),
            data["partition"],
            data["message"],
            data["payload"],
        )


class CheckinProcessingErrorData(TypedDict):
    id: str
    checkin: Any
    errors: Sequence[Any]


@dataclass(frozen=True)
class CheckinProcessingError:
    errors: Sequence[Any]
    checkin: CheckinItem
    id: uuid.UUID = field(default_factory=uuid.uuid4)

    @classmethod
    def from_dict(cls, data: CheckinProcessingErrorData) -> "CheckinProcessingError":
        return cls(
            id=uuid.UUID(data["id"]),
            checkin=CheckinItem.from_dict(data["checkin"]),
            errors=data["errors"],
        )


def _get_cluster() -> Any:
    return redis.redis_clusters.get(settings.SENTRY_MONITORS_REDIS_CLUSTER)


def build_monitor_identifier(monitor: Any) -> str:
    return f"monitor:{monitor.id}"


def build_error_identifier(uuid: uuid.UUID) -> str:
    return f"monitors.processing_errors.{uuid.hex}"


def build_project_identifier(project_id: int) -> str:
    return f"project:{project_id}"


def build_set_identifier(entity_identifier: str) -> str:
    return f"monitors.processing_errors_set.{entity_identifier}"


def fetch_processing_errors(entity_identifier: str) -> dict[uuid.UUID, Any]:
    redis = _get_cluster()
    pipeline = redis.pipeline()
    pipeline.zrange(build_set_identifier(entity_identifier, 0, MAX_ERRORS_PER_SET, desc=True))
    processing_errors = {}
    error_identifiers = [
        build_error_identifier(uuid.UUID(error_identifier))
        for error_identifier in chain(*pipeline.execute())
    ]
    for error_identifier in error_identifiers:
        raw_error = redis.mget(error_identifier)
        if raw_error is not None:
            processing_errors[error_identifier] = CheckinProcessingError.from_dict(
                json.loads(raw_error)
            )
    return processing_errors


def fix_processing_error_keys(apps: StateApps, schema_editor: BaseDatabaseSchemaEditor) -> None:
    Monitor = apps.get_model("monitors", "Monitor")
    Project = apps.get_model("sentry", "Project")

    redis = _get_cluster()
    pipeline = redis.pipeline()

    # step 1: fix all monitors
    breakpoint()
    print("got here")
    for monitor in RangeQuerySetWrapper(Monitor.objects.all()):
        monitor_identifier = build_monitor_identifier(monitor)
        processing_errors = fetch_processing_errors(monitor_identifier)
        for processing_error, error_id in zip(processing_errors):
            if processing_error.id != error_id:
                processing_error.id = error_id
                error_key = build_error_identifier(error_id)
                pipeline.set(
                    error_key, json.dumps(processing_error.to_dict()), ex=MONITOR_ERRORS_LIFETIME
                )
        pipeline.execute()

    # step 2: fix all projects
    for project in RangeQuerySetWrapper(Project.objects.all()):
        project_identifier = build_project_identifier(project.id)
        processing_errors = fetch_processing_errors(project_identifier)
        for processing_error, error_id in zip(processing_errors):
            if processing_error.id != error_id:
                processing_error.id = error_id
                error_key = build_error_identifier(error_id)
                pipeline.set(
                    error_key, json.dumps(processing_error.to_dict()), ex=MONITOR_ERRORS_LIFETIME
                )
        pipeline.execute()


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production.
    # This should only be used for operations where it's safe to run the migration after your
    # code has deployed. So this should not be used for most operations that alter the schema
    # of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually so that they can be
    #   monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   run this outside deployments so that we don't block them. Note that while adding an index
    #   is a schema change, it's completely safe to run the operation after the code has deployed.
    # Once deployed, run these manually via: https://develop.sentry.dev/database-migrations/#migration-deployment

    is_post_deployment = True

    dependencies = [
        ("monitors", "0002_fix_drift_default_to_db_default"),
    ]

    operations = [
        migrations.RunPython(
            fix_processing_error_keys,
            migrations.RunPython.noop,
            hints={"tables": ["sentry_monitor"]},
        )
    ]
