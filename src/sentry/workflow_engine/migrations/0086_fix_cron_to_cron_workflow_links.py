# Generated by Django 5.2.1 on 2025-09-19 17:27
import json  # noqa: S003
import logging
from collections import defaultdict
from copy import deepcopy

from django.db import migrations
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps

from sentry.new_migrations.migrations import CheckedMigration

logger = logging.getLogger(__name__)


def fix_cron_to_cron_workflow_links(
    apps: StateApps, schema_editor: BaseDatabaseSchemaEditor
) -> None:
    """
    Fix the over-linking from migration 0085 where every cron detector was linked
    to every workflow in a project. This migration:

    1. For monitors with alert_rule_id: Keep ONLY the link to their deduped workflow
    2. For monitors without alert_rule_id: Remove ALL workflow links
    3. Issue workflows will be re-linked in the next migration
    """
    Monitor = apps.get_model("monitors", "Monitor")
    Rule = apps.get_model("sentry", "Rule")
    AlertRuleWorkflow = apps.get_model("workflow_engine", "AlertRuleWorkflow")
    DataSourceDetector = apps.get_model("workflow_engine", "DataSourceDetector")
    DetectorWorkflow = apps.get_model("workflow_engine", "DetectorWorkflow")

    rules_by_org = defaultdict(list)
    for rule in Rule.objects.filter(source=1):
        rules_by_org[rule.project.organization_id].append(rule)

    for organization_id, rules in rules_by_org.items():
        links = AlertRuleWorkflow.objects.filter(rule_id__in=[r.id for r in rules]).select_related(
            "workflow"
        )
        rule_workflows = {link.rule_id: link.workflow for link in links}

        # Process ALL rules, not just those with workflows
        # Rules without workflows need to be mapped to the primary workflow of their hash group
        if not rules:
            continue

        rule_hashes = defaultdict(list)
        for rule in rules:
            data = deepcopy(rule.data)
            for action in data.get("actions", []):
                action.pop("uuid", None)
            new_conditions = []
            for condition in data.get("conditions", []):
                if (
                    condition.get("id") == "sentry.rules.filters.tagged_event.TaggedEventFilter"
                    and condition.get("key") == "monitor.slug"
                ):
                    continue
                new_conditions.append(condition)
            data["conditions"] = new_conditions
            data["environment_id"] = rule.environment_id
            data["owner_user_id"] = rule.owner_user_id
            data["owner_team_id"] = rule.owner_team_id
            rule_hashes[json.dumps(data, sort_keys=True)].append(rule.id)

        hash_to_workflow = {}
        for rule_hash, rule_ids in rule_hashes.items():
            for rule_id in rule_ids:
                if rule_id in rule_workflows:
                    hash_to_workflow[rule_hash] = rule_workflows[rule_id]
                    break
            else:
                logger.error(
                    "fix_cron_to_cron_workflow_links.no_workflow_for_rule_group",
                    extra={
                        "organization_id": organization_id,
                        "rule_ids": rule_ids,
                        "rule_count": len(rule_ids),
                    },
                )

        rule_to_primary_workflow = {}
        for rule_hash, rule_ids in rule_hashes.items():
            if rule_hash in hash_to_workflow:
                workflow = hash_to_workflow[rule_hash]
                for rule_id in rule_ids:
                    rule_to_primary_workflow[rule_id] = workflow.id
            else:
                logger.error(
                    "fix_cron_to_cron_workflow_links.no_workflow_mapping",
                    extra={
                        "organization_id": organization_id,
                        "rule_ids": rule_ids,
                        "rule_count": len(rule_ids),
                    },
                )

        data_source_links = DataSourceDetector.objects.filter(
            data_source__organization_id=organization_id,
            data_source__type="cron_monitor",
        ).select_related("data_source", "detector")

        monitor_id_to_detector = {
            int(dsl.data_source.source_id): dsl.detector for dsl in data_source_links
        }

        monitors = list(Monitor.objects.filter(organization_id=organization_id))
        for monitor in monitors:
            if monitor.id not in monitor_id_to_detector:
                logger.error(
                    "fix_cron_to_cron_workflow_links.monitor_missing_detector",
                    extra={
                        "organization_id": organization_id,
                        "monitor_id": monitor.id,
                    },
                )
                continue

            detector = monitor_id_to_detector[monitor.id]

            if "alert_rule_id" not in monitor.config:
                DetectorWorkflow.objects.filter(detector=detector).delete()
                continue

            alert_rule_id = int(monitor.config["alert_rule_id"])

            if alert_rule_id in rule_to_primary_workflow:
                correct_workflow_id = rule_to_primary_workflow[alert_rule_id]
                DetectorWorkflow.objects.filter(detector=detector).exclude(
                    workflow_id=correct_workflow_id
                ).delete()
            else:
                logger.error(
                    "fix_cron_to_cron_workflow_links.monitor_rule_missing_workflow",
                    extra={
                        "organization_id": organization_id,
                        "monitor_id": monitor.id,
                        "alert_rule_id": alert_rule_id,
                    },
                )


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production.
    # This should only be used for operations where it's safe to run the migration after your
    # code has deployed. So this should not be used for most operations that alter the schema
    # of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually so that they can be
    #   monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   run this outside deployments so that we don't block them. Note that while adding an index
    #   is a schema change, it's completely safe to run the operation after the code has deployed.
    # Once deployed, run these manually via: https://develop.sentry.dev/database-migrations/#migration-deployment

    is_post_deployment = True

    dependencies = [
        ("workflow_engine", "0085_crons_link_detectors_to_all_workflows"),
    ]

    operations = [
        migrations.RunPython(
            fix_cron_to_cron_workflow_links,
            migrations.RunPython.noop,
            hints={"tables": ["workflow_engine_detectorworkflow"]},
        ),
    ]
