# Generated by Django 5.2.1 on 2025-08-26 17:43
import json  # noqa: S003
from collections import defaultdict
from copy import deepcopy
from typing import Any

from django.db import migrations, router, transaction
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps

from sentry.new_migrations.migrations import CheckedMigration


def dedupe_cron_shadow_workflows(apps: StateApps, schema_editor: BaseDatabaseSchemaEditor) -> None:
    Monitor = apps.get_model("monitors", "Monitor")
    Rule = apps.get_model("sentry", "Rule")
    AlertRuleWorkflow = apps.get_model("workflow_engine", "AlertRuleWorkflow")
    DataSourceDetector = apps.get_model("workflow_engine", "DataSourceDetector")
    DetectorWorkflow = apps.get_model("workflow_engine", "DetectorWorkflow")
    rules_by_org: defaultdict[int, list[Any]] = defaultdict(list)
    for rule in Rule.objects.filter(source=1):
        rules_by_org[rule.project.organization_id].append(rule)

    for organization_id, rules in rules_by_org.items():
        links = AlertRuleWorkflow.objects.filter(rule_id__in=[r.id for r in rules]).select_related(
            "workflow"
        )
        rule_workflows = {link.rule_id: link.workflow for link in links}
        # We filter out rules with no links here - if we re-run the script, we don't need to reprocess rules that
        # have already been de-duped
        rules = [r for r in rules if r.id in rule_workflows]

        monitors = Monitor.objects.filter(organization_id=organization_id)
        rule_monitors = {
            int(m.config["alert_rule_id"]): m for m in monitors if "alert_rule_id" in m.config
        }
        data_source_links = DataSourceDetector.objects.filter(
            data_source__organization_id=organization_id,
            data_source__type="cron_monitor",
        ).select_related("data_source", "detector")
        monitor_id_to_detector = {
            int(dsl.data_source.source_id): dsl.detector for dsl in data_source_links
        }
        rule_hashes: defaultdict[str, list[int]] = defaultdict(list)
        for rule in rules:
            data = deepcopy(rule.data)
            # Clean up the data so that it's easy to compare
            for action in data.get("actions", []):
                action.pop("uuid", None)
            new_conditions = []
            for condition in data.get("conditions", []):
                # Filter out the condition that we use to restrict this to specific cron monitors, so that we can
                # compare for identical rules across the whole org
                if (
                    condition.get("id") == "sentry.rules.filters.tagged_event.TaggedEventFilter"
                    and condition.get("key") == "monitor.slug"
                ):
                    continue
                new_conditions.append(condition)
            data["conditions"] = new_conditions
            data["environment_id"] = rule.environment_id
            data["owner_user_id"] = rule.owner_user_id
            data["owner_team_id"] = rule.owner_team_id
            rule_hashes[json.dumps(data, sort_keys=True)].append(rule.id)

        for rule_ids in rule_hashes.values():
            with transaction.atomic(router.db_for_write(Rule)):
                primary_workflow = rule_workflows[rule_ids[0]]
                rule_ids_to_remove = rule_ids[1:]
                for rule_id_to_remove in rule_ids_to_remove:
                    if rule_id_to_remove in rule_workflows:
                        rule_workflows[rule_id_to_remove].delete()

                for rule_id in rule_ids:
                    if rule_id in rule_monitors:
                        detector = monitor_id_to_detector[rule_monitors[rule_id].id]
                        DetectorWorkflow.objects.get_or_create(
                            detector=detector, workflow=primary_workflow
                        )


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production.
    # This should only be used for operations where it's safe to run the migration after your
    # code has deployed. So this should not be used for most operations that alter the schema
    # of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually so that they can be
    #   monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   run this outside deployments so that we don't block them. Note that while adding an index
    #   is a schema change, it's completely safe to run the operation after the code has deployed.
    # Once deployed, run these manually via: https://develop.sentry.dev/database-migrations/#migration-deployment

    is_post_deployment = True

    dependencies = [
        ("workflow_engine", "0083_add_status_to_action"),
        ("monitors", "0009_backfill_monitor_detectors"),
    ]

    operations = [
        migrations.RunPython(
            dedupe_cron_shadow_workflows,
            migrations.RunPython.noop,
            hints={"tables": ["sentry_rule"]},
        ),
    ]
