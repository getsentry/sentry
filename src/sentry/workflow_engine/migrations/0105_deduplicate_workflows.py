# Generated by Django 5.2.8 on 2025-12-15 18:37

from typing import Any

from django.db import migrations
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps
from django.db.models import Count, Prefetch

from sentry.new_migrations.migrations import CheckedMigration
from sentry.utils import json
from sentry.utils.query import RangeQuerySetWrapper


class WorkflowData:
    workflow: Any  # Can't be strictly typed because it's a migration

    def __init__(self, workflow: Any):
        self.workflow = workflow

    def serialize(self) -> str:
        # Extract triggers
        trigger_conditions = []
        if self.workflow.when_condition_group and hasattr(
            self.workflow.when_condition_group, "prefetched_trigger_conditions"
        ):
            for condition in self.workflow.when_condition_group.prefetched_trigger_conditions:
                trigger_conditions.append(
                    {
                        "type": condition.type,
                        "comparison": condition.comparison,
                        "result": condition.condition_result,
                    }
                )

        # Extract action groups
        action_groups = []
        if hasattr(self.workflow, "prefetched_action_groups"):
            for wdcg in self.workflow.prefetched_action_groups:
                group_conditions = []
                for condition in wdcg.condition_group.prefetched_conditions:
                    group_conditions.append(
                        {
                            "type": condition.type,
                            "comparison": condition.comparison,
                            "result": condition.condition_result,
                        }
                    )

                group_actions = []
                for action_data in wdcg.condition_group.prefetched_actions:
                    group_actions.append(
                        {
                            "type": action_data.action.type,
                            "config": action_data.action.config,
                        }
                    )

                # Sort conditions and actions for consistent hashing
                group_conditions.sort(
                    key=lambda c: (c["type"], json.dumps(c["comparison"], sort_keys=True))
                )
                group_actions.sort(
                    key=lambda a: (a["type"], json.dumps(a["config"], sort_keys=True))
                )

                action_groups.append(
                    {
                        "conditions": group_conditions,
                        "actions": group_actions,
                    }
                )

        # Sort trigger conditions and action groups for consistent hashing
        trigger_conditions.sort(
            key=lambda c: (c["type"], json.dumps(c["comparison"], sort_keys=True))
        )
        action_groups.sort(key=lambda g: json.dumps(g, sort_keys=True))

        workflow_data = {
            "organization_id": self.workflow.organization_id,
            "environment_id": self.workflow.environment_id,
            "trigger_conditions": trigger_conditions,
            "action_groups": action_groups,
        }

        return json.dumps(workflow_data, sort_keys=True)


def deduplicate_workflows(app: StateApps, schema_editor: BaseDatabaseSchemaEditor) -> None:
    Organization = app.get_model("sentry", "Organization")

    AlertRuleWorkflow = app.get_model("workflow_engine", "AlertRuleWorkflow")
    DataCondition = app.get_model("workflow_engine", "DataCondition")
    DataConditionGroupAction = app.get_model("workflow_engine", "DataConditionGroupAction")
    DetectorWorkflow = app.get_model("workflow_engine", "DetectorWorkflow")
    Workflow = app.get_model("workflow_engine", "Workflow")
    WorkflowDataConditionGroup = app.get_model("workflow_engine", "WorkflowDataConditionGroup")

    # Filters out ~65% of orgs by only selecting orgs with more than 1 workflow
    organizations = Organization.objects.annotate(workflow_count=Count("workflow")).filter(
        workflow_count__gt=1
    )

    for org in RangeQuerySetWrapper(organizations):
        workflows = (
            Workflow.objects.filter(organization=org)
            .select_related("when_condition_group")
            .prefetch_related(
                Prefetch(
                    "when_condition_group__conditions",
                    queryset=DataCondition.objects.all(),
                    to_attr="prefetched_trigger_conditions",
                ),
                Prefetch(
                    "workflowdataconditiongroup_set",
                    queryset=WorkflowDataConditionGroup.objects.select_related(
                        "condition_group"
                    ).prefetch_related(
                        Prefetch(
                            "condition_group__conditions",
                            queryset=DataCondition.objects.all(),
                            to_attr="prefetched_conditions",
                        ),
                        Prefetch(
                            "condition_group__dataconditiongroupaction_set",
                            queryset=DataConditionGroupAction.objects.select_related("action"),
                            to_attr="prefetched_actions",
                        ),
                    ),
                    to_attr="prefetched_action_groups",
                ),
                # Used to update alert rule <> workflow connections in legacy models
                Prefetch(
                    "alertruleworkflow_set",
                    queryset=AlertRuleWorkflow.objects.all(),
                    to_attr="prefetched_rule_workflows",
                ),
            )
            .distinct()
        )

        workflow_dedupe_hash: dict[str, list[int]] = {}
        for workflow in workflows:
            workflow_data = WorkflowData(workflow)
            workflow_hash = workflow_data.serialize()

            if workflow_hash in workflow_dedupe_hash:
                workflow_dedupe_hash[workflow_hash].append(workflow.id)
            else:
                workflow_dedupe_hash[workflow_hash] = [workflow.id]

        for workflow_hash, workflow_ids in workflow_dedupe_hash.items():
            # The workflow is not duplicated in the org, continue
            if len(workflow_ids) <= 1:
                continue

            # Get the canonical version of the workflow, just use the first one created
            workflow_id = workflow_ids.pop(0)

            # Update the DetectorWorkflow references to use the canonical version
            DetectorWorkflow.objects.filter(workflow_id__in=workflow_ids).update(
                workflow_id=workflow_id
            )

            # Update AlertRuleWorkflow entries to point to the canonical workflow
            AlertRuleWorkflow.objects.filter(workflow_id__in=workflow_ids).update(
                workflow_id=workflow_id
            )

            # Delete the duplicate workflows
            Workflow.objects.filter(id__in=workflow_ids).delete()


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production.
    # This should only be used for operations where it's safe to run the migration after your
    # code has deployed. So this should not be used for most operations that alter the schema
    # of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually so that they can be
    #   monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   run this outside deployments so that we don't block them. Note that while adding an index
    #   is a schema change, it's completely safe to run the operation after the code has deployed.
    # Once deployed, run these manually via: https://develop.sentry.dev/database-migrations/#migration-deployment

    is_post_deployment = False

    dependencies = [
        ("workflow_engine", "0104_action_data_fallthrough_type"),
    ]

    operations = [
        migrations.RunPython(
            deduplicate_workflows,
            migrations.RunPython.noop,
            hints={"tables": ["workflow_engine_workflow", "workflow_engine_detectorworkflow"]},
        )
    ]
