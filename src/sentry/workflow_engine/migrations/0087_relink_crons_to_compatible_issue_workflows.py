# Generated by Django 5.2.1 on 2025-09-19 19:40
import json  # noqa: S003
import logging
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any

from django.db import migrations
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps
from django.db.models import Prefetch

from sentry.new_migrations.migrations import CheckedMigration
from sentry.utils.query import RangeQuerySetWrapper

logger = logging.getLogger(__name__)

# Conditions that we know work with cron monitor events
# Note: assigned_to is handled specially - only allowed if it matches monitor owner
ALLOWED_CONDITIONS = {
    "first_seen_event",
    "regression_event",
    "reappeared_event",
    "every_event",
    "age_comparison",
}


@dataclass(frozen=True)
class ConditionData:
    """Represents a condition with its type, comparison, and result."""

    type: str
    comparison: dict[str, Any]
    result: bool

    def to_dict(self) -> dict[str, Any]:
        return {
            "type": self.type,
            "comparison": self.comparison,
            "result": self.result,
        }


@dataclass(frozen=True)
class ActionData:
    """Represents an action with its type and config."""

    type: str
    config: dict[str, Any]

    def to_dict(self) -> dict[str, Any]:
        return {
            "type": self.type,
            "config": self.config,
        }


@dataclass(frozen=True)
class ActionGroupData:
    """Represents an action group with its conditions and actions."""

    conditions: tuple[ConditionData, ...]
    actions: tuple[ActionData, ...]

    def to_dict(self) -> dict[str, Any]:
        return {
            "conditions": [c.to_dict() for c in self.conditions],
            "actions": [a.to_dict() for a in self.actions],
        }


@dataclass
class WorkflowData:
    """Represents a workflow with all its conditions and actions."""

    workflow: Any
    project_id: int
    environment_id: int | None
    frequency: int | None
    when_conditions: tuple[ConditionData, ...] = field(default_factory=tuple)
    action_groups: tuple[ActionGroupData, ...] = field(default_factory=tuple)

    def is_compatible_for_monitor(self, monitor: Any) -> bool:
        """Check if all conditions are compatible with cron events for this monitor."""
        for condition in self.when_conditions:
            if not self._is_condition_compatible(condition, monitor):
                return False

        for group in self.action_groups:
            for condition in group.conditions:
                if not self._is_condition_compatible(condition, monitor):
                    return False

        return True

    def _is_condition_compatible(self, condition: ConditionData, monitor: Any) -> bool:
        """Check if a condition is compatible with cron events for this monitor."""
        if condition.type == "assigned_to":
            target_type = condition.comparison.get("target_type")
            target_identifier = condition.comparison.get("target_identifier")

            if target_type == "Team" and monitor.owner_team_id:
                return target_identifier == monitor.owner_team_id
            elif target_type == "Member" and monitor.owner_user_id:
                return target_identifier == monitor.owner_user_id
            elif target_type == "Unassigned":
                # Unassigned is compatible if monitor has no owner
                return monitor.owner_team_id is None and monitor.owner_user_id is None
            else:
                return False

        return condition.type in ALLOWED_CONDITIONS

    def get_hash(self) -> str:
        """Get a unique hash for deduplication."""
        hash_data = {
            "project_id": self.project_id,
            "environment_id": self.environment_id,
            "frequency": self.frequency,
            "when_conditions": [
                c.to_dict()
                for c in sorted(
                    self.when_conditions,
                    key=lambda c: (c.type, json.dumps(c.comparison, sort_keys=True)),
                )
            ],
            "action_groups": [
                {
                    "conditions": [
                        c.to_dict()
                        for c in sorted(
                            g.conditions,
                            key=lambda c: (c.type, json.dumps(c.comparison, sort_keys=True)),
                        )
                    ],
                    "actions": [
                        a.to_dict()
                        for a in sorted(
                            g.actions, key=lambda a: (a.type, json.dumps(a.config, sort_keys=True))
                        )
                    ],
                }
                for g in sorted(
                    self.action_groups, key=lambda g: json.dumps(g.to_dict(), sort_keys=True)
                )
            ],
        }
        return json.dumps(hash_data, sort_keys=True)


def build_workflow_data(
    workflow: Any,
    rule_id: int,
    rules_by_id: dict[int, Any],
    when_conditions: list[Any],
    action_groups_data: list[tuple[list[Any], list[Any]]],
) -> WorkflowData | None:
    """Build a WorkflowData object from a workflow and its related data."""
    rule = rules_by_id.get(rule_id)
    if not rule:
        return None
    when_condition_list = []
    for condition in when_conditions:
        when_condition_list.append(
            ConditionData(
                type=condition.type,
                comparison=condition.comparison,
                result=condition.condition_result,
            )
        )
    action_group_list = []
    for group_conditions, group_actions in action_groups_data:
        conditions = tuple(
            ConditionData(
                type=c.type,
                comparison=c.comparison,
                result=c.condition_result,
            )
            for c in group_conditions
        )
        actions = tuple(
            ActionData(
                type=a.type,
                config=a.config,
            )
            for a in group_actions
        )
        action_group_list.append(ActionGroupData(conditions=conditions, actions=actions))

    return WorkflowData(
        workflow=workflow,
        project_id=rule.project_id,
        environment_id=workflow.environment_id,
        frequency=rule.data.get("frequency", 30),
        when_conditions=tuple(when_condition_list),
        action_groups=tuple(action_group_list),
    )


def link_crons_to_compatible_issue_workflows(
    apps: StateApps, schema_editor: BaseDatabaseSchemaEditor
) -> None:
    """
    Re-link cron detectors to compatible issue workflows after migration 0086.

    This migration:
    1. Filters issue workflows to only those with compatible conditions for cron events
    2. Deduplicates workflows with identical conditions and actions
    3. Links cron detectors to the unique, compatible workflows
    4. For assigned_to conditions, only allows them if they match the monitor's owner
    """
    Detector = apps.get_model("workflow_engine", "Detector")
    DetectorWorkflow = apps.get_model("workflow_engine", "DetectorWorkflow")
    Workflow = apps.get_model("workflow_engine", "Workflow")
    Rule = apps.get_model("sentry", "Rule")
    DataCondition = apps.get_model("workflow_engine", "DataCondition")
    WorkflowDataConditionGroup = apps.get_model("workflow_engine", "WorkflowDataConditionGroup")
    DataConditionGroupAction = apps.get_model("workflow_engine", "DataConditionGroupAction")
    AlertRuleWorkflow = apps.get_model("workflow_engine", "AlertRuleWorkflow")
    DataSourceDetector = apps.get_model("workflow_engine", "DataSourceDetector")
    Monitor = apps.get_model("monitors", "Monitor")

    rules_by_project = defaultdict(list)

    for rule in RangeQuerySetWrapper(Rule.objects.all(), step=10000):
        if rule.source == 0:
            rules_by_project[rule.project_id].append(rule)

    if not rules_by_project:
        logger.info("No issue rules found, skipping migration")
        return

    total_links_created = 0
    total_projects_processed = 0

    for project_id, project_rules in rules_by_project.items():
        rule_ids = [r.id for r in project_rules]
        rules_by_id = {r.id: r for r in project_rules}

        issue_workflows = list(
            Workflow.objects.filter(alertruleworkflow__rule_id__in=rule_ids)
            .select_related("when_condition_group")
            .prefetch_related(
                Prefetch(
                    "when_condition_group__conditions",
                    queryset=DataCondition.objects.all(),
                    to_attr="prefetched_when_conditions",
                ),
                Prefetch(
                    "workflowdataconditiongroup_set",
                    queryset=WorkflowDataConditionGroup.objects.select_related(
                        "condition_group"
                    ).prefetch_related(
                        Prefetch(
                            "condition_group__conditions",
                            queryset=DataCondition.objects.all(),
                            to_attr="prefetched_conditions",
                        ),
                        Prefetch(
                            "condition_group__dataconditiongroupaction_set",
                            queryset=DataConditionGroupAction.objects.select_related("action"),
                            to_attr="prefetched_actions",
                        ),
                    ),
                    to_attr="prefetched_action_groups",
                ),
                Prefetch(
                    "alertruleworkflow_set",
                    queryset=AlertRuleWorkflow.objects.all(),
                    to_attr="prefetched_rule_workflows",
                ),
            )
            .distinct()
        )

        if not issue_workflows:
            continue

        workflow_data_list = []
        for workflow in issue_workflows:
            if not workflow.prefetched_rule_workflows:
                continue
            rule_workflow = workflow.prefetched_rule_workflows[0]

            when_conditions = []
            if workflow.when_condition_group:
                when_conditions = workflow.when_condition_group.prefetched_when_conditions

            action_groups_data = []
            for wdcg in workflow.prefetched_action_groups:
                group_conditions = wdcg.condition_group.prefetched_conditions
                group_actions = [ga.action for ga in wdcg.condition_group.prefetched_actions]
                action_groups_data.append((group_conditions, group_actions))

            workflow_data = build_workflow_data(
                workflow, rule_workflow.rule_id, rules_by_id, when_conditions, action_groups_data
            )
            if workflow_data:
                workflow_data_list.append(workflow_data)

        if not workflow_data_list:
            continue

        cron_detectors = list(
            Detector.objects.filter(type="monitor_check_in_failure", project_id=project_id)
        )

        if not cron_detectors:
            continue

        detector_ids = [d.id for d in cron_detectors]
        data_source_detectors = DataSourceDetector.objects.filter(
            detector_id__in=detector_ids
        ).select_related("data_source")

        detector_to_monitor_id = {}
        for dsd in data_source_detectors:
            if dsd.data_source.type == "cron_monitor":
                detector_to_monitor_id[dsd.detector_id] = int(dsd.data_source.source_id)

        monitor_ids = list(detector_to_monitor_id.values())
        monitors_by_id = {m.id: m for m in Monitor.objects.filter(id__in=monitor_ids)}

        # Link cron detectors to compatible workflows in this project
        project_links_created = 0
        for detector in cron_detectors:
            monitor_id = detector_to_monitor_id.get(detector.id)
            if not monitor_id:
                continue

            monitor = monitors_by_id.get(monitor_id)
            if not monitor:
                continue

            compatible_workflows = []
            for wd in workflow_data_list:
                if wd.is_compatible_for_monitor(monitor):
                    compatible_workflows.append(wd)

            hash_to_workflow_data = {}
            for wd in compatible_workflows:
                workflow_hash = wd.get_hash()
                if workflow_hash not in hash_to_workflow_data:
                    hash_to_workflow_data[workflow_hash] = wd

            for wd in hash_to_workflow_data.values():
                _, created = DetectorWorkflow.objects.get_or_create(
                    detector=detector,
                    workflow=wd.workflow,
                )
                if created:
                    project_links_created += 1

        if project_links_created > 0:
            logger.info(
                "Processed project",
                extra={
                    "project_id": project_id,
                    "links_created": project_links_created,
                    "total_workflows": len(workflow_data_list),
                    "cron_detectors": len(cron_detectors),
                },
            )

        total_links_created += project_links_created
        total_projects_processed += 1

    logger.info(
        "Migration complete",
        extra={
            "total_links_created": total_links_created,
            "projects_processed": total_projects_processed,
        },
    )


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production.
    # This should only be used for operations where it's safe to run the migration after your
    # code has deployed. So this should not be used for most operations that alter the schema
    # of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually so that they can be
    #   monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   run this outside deployments so that we don't block them. Note that while adding an index
    #   is a schema change, it's completely safe to run the operation after the code has deployed.
    # Once deployed, run these manually via: https://develop.sentry.dev/database-migrations/#migration-deployment

    is_post_deployment = True

    dependencies = [
        ("workflow_engine", "0086_fix_cron_to_cron_workflow_links"),
        ("monitors", "0009_backfill_monitor_detectors"),
    ]

    operations = [
        migrations.RunPython(
            link_crons_to_compatible_issue_workflows,
            migrations.RunPython.noop,
            hints={"tables": ["workflow_engine_detectorworkflow"]},
        ),
    ]
