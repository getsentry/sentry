# Generated by Django 2.2.28 on 2023-02-14 19:42
import csv
import os.path
import typing
import uuid
from collections import UserDict, defaultdict
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Mapping, Optional, Sequence, Tuple

from django.db import migrations
from snuba_sdk import (
    Column,
    Condition,
    Direction,
    Entity,
    Function,
    Limit,
    Offset,
    Op,
    OrderBy,
    Query,
    Request,
)

from sentry import eventtypes
from sentry.event_manager import GroupInfo
from sentry.eventstore.models import Event
from sentry.issues.grouptype import (
    PerformanceConsecutiveDBQueriesGroupType,
    PerformanceFileIOMainThreadGroupType,
    PerformanceMNPlusOneDBQueriesGroupType,
    PerformanceNPlusOneAPICallsGroupType,
    PerformanceNPlusOneGroupType,
    PerformanceRenderBlockingAssetSpanGroupType,
    PerformanceSlowDBQueryGroupType,
    PerformanceUncompressedAssetsGroupType,
)
from sentry.issues.ingest import process_occurrence_data, send_issue_occurrence_to_eventstream
from sentry.issues.issue_occurrence import IssueOccurrence, IssueOccurrenceData
from sentry.issues.occurrence_consumer import lookup_event
from sentry.new_migrations.migrations import CheckedMigration
from sentry.snuba.dataset import Dataset, EntityKey

if typing.TYPE_CHECKING:
    from sentry.models import Group

MIGRATION_NAME = "0378_backfill_perf_issue_events_issue_platform"


# TODO: figure out how to parameterize this to the RunPython call
DRY_RUN = False
WRITE_TO_FILE = True

START_DATETIME = datetime(2008, 5, 8).replace(tzinfo=timezone.utc)

# TODO: update this to the date when we began writing to org-project performance issues
#       this is important to set to avoid unnecessarily scanning the full 90 retention period
#       when there's no data before this date
ISSUE_PLATFORM_INGEST_START_DATETIME = (datetime(2023, 3, 13) - timedelta(days=1)).replace(
    microsecond=0, tzinfo=timezone.utc
)

# TODO: update this to when we're fully dual-writing to transactions and search_issues
#       since we'll be dual-writing,
ISSUE_PLATFORM_DUAL_WRITE_DATETIME = (datetime(2023, 3, 25) + timedelta(days=1)).replace(
    microsecond=0, tzinfo=timezone.utc
)

PROGRESS_SUCCESS_PATH_PREFIX = "success"
PROGRESS_ERROR_PATH_PREFIX = "error"

PERFORMANCE_TYPES = (
    PerformanceSlowDBQueryGroupType.type_id,
    PerformanceRenderBlockingAssetSpanGroupType.type_id,
    PerformanceNPlusOneGroupType.type_id,
    PerformanceConsecutiveDBQueriesGroupType.type_id,
    PerformanceFileIOMainThreadGroupType.type_id,
    PerformanceNPlusOneAPICallsGroupType.type_id,
    PerformanceMNPlusOneDBQueriesGroupType.type_id,
    PerformanceUncompressedAssetsGroupType.type_id,
)


class TransactionRow(typing.TypedDict):
    project_id: int
    group_id: int
    event_id: str
    finish_ts: datetime


class BackfillEventSuccess(typing.Protocol):
    def __call__(
        self, timestamp: datetime, created_group: "Group", mapped_occurrence: IssueOccurrenceData
    ) -> None:
        pass


class BackfillEventError(typing.Protocol):
    def __call__(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        pass


def print_success(
    timestamp: datetime, created_group: "Group", mapped_occurrence: IssueOccurrenceData
) -> None:
    print(  # noqa: S002
        f"project_id={created_group.project_id}, group_id={created_group.id}, event_id={mapped_occurrence['event_id']}, occurrence_id={mapped_occurrence['id']}, fingerprint={mapped_occurrence['fingerprint']}"
    )


def print_exception_on_error(
    attempted_row: TransactionRow,
    exc: Exception,
    occurrence_nodestore_saved: bool,
    occurrence_eventstream_sent: bool,
) -> None:
    print(  # noqa: S002
        f"row={attempted_row}, exception={exc}, nodestore_saved={occurrence_nodestore_saved}, eventstream_sent={occurrence_eventstream_sent}"
    )


def backfill_eventstream(apps: Any, schema_editor: Any) -> None:
    """
    Backfills Performance-issue events from the transaction table to the IssuePlatform dataset(search_issues).
    1. Source Groups from postgres as the 'source of truth' for issues.
    2. For each project with a performance issue:
         a. Query snuba for the transactions backing a perf issue. We only need (project_id, group_id, event_id).
            This is paginated with a limit of 10000 transactions per page to avoid pull in too much data into memory.
         b. Query snuba for generic events backing a perf issue (we'll be dual-writing to both tables, so we need
            to ensure the event doesn't already exist in search_issues).
         c. For each transaction:
              i. Retrieve the Group row from postgres.
              ii. Retrieve the GroupHash row from postgres.
              iii. Retrieve the entire raw Transaction from node store.
              iv.
              v. Map the Transaction event to an instance of IssueOccurrenceData.
              v. Save the IssueOccurrence to node store. Normally this step would also save the appropriate models to
                 postgres like Group, GroupHash, GroupEnvironment, GroupRelease, and increment Release counts, but
                 we aren't doing that on this backfill since they should already exist when the transaction was already
                 ingested through the old style of performance issue creation.
              vi. Send the IssueOccurrence through the eventstream to be saved in search_issues.
         c. Repeat step (a), until the paginated snuba query returns no data.
    """

    Group = apps.get_model("sentry", "Group")
    GroupHash = apps.get_model("sentry", "GroupHash")

    projects_with_perf_issues = (
        Group.objects.filter(type__in=PERFORMANCE_TYPES)
        .values("project_id")
        .order_by("project_id")
        .distinct()
    )

    for project_perf_issue in projects_with_perf_issues:
        project_id = project_perf_issue["project_id"]

        path = f"{MIGRATION_NAME}/{project_id}/"
        Path(path).mkdir(parents=True, exist_ok=True)

        project_done = f"{path}done.txt"

        if not os.path.isfile(project_done):
            previous_progress = f"{path}previous.csv"
            previous_error = f"{path}error.csv"
            last_timestamp = START_DATETIME
            last_event_id = None

            # we previously errored out, start from error mark
            if os.path.isfile(previous_error):
                with open(previous_progress) as previous_error_file:
                    reader = csv.DictReader(previous_error_file)
                    last_row = None
                    for row in reader:
                        last_row = {
                            "finish_ts": datetime.fromisoformat(row["finish_ts"]),
                            "event_id": row["event_id"],
                            "exception": row["exception"],
                            "nodestore_saved": True if row["nodestore_saved"] == "1" else False,
                            "eventstream_sent": True if row["eventstream_sent"] == "1" else False,
                        }
                    if last_row is not None:
                        last_timestamp = last_row["finish_ts"]  # type: ignore[assignment]
                        last_event_id = str(last_row["event_id"])
                        # TODO: if nodestore_saved or eventstream_sent is True, we probably shouldn't re-process the
                        #        event. we'll dry-run this script to see if we do end up getting errors and work
                        #        accordingly
                        if not DRY_RUN:
                            if last_row["eventstream_sent"]:
                                raise Exception(
                                    f"previously errored event {last_row} was already sent to eventstream, we probably need to manually check this event in the dataset and reconcile the difference"
                                )
            # script crashed without logging the error, we start from this position
            elif os.path.isfile(previous_progress):
                with open(previous_progress) as previous_progress_file:
                    reader = csv.DictReader(previous_progress_file)
                    for row in reader:
                        last_timestamp = datetime.fromisoformat(row["finish_ts"])
                        last_event_id = str(row["event_id"])

            with open(previous_progress, "w") as track_progress, open(
                previous_error, "w"
            ) as track_error:
                track_progress.seek(0)
                backfill_by_project(
                    project_id,
                    Group,
                    GroupHash,
                    DRY_RUN,
                    track_progress,
                    track_error,
                    previous_finish_ts=last_timestamp,
                    previous_event_id=last_event_id,
                )

            # at this point we've successfully processed a project without errors we create a /{project_id}/done.txt
            # file to mark it as complete and skip processing this project if we have to run the script again
            # delete /{project_id}/previous.csv and /{project_id}/error.csv since they're not needed anymore
            with open(project_done, "w") as _:
                pass
            if os.path.isfile(previous_progress):
                os.remove(previous_progress)
            if os.path.isfile(previous_error):
                os.remove(previous_error)


class ProjectGroupTotalProgress:
    """
    This class was used to track the progress of a backfill for all events for an org-project-group combination.
    Likely not needed anymore since we're tracking the progress for an org-project. Keeping it here for now just in case
    """

    last_success: Optional[Tuple[datetime, str]]  # (finish_ts, event_id)
    last_error: Optional[
        Mapping[str, Any]
    ]  # {"finish_ts": <>, "event_id": <>, "exception": <>, "nodestore_saved": <>, "eventstream_sent": <>}
    success_file: str
    error_file: str
    opened_success_file: typing.TextIO
    opened_error_file: typing.TextIO

    SUCCESS_FIELD_NAMES: Sequence[str] = ["finish_ts", "event_id", "occurrence_id", "fingerprint"]
    ERROR_FIELD_NAMES: Sequence[str] = [
        "finish_ts",
        "event_id",
        "exception",
        "nodestore_saved",
        "eventstream_sent",
    ]

    def __init__(self, project_id: int, group_id: int) -> None:
        self.project_id = project_id
        self.group_id = group_id
        self.last_success = None
        self.last_error = None
        self.success_file = self.__get_file_path(
            self.project_id, PROGRESS_SUCCESS_PATH_PREFIX, self.group_id
        )
        self.error_file = self.__get_file_path(
            self.project_id, PROGRESS_ERROR_PATH_PREFIX, self.group_id
        )
        self.__parse_success_file()
        self.__parse_error_file()

        self.opened_success_file = open(self.success_file, "w")
        self.opened_error_file = open(self.error_file, "w")

    def consume_error(self) -> Optional[Mapping[str, Any]]:
        ret_val = self.last_error
        self.clear_error()
        return ret_val

    def clear_error(self) -> None:
        self.last_error = None
        try:
            self.opened_error_file.close()
        except OSError:
            pass
        try:
            os.remove(self.error_file)
        except FileNotFoundError:
            pass

    def already_processed(self, timestamp: datetime, event_id: str) -> bool:
        if self.last_success is None and self.last_error is None:
            return False
        if self.last_error is not None:
            raise Exception("last_error must be cleared before processing more events")

        if self.last_success is not None:
            if timestamp < self.last_success[0]:
                return True
            elif timestamp == self.last_success[0]:
                return event_id >= self.last_success[1]
            else:
                return False
        else:
            return False

    def close(self) -> None:
        try:
            self.opened_success_file.close()
        except ValueError:
            pass

        try:
            self.opened_error_file.close()
        except ValueError:
            pass

    def save_success_to_file(
        self, timestamp: datetime, created_group: "Group", mapped_occurrence: IssueOccurrenceData
    ) -> None:
        self.last_success = (timestamp, mapped_occurrence["event_id"])
        success_writer = csv.DictWriter(
            self.opened_success_file, fieldnames=self.SUCCESS_FIELD_NAMES
        )
        success_writer.writerow(
            {
                self.SUCCESS_FIELD_NAMES[0]: timestamp,
                self.SUCCESS_FIELD_NAMES[1]: mapped_occurrence["event_id"],
                self.SUCCESS_FIELD_NAMES[2]: mapped_occurrence["id"],
                self.SUCCESS_FIELD_NAMES[3]: mapped_occurrence["fingerprint"][0],
            }
        )
        # self.opened_success_file.seek(0)

    def save_error_to_file(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        row = {
            self.ERROR_FIELD_NAMES[0]: attempted_row["finish_ts"],
            self.ERROR_FIELD_NAMES[1]: attempted_row["event_id"],
            self.ERROR_FIELD_NAMES[2]: type(exc).__name__,
            # save a little disk space by serializing the bool as a 1/0
            self.ERROR_FIELD_NAMES[3]: "1" if occurrence_nodestore_saved is True else "0",
            self.ERROR_FIELD_NAMES[4]: "1" if occurrence_eventstream_sent is True else "0",
        }
        self.last_error = row
        error_writer = csv.DictWriter(self.opened_error_file, fieldnames=self.ERROR_FIELD_NAMES)
        error_writer.writerow(row)
        # self.opened_error_file.seek(0)

    def __parse_success_file(self) -> Optional[Tuple[datetime, str]]:
        try:
            with open(self.success_file) as f:
                reader = csv.DictReader(f, fieldnames=self.SUCCESS_FIELD_NAMES)
                last_row = None
                for row in reader:
                    last_row = (row["finish_ts"], row["event_id"])
                return last_row  # type: ignore[return-value]
        except FileNotFoundError:
            return None

    def __parse_error_file(self) -> Optional[Mapping[str, Any]]:
        try:
            with open(self.error_file) as f:
                reader = csv.DictReader(f, fieldnames=self.ERROR_FIELD_NAMES)
                last_row = None
                for row in reader:
                    last_row = {
                        "event_id": row[self.ERROR_FIELD_NAMES[0]],
                        "exception": row[self.ERROR_FIELD_NAMES[1]],
                        "nodestore_saved": True if row[self.ERROR_FIELD_NAMES[2]] == "1" else False,
                        "eventstream_sent": True
                        if row[self.ERROR_FIELD_NAMES[3]] == "1"
                        else False,
                    }
                return last_row
        except FileNotFoundError:
            return None

    def __get_file_path(self, project_id: int, path_suffix: str, group_id: int) -> str:
        # TODO: need to verify the relative path
        path = f"{MIGRATION_NAME}/{project_id}/{group_id}/"
        Path(path).mkdir(parents=True, exist_ok=True)

        return f"{path}{path_suffix}.csv"


if typing.TYPE_CHECKING:
    FileBackedProjectProgressType = UserDict[int, ProjectGroupTotalProgress]
else:
    FileBackedProjectProgressType = UserDict


class FileBackedProjectProgress(FileBackedProjectProgressType):
    """
    Holds the file handles for maintaining the progress of migrations for a project. Hopefully, we shouldn't hit
    the OS limit on open handles since the project with the most performance issues is around 9k. So this class
    should only be holding at most 2 * 9k file handles before this migration moves on to the next project.

    """

    def __init__(self, project_id: int) -> None:
        super().__init__()
        self.project_id = project_id

    def __enter__(self) -> "FileBackedProjectProgress":
        return self

    def __exit__(self, *args: Any, **kwargs: Any) -> None:
        try:
            for progress in self.values():
                progress.close()
        except ValueError:
            pass

    def get_or_init(self, group_id: int) -> ProjectGroupTotalProgress:
        if self.get(group_id):
            return self.__getitem__(group_id)
        else:
            progress = ProjectGroupTotalProgress(self.project_id, group_id)
            self.__setitem__(group_id, progress)
            return progress


def get_already_ingested_events(project_id: int) -> Mapping[int, typing.Set[str]]:
    """
    Retrieves all the IssuePlatform events for a project and loads it all into memory. This is probably ok as long
    as we run this script with the right start and end time bounds.
    """
    already_ingested_events: Mapping[int, typing.Set[str]] = defaultdict(
        set
    )  # group_id: Set[event_id]

    # it's probably ok to query all the events here instead of paginating since we should be running this script
    # closely after we cut-over

    next_offset: Optional[int] = 0
    while next_offset is not None:
        rows, next_offset = _query_issue_platform_events(
            project_id,
            start=ISSUE_PLATFORM_INGEST_START_DATETIME,
            end=ISSUE_PLATFORM_DUAL_WRITE_DATETIME,
            offset=next_offset,
        )
        for issue_platform_events in rows:
            already_ingested_events[issue_platform_events["group_id"]].add(
                issue_platform_events["event_id"]
            )

    return already_ingested_events


def backfill_by_project(
    project_id: int,
    Group: Any,
    GroupHash: Any,
    dry_run: bool,
    project_progress: typing.TextIO,
    project_error: typing.TextIO,
    previous_finish_ts: Optional[datetime] = None,
    previous_event_id: Optional[str] = None,
) -> None:
    already_ingested_events = get_already_ingested_events(project_id)

    next_offset: Optional[int] = 0
    start_datetime = previous_finish_ts if previous_finish_ts is not None else START_DATETIME
    project_progress_writer = csv.DictWriter(project_progress, fieldnames=["finish_ts", "event_id"])

    with FileBackedProjectProgress(project_id) as previous_progress:
        while next_offset is not None:
            rows, next_offset = _query_performance_issue_events(
                project_id=project_id,
                start=start_datetime,
                end=ISSUE_PLATFORM_DUAL_WRITE_DATETIME,
                offset=next_offset,
            )

            for row in rows:
                group_id = row["group_id"]
                event_id = row["event_id"]
                finish_ts = datetime.fromisoformat(row["finish_ts"])
                nodestore_saved = False
                eventstream_sent = False

                if previous_finish_ts is not None:
                    if finish_ts < previous_finish_ts:
                        # current row is somehow before when we should start, skip this row
                        continue
                    elif finish_ts > previous_finish_ts:
                        # current row is after our starting position, process it
                        pass
                    else:
                        if previous_event_id is not None and event_id <= previous_event_id:
                            continue
                try:
                    # avoid re-ingesting the same transaction since we'll be dual-writing
                    if event_id not in already_ingested_events[group_id]:
                        group_progress: ProjectGroupTotalProgress = previous_progress.get_or_init(
                            group_id
                        )
                        error_maybe = group_progress.consume_error()
                        if error_maybe is None:
                            already_processed = group_progress.already_processed(
                                finish_ts, event_id
                            )
                            if already_processed:
                                continue
                            else:
                                nodestore_saved, eventstream_sent = backfill_perf_issue_occurrence(
                                    row,
                                    Group,
                                    GroupHash,
                                    print_success
                                    if not WRITE_TO_FILE
                                    else group_progress.save_success_to_file,
                                    print_exception_on_error,
                                    # if not WRITE_TO_FILE
                                    # else group_progress.save_error_to_file,
                                    dry_run=dry_run,
                                )
                        else:
                            # we changed how we're handling errors, instead we halt all progress when we encounter
                            # an error instead of trying to recover in-place
                            # there shouldn't be any previous errors logged for a project-group
                            raise Exception(
                                f"Unexpected error for project({project_id}), group_id({group_id}), event_id({event_id}), finish_ts({finish_ts}), nodestore_saved={nodestore_saved}, eventstream_sent={eventstream_sent}"
                            )
                            # we've encountered a previous error while processing the page, reset the query and start
                            # from the error (finish_ts, event_id) position
                            # resume progress from the (finish_ts, event_id) in the last processed error
                            # last_error_finish_ts = error_maybe["finish_ts"]
                            # last_error_event_id = error_maybe["event_id"]
                            # last_error_exception = error_maybe["exception"]
                            # last_error_nodestore_saved = error_maybe["nodestore_saved"]
                            # last_error_eventstream_sent = error_maybe["eventstream_sent"]
                            # # TODO: implement some sort of pagination scheme where we pass back in the finish_ts
                            #
                            # # reset
                            # next_offset = 0
                            # start_datetime = last_error_finish_ts
                            # pass

                    # commit progress to file, if we crash or encounter an error, we'll resume back at this point
                    # we track the progress of each event scoped to a project-group, so it's ok if this event was already
                    # processed from a previous run
                    project_progress.seek(0)
                    project_progress_writer.writerow(
                        {"finish_ts": finish_ts.isoformat(), "event_id": event_id}
                    )
                except Exception as e:
                    project_error.seek(0)
                    writer = csv.DictWriter(
                        project_error,
                        fieldnames=[
                            "finish_ts",
                            "event_id",
                            "exception",
                            "nodestore_saved",
                            "eventstream_sent",
                        ],
                    )
                    writer.writerow(
                        {
                            "finish_ts": finish_ts.isoformat(),
                            "event_id": event_id,
                            "exception": type(e).__name__,
                            "nodestore_saved": "1" if nodestore_saved else "0",
                            "eventstream_sent": "1" if eventstream_sent else "0",
                        }
                    )
                    raise


def backfill_perf_issue_occurrence(
    row: TransactionRow,
    Group: Any,
    GroupHash: Any,
    on_success: BackfillEventSuccess = lambda *args: None,
    on_exception: BackfillEventError = lambda *args: None,
    dry_run: bool = True,
) -> Tuple[bool, bool]:
    occurrence_nodestore_saved = False
    occurrence_eventstream_sent = False

    try:
        project_id = row["project_id"]
        group_id = row["group_id"]
        event_id = row["event_id"]
        finish_ts = row["finish_ts"]

        group = Group.objects.get(id=group_id)
        group_hash = GroupHash.objects.get(group_id=group_id)

        event: Event = lookup_event(project_id=project_id, event_id=event_id)

        # TODO: this data probably maps to evidence_data and/or evidence_display
        # from sentry.utils.performance_issues.performance_detection import EventPerformanceProblem
        #
        # hashes = event.get_hashes()
        # problems: Sequence[Optional[EventPerformanceProblem]] = EventPerformanceProblem.fetch_multi([(event, h) for h in hashes.hashes])

        et = eventtypes.get(group.data.get("type", "default"))()
        issue_title = et.get_title(group.data["metadata"])
        assert issue_title
        # need to map the base raw data to an issue occurrence
        # make sure this is consistent with how we plan to ingest performance issue occurrences
        occurrence_data: IssueOccurrenceData = IssueOccurrenceData(
            id=uuid.uuid4().hex,
            project_id=project_id,
            event_id=event_id,
            fingerprint=[group_hash.hash],
            issue_title=issue_title,  # TODO: verify
            subtitle=group.culprit,  # TODO: verify
            resource_id=None,
            evidence_data={},  # TODO: verify
            evidence_display=[],  # TODO: verify
            type=group.type,
            detection_time=datetime.now().timestamp(),
            level=None,
        )
        if dry_run is False:
            occurrence, group_info = __save_issue_occurrence(occurrence_data, event, group)
            occurrence_nodestore_saved = True

            send_issue_occurrence_to_eventstream(event, occurrence, group_info, skip_consume=True)
            occurrence_eventstream_sent = True

        on_success(finish_ts, group, occurrence_data)
        return occurrence_nodestore_saved, occurrence_eventstream_sent
    except Exception as e:
        on_exception(row, e, occurrence_nodestore_saved, occurrence_eventstream_sent)
        raise


def __save_issue_occurrence(
    occurrence_data: IssueOccurrenceData, event: Event, group: "Group"
) -> Tuple[IssueOccurrence, GroupInfo]:
    process_occurrence_data(occurrence_data)
    # Convert occurrence data to `IssueOccurrence`
    occurrence = IssueOccurrence.from_dict(occurrence_data)
    if occurrence.event_id != event.event_id:
        raise ValueError("IssueOccurrence must have the same event_id as the passed Event")
    # Note: For now we trust the project id passed along with the event. Later on we should make
    # sure that this is somehow validated.
    occurrence.save()

    # don't need to create releases or environments since they should be created already

    # synthesize a 'fake' group_info based off of existing data in postgres
    group_info: GroupInfo = GroupInfo(group=group, is_new=False, is_regression=False)

    return occurrence, group_info


def _query_performance_issue_events(
    project_id: int, start: datetime, end: datetime, offset: int = 0
) -> Tuple[Sequence[TransactionRow], Optional[int]]:
    page_limit = 10000

    snuba_request = Request(
        dataset=Dataset.Transactions.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.Transactions.value),
            select=[
                Column("project_id"),
                Function("arrayJoin", parameters=[Column("group_ids")], alias="group_id"),
                Column("event_id"),
                Column("finish_ts"),
            ],
            where=[
                Condition(Column("group_ids"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("finish_ts"), Op.GTE, start),
                Condition(Column("finish_ts"), Op.LT, end),
            ],
            groupby=[
                Column("project_id"),
                Column("group_id"),
                Column("event_id"),
                Column("finish_ts"),
            ],
            orderby=[
                OrderBy(Column("finish_ts"), direction=Direction.ASC),
                OrderBy(Column("event_id"), direction=Direction.ASC),
            ],
            limit=Limit(page_limit),
            offset=Offset(offset),
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    result_data = result_snql["data"]
    next_offset = None if not result_data else offset + page_limit
    return result_data, next_offset


def _query_issue_platform_events(
    project_id: int, start: datetime, end: datetime, offset: int = 0
) -> Tuple[Sequence[Mapping[str, Any]], Optional[int]]:
    page_limit = 10000

    snuba_request = Request(
        dataset=Dataset.IssuePlatform.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.IssuePlatform.value),
            select=[
                Column("group_id"),
                Column("event_id"),
                Column("timestamp"),
            ],
            where=[
                Condition(Column("group_id"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("occurrence_type_id"), Op.IN, PERFORMANCE_TYPES),
                Condition(Column("timestamp"), Op.GTE, start),
                Condition(Column("timestamp"), Op.LT, end),
            ],
            groupby=[
                Column("group_id"),
                Column("occurrence_type_id"),
                Column("event_id"),
                Column("timestamp"),
            ],
            orderby=[
                OrderBy(Column("timestamp"), direction=Direction.ASC),
                OrderBy(Column("event_id"), direction=Direction.ASC),
            ],
            limit=Limit(page_limit),
            offset=Offset(offset),
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    result_data = result_snql["data"]
    next_offset = None if not result_data else offset + page_limit
    return result_data, next_offset


class Migration(CheckedMigration):  # type: ignore[misc]
    # This flag is used to mark that a migration shouldn't be automatically run in production. For
    # the most part, this should only be used for operations where it's safe to run the migration
    # after your code has deployed. So this should not be used for most operations that alter the
    # schema of a table.
    # Here are some things that make sense to mark as dangerous:
    # - Large data migrations. Typically we want these to be run manually by ops so that they can
    #   be monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   have ops run this and not block the deploy. Note that while adding an index is a schema
    #   change, it's completely safe to run the operation after the code has deployed.
    is_dangerous = True

    dependencies = [
        ("sentry", "0377_groupedmesssage_type_individual_index"),
    ]

    operations = [
        migrations.RunPython(
            backfill_eventstream,
            reverse_code=migrations.RunPython.noop,
            hints={"tables": ["sentry_groupedmessage", "sentry_grouphash"]},
        )
    ]
