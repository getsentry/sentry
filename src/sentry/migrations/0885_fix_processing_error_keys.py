# Generated by Django 5.2.1 on 2025-05-13 17:37

from typing import Any
import uuid
from itertools import chain
from datetime import datetime
from typing import Literal, NotRequired, Required, TypedDict, Union

from django.conf import settings
from django.db import migrations
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps

from sentry.new_migrations.migrations import CheckedMigration
from sentry.utils import json, redis
from sentry.utils.query import RangeQuerySetWrapper

MAX_ERRORS_PER_SET = 10


class CheckIn(TypedDict, total=False):
    """
    check_in.

    A message that contains a monitor check-in payload.
    """

    message_type: Required[Literal["check_in"]]
    """
    Discriminant marker identifying this as a check-in message.

    Required property
    """

    payload: Required[bytes]
    """
    bytes. JSON string of the wrapped monitor check-in event.

    $bytes: True

    Required property
    """

    start_time: Required[Union[int, float]]
    """
    The time relay received the envelope containing the check-in. In seconds.

    Required property
    """

    project_id: Required[int]
    """
    The project for which this check-in is being sent.

    minimum: 0
    maximum: 18446744073709551615

    Required property
    """

    sdk: Union[str, None]
    """ The originating SDK client identifier string. """

    retention_days: Required[int]
    """
    minimum: 0
    maximum: 65535

    Required property
    """


class CheckinTrace(TypedDict):
    trace_id: str


class CheckinContexts(TypedDict):
    trace: NotRequired[CheckinTrace]


class CheckinPayload(TypedDict):
    check_in_id: str
    monitor_slug: str
    status: str
    environment: NotRequired[str]
    duration: NotRequired[int]
    monitor_config: NotRequired[dict]
    contexts: NotRequired[CheckinContexts]


class CheckinItemData(TypedDict):
    """
    See `CheckinItem` for definition
    """

    ts: str
    partition: int
    message: CheckIn
    payload: CheckinPayload


@dataclass
class CheckinItem:
    """
    Represents a check-in to be processed
    """

    ts: datetime
    """
    The timestamp the check-in was produced into the kafka topic. This differs
    from the start_time that is part of the CheckIn
    """

    partition: int
    """
    The kafka partition id the check-in was produced into.
    """

    message: CheckIn
    """
    The original unpacked check-in message contents.
    """

    payload: CheckinPayload
    """
    The json-decoded check-in payload contained within the message. Includes
    the full check-in details.
    """

    @cached_property
    def valid_monitor_slug(self):
        return slugify(self.payload["monitor_slug"])[:DEFAULT_SLUG_MAX_LENGTH].strip("-")

    @property
    def processing_key(self):
        """
        This key is used to uniquely identify the check-in group this check-in
        belongs to. Check-ins grouped together will never be processed in
        parallel with other check-ins belonging to the same group
        """
        project_id = self.message["project_id"]
        env = self.payload.get("environment")
        return f"{project_id}:{self.valid_monitor_slug}:{env}"

    def to_dict(self) -> CheckinItemData:
        return {
            "ts": self.ts.isoformat(),
            "partition": self.partition,
            "message": self.message,
            "payload": self.payload,
        }

    @classmethod
    def from_dict(cls, data: CheckinItemData) -> CheckinItem:
        return cls(
            datetime.fromisoformat(data["ts"]),
            data["partition"],
            data["message"],
            data["payload"],
        )


class CheckinProcessingErrorData(TypedDict):
    id: str
    checkin: CheckinItemData
    errors: Sequence[ProcessingError]


@dataclass(frozen=True)
class CheckinProcessingError:
    errors: Sequence[ProcessingError]
    checkin: CheckinItem
    id: uuid.UUID = field(default_factory=uuid.uuid4)

    def to_dict(self) -> CheckinProcessingErrorData:
        return {
            "id": self.id.hex,
            "checkin": self.checkin.to_dict(),
            "errors": self.errors,
        }

    @classmethod
    def from_dict(cls, data: CheckinProcessingErrorData) -> "CheckinProcessingError":
        return cls(
            id=uuid.UUID(data["id"]),
            checkin=CheckinItem.from_dict(data["checkin"]),
            errors=data["errors"],
        )


def _get_cluster() -> Any:
    return redis.redis_clusters.get(settings.SENTRY_MONITORS_REDIS_CLUSTER)


def build_monitor_identifier(monitor: Any) -> str:
    return f"monitor:{monitor.id}"


def build_error_identifier(uuid: uuid.UUID) -> str:
    return f"monitors.processing_errors.{uuid.hex}"


def build_project_identifier(project_id: int) -> str:
    return f"project:{project_id}"


def build_set_identifier(entity_identifier: str) -> str:
    return f"monitors.processing_errors_set.{entity_identifier}"


def fetch_processing_errors(entity_identifier: str) -> dict[uuid.UUID, Any]:
    redis = _get_cluster()
    pipeline = redis.pipeline()
    pipeline.zrange(build_set_identifier(entity_identifier, 0, MAX_ERRORS_PER_SET, desc=True))
    processing_errors = {}
    error_identifiers = [
        build_error_identifier(uuid.UUID(error_identifier))
        for error_identifier in chain(*pipeline.execute())
    ]
    for error_identifier in error_identifiers:
        raw_error = redis.mget(error_identifier)
        if raw_error is not None:
            processing_errors[error_identifier] = CheckinProcessingError.from_dict(
                json.loads(raw_error)
            )
    return processing_errors


def fix_processing_error_keys(apps: StateApps, schema_editor: BaseDatabaseSchemaEditor) -> None:
    Monitor = apps.get_model("sentry", "Monitor")
    Project = apps.get_model("sentry", "Project")

    # step 1: fix all monitors
    for monitor in RangeQuerySetWrapper(Monitor.objects.all()):
        monitor_identifier = build_monitor_identifier(monitor)
        processing_errors = fetch_processing_errors(monitor_identifier)
        for error_id in processing_errors:
            if processing_errors[error_id].id != error_id:
                processing_errors[error_id].id = error_id

    # step 2: fix all projects
    for project in RangeQuerySetWrapper(Project.objects.all()):
        project_identifier = build_project_identifier(project.id)
        processing_errors = fetch_processing_errors(project_identifier)
        for error_id in processing_errors:
            if processing_errors[error_id].id != error_id:
                processing_errors[error_id].id = error_id


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production.
    # This should only be used for operations where it's safe to run the migration after your
    # code has deployed. So this should not be used for most operations that alter the schema
    # of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually so that they can be
    #   monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   run this outside deployments so that we don't block them. Note that while adding an index
    #   is a schema change, it's completely safe to run the operation after the code has deployed.
    # Once deployed, run these manually via: https://develop.sentry.dev/database-migrations/#migration-deployment

    is_post_deployment = True

    dependencies = [
        ("sentry", "0884_delete_incident_snapshot_tables_pt2"),
    ]

    operations = [
        migrations.RunPython(
            fix_processing_error_keys,
            migrations.RunPython.noop,
            hints={"tables": ["sentry_monitor"]},
        )
    ]
