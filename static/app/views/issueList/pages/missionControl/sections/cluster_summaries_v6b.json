[{"project_ids":["11276"],"cluster_id":0,"group_ids":[415095452,6765971711],"issue_titles":["SyntaxError: Invalid or unexpected token","SyntaxError: Unexpected EOF"],"root_cause_summaries":["Transpilation bug in `@swc/plugin-emotion` generates truncated JavaScript for CSS template literals, causing `SyntaxError: Unexpected EOF`.","@swc/plugin-emotion bug generates malformed JavaScript lacking statement separators, causing SyntaxError."],"transactions":["/issues/:groupId/","/issues/"],"title":"CSS template literal causes SyntaxError in serviceincidents.tsx","description":"A malformed CSS-in-JS template literal in serviceincidents.tsx triggers SyntaxError: Invalid or unexpected token and Unexpected EOF, likely due to an unclosed interpolation or stray character in the `${getPositionFromTime(end)}px` declaration.","tags":["Configuration","Input Validation","TypeScript","Syntax Error","Unexpected EOF"],"cluster_size":2,"cluster_min_similarity":0.9509819531752508,"cluster_avg_similarity":0.9509819531752508},{"project_ids":["11276"],"cluster_id":9,"group_ids":[4490412973,6674579870],"issue_titles":["Error: AbortError: The play() request was interrupted by a call to pause(). https://goo.gl/LdLk22","Error: AbortError: The play() request was interrupted by a new load request."],"root_cause_summaries":["Rapid user interactions trigger competing `replayer.play()` calls, one immediate, one deferred, causing the latter to abort the former.","Rapid play/pause clicks cause race condition; `replayer.play()` interrupted by `replayer.pause()`, triggering AbortError."],"transactions":["/explore/replays/:replaySlug/"],"title":"Media play() aborted by pause/load interrupts","description":"Browser media playback requests are being aborted because play() is interrupted by either pause() or a subsequent load() call, causing AbortError in the client.","tags":["API","Client-Side","Media Playback","AbortError"],"cluster_size":2,"cluster_min_similarity":0.9636275579513075,"cluster_avg_similarity":0.9636275579513075},{"project_ids":["11276"],"cluster_id":12,"group_ids":[4558955339,5742831308,6649354464,6797754290],"issue_titles":["Error: NotAllowedError: play() failed because the user didn't interact with the document first. https://goo.gl/xX8pDD","Error: NotAllowedError: play() failed because the user didn't interact with the document first.","NotAllowedError: The request is not allowed by the user agent or the platform in the current context, possibly because the user denied permission."],"root_cause_summaries":["rrweb's media element state synchronization attempts programmatic playback, triggering browser autoplay policy blocks and NotAllowedError.","Video replay's automatic time-seek on page load triggers browser's autoplay block due to no prior user interaction.","Replay autoplay initiated programmatically without user interaction, violating browser policy, causing NotAllowedError.","rrweb's rrdom attempts to autoplay recorded media elements without user interaction, triggering browser's NotAllowedError."],"transactions":["/issues/feedback/","/issues/:groupId/"],"title":"Autoplay blocked: media play() without user interaction","description":"Replay replayer attempts to call HTMLMediaElement.play() during diffing without prior user interaction, and the browser blocks it with NotAllowedError. This prevents media from starting during session replay until a user gesture occurs.","tags":["API","Client-Side","Browser Policy","HTMLMediaElement","NotAllowedError","Autoplay Blocked","rrweb Replayer"],"cluster_size":4,"cluster_min_similarity":0.9313981866838063,"cluster_avg_similarity":0.9580310446798465},{"project_ids":["1"],"cluster_id":16,"group_ids":[5121405974,6275921240,6428172190],"issue_titles":["HighCardinalityWidgetException: Cardinality exceeded for dashboard_widget_query:83425 with count:16619 and column:account_id","HighCardinalityWidgetException: Cardinality exceeded for dashboard_widget_query:297759 with count:14747 and column:http.url","HighCardinalityWidgetException: Cardinality exceeded for dashboard_widget_query:937455 with count:14922 and column:measurements.ttfb"],"root_cause_summaries":["Dashboard widget query's http.url column exceeded 10,000 unique values, triggering cardinality limit exception.","Continuous timing metric `measurements.ttfb` exceeded cardinality limit, disabling on-demand extraction due to design mismatch.","Widget query's `account_id` column exceeded 10,000 unique values, triggering high-cardinality exception to prevent metric system overload."],"transactions":["sentry.tasks.on_demand_metrics.process_widget_specs"],"title":"Dashboard widget queries exceed cardinality limits","description":"HighCardinalityWidgetException occurs when dashboard widget queries return too many distinct values for columns like http.url, measurements.ttfb, and account_id, breaching configured cardinality thresholds.","tags":["Data Integrity","Configuration","API","Dashboard","Cardinality Limit","HighCardinalityWidgetException"],"cluster_size":3,"cluster_min_similarity":0.9501235162510326,"cluster_avg_similarity":0.9573368004633362},{"project_ids":["1","6178942"],"cluster_id":17,"group_ids":[5136898062,6724789302,6734977390,6734977392,6757725558,6759240917,6776543970],"issue_titles":["Retriable: [Errno 111] Connection refused","OperationalError: [Errno 111] Connection refused"],"root_cause_summaries":["RabbitMQ connection refused when `process_subscription` attempted to enqueue a follow-up task, indicating broker unavailability during execution.","Taskworker's Celery component fails to connect to an inaccessible RabbitMQ broker at 10.0.0.1:5672, preventing task queuing.","AMQP broker at 10.0.0.1:5672 refused connection during task queuing, preventing Celery worker from sending new tasks.","Celery worker failed to connect to RabbitMQ at `10.0.0.1:5672` due to connection refused, preventing task dispatch.","RabbitMQ broker at 10.0.0.1:5672 refused connection, preventing task enqueuing and causing secondary errors.","Taskworker process failed to connect to Celery's RabbitMQ broker due to network/configuration issues, preventing task enqueueing.","RabbitMQ broker at 10.0.0.1:5672 is unreachable or not accepting connections, causing Celery beat to fail."],"transactions":["getsentry.tasks.create_invoices.process_subscription","ingest_consumer.process_event","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.tasks.post_process.post_process_group","kombu.connection in _reraise_as_library_errors"],"title":"Celery tasks fail to enqueue: broker connection refused","description":"Multiple Celery producers and workers cannot connect to the message broker (py-amqp/AMQP), causing task submission and scheduling to fail and triggering secondary errors in post-processing and app webhooks. Impact: events and post-process jobs are not dispatched while the broker is unreachable.","tags":["Queueing","Networking","External System","Connection Refused","Celery","AMQP","Broker Unavailable"],"cluster_size":7,"cluster_min_similarity":0.9277233348322661,"cluster_avg_similarity":0.9521482302397088},{"project_ids":["1"],"cluster_id":21,"group_ids":[5283455347,6670041531,6737975011],"issue_titles":["ProtocolError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))"],"root_cause_summaries":["Vroom service or network intermediary prematurely closes TCP connection during file streaming, causing `ConnectionResetError`.","Filestore service or network infrastructure abruptly terminates connection during data streaming, causing `ConnectionResetError`.","Filestore service abruptly terminates connections during large debug file transfers, causing `ConnectionResetError`."],"transactions":["sentry.tasks.assemble.assemble_artifacts","getsentry.filestore in _get_file"],"title":"Filestore stream resets during artifact reads","description":"Artifact assembly and debug file reads fail when streaming chunks from the filestore, as the remote peer resets the TCP connection mid-transfer. This causes ProtocolError/ConnectionResetError in assemble_artifacts and artifact lookup paths.","tags":["Networking","API","External System","Connection Reset","HTTP Streaming","TCP","Filestore"],"cluster_size":3,"cluster_min_similarity":0.9448373550800436,"cluster_avg_similarity":0.9559688520582688},{"project_ids":["1"],"cluster_id":24,"group_ids":[5360930327,5360933462,5941478777,6016575103,6623690215,6646104327,6646530763,6662995585,6675765299,6675774000,6675809639,6675832959,6676253500,6676253511,6712110232,6713214240,6735551077,6789207903,6792499567,6792968209],"issue_titles":["ReadTimeoutError: HTTPConnectionPool(host='192.168.208.181', port=8080): Read timed out. (read timeout=5)","ReadTimeoutError: HTTPConnectionPool(host='192.168.208.181', port=8080): Read timed out."],"root_cause_summaries":["Filestore service fails to return debug file chunk within 5-second read timeout, causing immediate `ReadTimeoutError` due to no retries.","Internal filestore service at `192.168.208.181:8080` failed to respond within 5 seconds, causing a `ReadTimeoutError` during attachment retrieval.","Filestore service at 192.168.208.181:8080 failed to respond within 5 seconds, causing read timeout.","Filestore service's slow blob retrieval exceeds 5-second read timeout, causing bundle assembly failure.","Filestore server failed to serve artifact bundle within 5-second read timeout, causing task failure without retries.","Filestore service DELETE requests time out, causing task failures due to explicit non-retry policy for read timeouts.","Internal filestore service at 192.168.208.181:8080 failed to respond within 5 seconds during dsym file retrieval, causing a read timeout.","Filestore client's 5-second read timeout is too short for streaming large artifact data from the internal service.","External filestore service at 192.168.208.181:8080 failed to respond within 5-second timeout during attachment upload.","Filestore service read timeout due to large dSYM uploads exceeding the 5-second configured limit.","Filestore service failed to respond within 5-second read timeout, preventing 1.6MB attachment upload, due to service overload or network latency.","Artifact index read timed out due to large file size exceeding the fixed 5-second filestore timeout, with no retries configured for read timeouts.","External filestore service failed to respond within 5-second read timeout during attachment upload.","Filestore service at 192.168.208.181:8080 unresponsive, causing 5-second read timeout during chunk upload.","Filestore service read timeout during chunk retrieval, exacerbated by a 5-second timeout and no retries on read errors.","Filestore service at `192.168.208.181:8080` failed to respond within 5 seconds, causing a read timeout during attachment upload.","5-second read timeout for streaming large files from filestore is too aggressive for distributed environment.","Filestore service at `192.168.208.181:8080` failed to respond within 5 seconds, causing a `ReadTimeoutError`.","Filestore service failed to deliver file blob within 5-second timeout, causing download failure.","Filestore service read timeout during artifact bundle assembly, due to filestore unresponsiveness or network latency, exacerbated by no-retry policy."],"transactions":["sentry.debug_files.tasks.backfill_artifact_bundle_db_indexing","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/attachments/{attachment_id}/","getsentry.filestore in _get_file","ingest_consumer.process_event","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/files/dsyms/","sentry.tasks.store.save_event_attachments","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/{version}/files/","sentry.tasks.assemble.assemble_artifacts","sentry.tasks.assemble.assemble_dif","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/artifact-lookup/","getsentry.filestore in increment","/api/0/organizations/{organization_id_or_slug}/chunk-upload/"],"title":"Filestore reads time out during artifact/attachment streaming","description":"Requests to the filestore client time out while streaming artifact and attachment blobs (via urllib3/HTTPConnectionPool) from filestore._get_file, causing read failures in artifact lookup and event attachment endpoints.","tags":["Networking","API","Disk/Storage","Timeout","HTTP","urllib3","Filestore"],"cluster_size":20,"cluster_min_similarity":0.9088265084861712,"cluster_avg_similarity":0.9509669424972177},{"project_ids":["11276"],"cluster_id":27,"group_ids":[5569446707,5703216890],"issue_titles":["NotAllowedError: The request is not allowed by the user agent or the platform in the current context, possibly because the user denied permission."],"root_cause_summaries":["Browser autoplay policy blocks `video.play()` when `handleSegmentEnd` attempts automatic segment transition, as it's not a direct user gesture.","Browser autoplay policy blocks automatic video segment transitions not directly user-initiated, causing NotAllowedError."],"transactions":["/issues/:groupId/replays/","/explore/replays/:replaySlug/"],"title":"Browser blocks HTMLVideoElement.play() autoplay","description":"Calls to video.play() in videoreplayer.tsx are rejected with NotAllowedError due to browser autoplay/permission policies, likely because playback was not triggered by a user gesture. This prevents segments from starting after setVideoTime/handleSegmentEnd.","tags":["API","Client-Side","Input Validation","HTML Media","Autoplay Policy","NotAllowedError"],"cluster_size":2,"cluster_min_similarity":0.9690373431709615,"cluster_avg_similarity":0.9690373431709615},{"project_ids":["300688"],"cluster_id":22,"group_ids":[5630924517,6741537450,6741558891,6741561898,6741567098,6741567948,6741569424,6741576355,6741580490,6741587743],"issue_titles":["ConsumerError: KafkaError{code=UNKNOWN_TOPIC_OR_PART,val=3,str=\"Subscribed topic not available: transactions: Broker: Unknown topic or partition\"}","<unknown>"],"root_cause_summaries":["Kafka broker lacks the `ingest-replay-events` topic, preventing consumer subscription and causing `UnknownTopicOrPartition` error.","Kafka 'transactions' topic missing; consumer failed to poll due to topic not being created by `snuba bootstrap`.","Kafka broker reports `UnknownTopicOrPartition` because `processed-profiles` topic is missing or inaccessible in the Kafka cluster.","Kafka broker lacks 'profiles-call-tree' topic, preventing consumer subscription and causing 'UnknownTopicOrPartition' error.","Kafka topic 'snuba-profile-chunks' does not exist on the broker, preventing consumer subscription.","Kafka consumer failed to subscribe because the `snuba-queries` topic was not found on the Kafka broker.","Kafka consumer fails because the `snuba-items` topic is not found on the configured Kafka brokers.","Kafka consumer failed to subscribe to 'events' topic; topic either missing or inaccessible to the consumer.","Kafka consumer failed due to `outcomes` logical topic mapping to non-existent `outcomes-billing` physical topic.","Kafka consumer failed to subscribe because the 'loadbalancer-outcomes' topic was unknown or unavailable to the broker."],"transactions":["[cli init] consumer"],"title":"Kafka consumers fail on unknown topics/partitions","description":"Consumers using librdkafka report UnknownTopicOrPartition for subscribed topics (e.g., events, loadbalancer-outcomes, outcomes-billing, processed-profiles), indicating missing or misconfigured Kafka topics/partitions.","tags":["Queueing / Messaging","Configuration","Apache Kafka","UnknownTopicOrPartition","Consumer Poll Error"],"cluster_size":10,"cluster_min_similarity":0.9515885255511038,"cluster_avg_similarity":0.9684103466321325},{"project_ids":["1"],"cluster_id":33,"group_ids":[5667898811,6603107060,6612643023,6646325703,6673502666,6705203676,6711459865,6713340108,6713580065,6725380135,6728535328,6728535352,6728535387,6731108040,6738828471,6740952068,6766504328,6779784822,6779784853,6792379115,6792751930,6793674979,6794440815,6794593175,6794593241,6794593252,6794593299,6794593325,6794593328,6794593337,6794593358,6794593402,6794593465,6794593523,6794593562,6794593954,6794670740,6794670909,6794671036,6794677726,6795050820,6795050852,6798275869,6801739511,6801739523,6801739543,6802615434,6805523932],"issue_titles":["RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 103 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 52 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 18 exceeds limit of 16', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 104 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 102 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 17 exceeds limit of 16', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 26 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 72 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 22 exceeds limit of 18', 'overrides': {}, 'storage_key': 'S..."],"root_cause_summaries":["Bulk issue actions create concurrent workflow notification tasks, each independently querying Snuba, exceeding limits and triggering retries.","Bulk issue operations fan out into excessive concurrent Snuba queries, exceeding the concurrent policy limit.","Concurrent Slack notifications for activities without pre-populated event data trigger excessive Snuba queries, exceeding rate limits.","Slack notifications synchronously query Snuba for replay counts, exceeding concurrent limits during issue surges.","Notification spike caused 51 concurrent Snuba queries for latest event data, exceeding the 50-query limit, exacerbated by backend instability.","Concurrent regression notifications trigger excessive Snuba queries, exceeding the concurrency limit.","Notification digest's concurrent Snuba user count queries exceed referrer guard rail policy, causing rate limit errors.","Multiple Slack notification recipients concurrently fetch the same event data from Snuba, exceeding the query rate limit.","Concurrent Slack notification processing triggers excessive unbatched Snuba queries for 'users affected' data, exceeding the referrer guardrail.","Slack activity notifications trigger non-essential Snuba replay queries, exceeding concurrent limits and failing notification delivery.","Concurrent Sentry App workflow notifications individually query Snuba for issue stats, exceeding its concurrent query limit.","Issue summary generation's sequential Snuba queries exceed concurrency limits under concurrent load.","Multiple alert rules triggered by one event concurrently query Snuba for replay data, exceeding the rate limit.","Multiple Sentry App workflow notifications concurrently query Snuba for issue stats, exceeding the query rate limit.","High issue activity and multiple Sentry App installations cause concurrent Snuba queries to exceed rate limits.","High volume of issue events, multiplied by Sentry App integrations, causes concurrent Snuba queries during group serialization to exceed limits.","High volume of issue events triggers uncached Snuba user count queries, exceeding concurrent limits, causing task failures.","Concurrent `workflow_notification` tasks individually query Snuba for issue stats, exceeding the concurrent query limit for `errors_ro` storage.","Digest processing triggers N+1 Snuba queries for suspect committers, exceeding concurrent query limits.","Slack notifications for existing issues concurrently query Snuba for user counts, exceeding the concurrent query limit.","Concurrent Snuba user count queries, triggered by high-volume issue creation and Sentry App webhook processing, exceed Snuba's concurrency limit.","Concurrent issue serialization Snuba queries exceed rate limits due to high notification volume.","High event volume creates excessive concurrent Snuba queries for Slack notifications, exceeding Snuba's concurrent query limit.","Slack digest notifications trigger N+1 Snuba queries for user counts, exceeding the concurrent query limit when many issues are present.","Concurrent workflow notification tasks, due to cache misses, overwhelm Snuba's query concurrency limits.","Concurrent activity notification tasks issue too many Snuba queries, exceeding the concurrent query limit.","Concurrent assignment notifications trigger excessive Snuba queries for replay counts, exceeding the concurrent query limit.","Concurrent Sentry App webhook tasks, triggered by new issues, overwhelmed Snuba's query limit while fetching group user counts.","Workflow notification tasks concurrently serialize issues, triggering expensive Snuba queries, exceeding rate limits, and causing retries to amplify load.","Issue serialization's implicit Snuba query for 'seen stats' causes concurrent rate limit breaches when many issues are processed simultaneously.","Issue serialization in `workflow_notification` makes two unbatched Snuba queries per issue, exceeding concurrent limits under load.","Concurrent single-issue digest processing triggers excessive `Group.get_latest` Snuba queries, exceeding the referrer guardrail policy limit.","Concurrent performance alert notifications trigger excessive `Group.get_latest` Snuba queries, exceeding the `ReferrerGuardRailPolicy` limit.","Slack notification generation redundantly queries Snuba for each recipient, exceeding concurrent query limits due to unoptimized data fetching.","Slack notification generation synchronously queries Snuba for user counts, exceeding concurrent query limits during bursts.","Concurrent digest notifications trigger too many Snuba user count queries, exceeding the 50-query concurrency limit.","Slack notification generation repeatedly queries Snuba for identical user counts per recipient, exceeding concurrent query limits.","Concurrent `workflow_notification` tasks each query Snuba individually for issue stats, exceeding its concurrent query limit.","Concurrent digest tasks saturate Snuba's `slack_issue_notification` referrer, exceeding its 50-query limit.","Slack notification generation queries Snuba per recipient, causing concurrent query burst exceeding rate limit.","High issue creation rate spawns concurrent webhook tasks, each querying Snuba for user counts during group serialization, exceeding Snuba's concurrent query limit.","Regression notifications trigger excessive concurrent Snuba queries for latest event data, exceeding the rate limit.","Slack notifications' excessive concurrent Snuba user-count queries overwhelm Snuba's concurrency limit.","Concurrent Sentry App webhook processing triggers excessive Snuba user count queries, exceeding Snuba's concurrent query limit.","MS Teams notification generation implicitly queries Snuba for event data, exceeding concurrent query limits under high event volume.","Bulk issue ignore operations trigger excessive concurrent Snuba queries, exceeding rate limits, amplified by task retries.","Bulk issue operations dispatch individual tasks, each triggering a concurrent Snuba query for serialization, exceeding the Snuba concurrency limit.","Webhook fan-out creates concurrent tasks, each serializing the same issue, leading to a thundering herd of identical Snuba queries exceeding rate limits."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.sentry_apps.tasks.sentry_apps.workflow_notification","sentry.tasks.post_process.post_process_group","sentry.utils.snuba in _bulk_snuba_query","/api/0/issues|groups/{issue_id}/","sentry.tasks.digests.deliver_digest","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","ingest_consumer.process_event","sentry.tasks.activity.send_activity_notifications"],"title":"Snuba concurrent query limits blocking Slack notifications","description":"Snuba rejects concurrent queries from notification workflows (issue alerts, digests, activity) due to allocation policies, including ConcurrentRateLimitAllocationPolicy and ReferrerGuardRailPolicy. This prevents building Slack notification context (e.g., user counts and replay links) when query concurrency exceeds limits.","tags":["Rate Limiting","API","Configuration","Snuba","Concurrent Queries","Referrer Guard Rail","Slack Notifications"],"cluster_size":48,"cluster_min_similarity":0.9347477544747829,"cluster_avg_similarity":0.9603017534149954},{"project_ids":["1","300688"],"cluster_id":39,"group_ids":[5833133471,6579990516,6626412431,6651450233,6686967912],"issue_titles":["QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_2' in table 'events': While processing events._snuba_gen_2 AS _snuba_gen_2: While processing SELECT count() AS _snuba_count FROM (SELECT (positionCaseInsensitive(message AS `_snuba_events.message`, 'Ce...","QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_3' in table 'events': While processing events._snuba_gen_3 AS _snuba_gen_3: While processing SELECT count() AS _snuba_count FROM (SELECT ifNull(tags.value[indexOf(tags.key, 'runtime.name')] AS `_snuba_...","ClickhouseError: DB::Exception: There's no column 'events._snuba_gen_2' in table 'events': While processing events._snuba_gen_2 AS _snuba_gen_2: While processing SELECT events.`_snuba_events.time` AS `_snuba_events.time`, count() AS _snuba_count FROM (SELECT toDateTime(...","ClickhouseError: DB::Exception: There's no column 'ga._snuba_gen_2' in table 'ga': While processing ga._snuba_gen_2 AS _snuba_gen_2: While processing SELECT events.`_snuba_events.tags[pageName]` AS `_snuba_events.tags[pageName]`, `_snuba_events.tags[pageName]`, count() ...","QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_3' in table 'events': While processing events._snuba_gen_3 AS _snuba_gen_3: While processing SELECT events.`_snuba_events.time` AS `_snuba_events.time`, count() AS _snuba_count FROM (SELECT toDateTime(..."],"root_cause_summaries":["ClickHouse query fails because a pushed-down condition's generated alias was not added to the subquery's SELECT clause.","SnQL query builder generates unaliased boolean expressions in subquery SELECT, causing ClickHouse to auto-name columns, leading to reference failure.","Snuba's query generator failed to alias a complex boolean expression in a subquery, causing ClickHouse to report a missing column.","Snuba query generation incorrectly aliases a joined table's derived column to the primary table, causing a missing column error.","Snuba's ClickHouse SQL translation fails to alias complex boolean expressions in subqueries, causing missing column errors."],"transactions":["/api/0/organizations/{organization_id_or_slug}/alert-rules/{alert_rule_id}/","snql_dataset_query_view__events__api.organization-event-stats","snql_dataset_query_view__events__api.dashboards.tablewidget","/api/0/organizations/{organization_id_or_slug}/events-stats/","/api/0/organizations/{organization_id_or_slug}/events-meta/"],"title":"ClickHouse queries fail on generated column aliases","description":"Snuba-generated queries reference ephemeral aliases (e.g., events._snuba_gen_2/_3, ga._snuba_gen_2) as if they were physical columns, causing QueryMissingColumn errors in ClickHouse across events and ga tables. This breaks stats and alert queries in production until query generation/validation is fixed.","tags":["Database","API","ClickHouse","Snuba","QueryMissingColumn","Schema/Column Reference"],"cluster_size":5,"cluster_min_similarity":0.9251035895214341,"cluster_avg_similarity":0.9456068672375301},{"project_ids":["300688","1"],"cluster_id":40,"group_ids":[5836292937,6587571780,6587571802,6678807058,6776701806,6776780661,6776789695],"issue_titles":["QueryException: Code: 202. DB::Exception: Too many simultaneous queries. Maximum: 100.\"","QueryException: Code: 202. DB::Exception: Received from snuba-errors-tiger-mz-2-5:9000. DB::Exception: Too many simultaneous queries. Maximum: 200.\"","SnubaRPCError: code: 500","ClickhouseError: DB::Exception: Received from snuba-transactions-tiger-mz-1-2:9000. DB::Exception: Too many simultaneous queries. Maximum: 100. Stack trace:"],"root_cause_summaries":["ClickHouse's 100-query limit exceeded by Snuba's concurrent requests, amplified by cache misses and retry logic.","Trace-meta endpoint's nested concurrency amplifies Snuba queries, exceeding ClickHouse's concurrent query limit under load.","Snuba's `AttributeValuesRequest` lacks concurrency control, overwhelming ClickHouse's simultaneous query limit.","ClickHouse's global concurrent query limit exceeded by aggregate load, despite Snuba's per-tenant limits being respected.","Snuba's lack of global concurrency limits, combined with non-robust Clickhouse query execution, overloads Clickhouse's simultaneous query capacity.","ClickHouse query limit exceeded by aggregate Snuba services, exhausting retries.","Concurrent Sentry RPCs and Snuba's trace item query amplification overwhelm ClickHouse's simultaneous query limit."],"transactions":["EndpointGetTraces__v1","EndpointTimeSeries__v1","AttributeValuesRequest__v1","EndpointTraceItemTable__v1","[cli init] subscriptions-executor","snql_dataset_query_view__transactions__weekly_reports.key_transactions.this_week","/api/0/organizations/{organization_id_or_slug}/trace-meta/{trace_id}/"],"title":"ClickHouse limits trigger 'Too many simultaneous queries'","description":"Multiple Snuba query paths (errors, transactions, traces) are failing with ClickHouse 'Too many simultaneous queries' errors, indicating the cluster is hitting its concurrent query cap and rejecting requests.","tags":["Database","Queueing / Messaging","Resource Limits","ClickHouse","Too Many Simultaneous Queries","Snuba"],"cluster_size":7,"cluster_min_similarity":0.9298793842081059,"cluster_avg_similarity":0.9520693260439951},{"project_ids":["1"],"cluster_id":41,"group_ids":[5864599929,6646085496,6646172991,6734977576,6782644221,6792450203,6800373349,6806357457],"issue_titles":["RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 17 exceeds limit of 16', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer..."],"root_cause_summaries":["Ineffective Redis event counter for low snooze user count thresholds causes excessive concurrent Snuba queries, exceeding rate limits.","Ineffective caching in `test_user_counts` for low `user_count` values causes excessive concurrent Snuba queries, exceeding limits.","Concurrent `process_snoozes` tasks bypass Redis cache, triggering excessive Snuba queries, exceeding its concurrency limit.","Concurrent Snuba queries for snooze user counts, triggered by cache invalidation from high event volume, exceed Snuba's rate limit.","Concurrent `process_snoozes` tasks trigger excessive Snuba queries with identical referrer, exceeding Snuba's concurrent query limit.","Concurrent `is_escalating` Snuba queries from `post_process_group` tasks exceed the 23-query limit, causing `RateLimitExceeded`.","Concurrent tasks' short-TTL cache misses trigger Snuba concurrent query limit.","High event volume for ignored-until-escalating groups causes excessive concurrent Snuba queries, exceeding rate limits."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.utils.snuba in _bulk_snuba_query"],"title":"Snuba concurrent query limits blocking post-process checks","description":"Post-processing tasks (snooze validation and issue escalation checks) are failing because Snuba rejects queries under concurrent query allocation and referrer guard-rail policies. Excessive or inefficient queries from these features exceed concurrency limits, causing RateLimitExceeded errors.","tags":["Rate Limiting","API","Configuration","Snuba","Concurrent Queries","Referrer Guard Rail","RateLimitExceeded"],"cluster_size":8,"cluster_min_similarity":0.9462040594651066,"cluster_avg_similarity":0.9645156232868802},{"project_ids":["300688"],"cluster_id":45,"group_ids":[5977439441,5977440881,6727399980,6727400152,6741334049,6741334054,6752845185,6752845300,6752846957],"issue_titles":["<unknown>"],"root_cause_summaries":["Kafka broker at `192.168.223.237:9092` refused connection, preventing Snuba consumer startup due to `DEFAULT_BROKERS` misconfiguration.","Kafka client failed to establish TCP connection to broker `kafka-events-1.us-east1-c.c.sentry-s4s2.internal:9092` due to network timeout.","Kafka broker connection refused; the configured broker is unreachable or not listening on the expected port.","Kafka consumer failed to connect to broker due to incorrect configuration or network issues.","Kafka broker at 192.168.208.247:9092 refused connection, indicating broker unavailability or network misconfiguration preventing service access.","Kafka consumer failed to connect to broker 192.168.223.237:9092 due to a connection timeout.","Kafka group coordinator connection refused during consumer rebalance, causing processing termination.","Kafka broker connection refused; Arroyo consumer cannot connect, leading to strategy termination.","Kafka broker at 192.168.223.237:9092 refused connection during consumer rebalance, indicating broker unavailability or network misconfiguration."],"transactions":[],"title":"Kafka consumers hit BrokerTransportFailure during connect","description":"librdkafka clients fail to establish connections to Kafka brokers, timing out or receiving connection refused errors while attempting bootstrap and topic endpoint connections. This disrupts message consumption across topics like kafka-outcomes and kafka-spans.","tags":["Networking","External System","Queueing","Apache Kafka","librdkafka","Connection Refused","Timeout"],"cluster_size":9,"cluster_min_similarity":0.9307758896103092,"cluster_avg_similarity":0.9513804790132188},{"project_ids":["1"],"cluster_id":49,"group_ids":[6062112104,6615866385,6618300755,6640678616],"issue_titles":["QueryExecutionError: DB::Exception: Cannot convert string 2025-08-13T00:00:00+00:00 to type DateTime: While processing (match(CAST(transaction_op, 'Nullable(String)') AS _snuba_transaction_op, '(?i)^.*http.*$') = 1) AND ((CAST(release, 'Nullable(String)') AS _snuba_release)...","QueryExecutionError: DB::Exception: Cannot convert string 2025-07-09T00:00:00+00:00 to type DateTime: While processing (granularity = 2) AND ((project_id AS _snuba_project_id) = 5407140) AND ((timestamp AS _snuba_timestamp) >= toDateTime('2025-06-09T20:00:00', 'Universal'))...","QueryExecutionError: DB::Exception: Cannot convert string 2025-08-12T00:00:00+00:00 to type DateTime: While processing ((environment AS _snuba_environment) = 'PRODUCTION') AND ((deleted = 0) AND (match(ifNull(tags.value[indexOf(tags.key, 'root_eta_name')] AS `_snuba_tags[ro...","QueryExecutionError: DB::Exception: Cannot convert string 2025-07-10T00:00:00+00:00 to type DateTime: While processing ((project_id AS _snuba_project_id) IN [6547458, 4503998692458496, 4504378447822848, 4504605676142592, 4506428155691008, 4507327662522368, 4507334448840704,..."],"root_cause_summaries":["ClickHouse DateTime values serialized with timezone, then reused in query, causing conversion failure.","ClickHouse query failed: timezone-aware timestamp strings from prior query results cannot convert to ClickHouse DateTime.","ClickHouse fails to convert timezone-aware timestamps from Snuba's initial query into `DateTime` for subsequent `IN` clause filtering.","ClickHouse `DateTime` values, serialized with timezone, fail conversion when used in subsequent `IN` clause."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events-stats/"],"title":"ClickHouse DateTime cast fails on string dates","description":"Snuba queries for organization event and metrics timeseries are sending string literals that ClickHouse cannot cast to DateTime, causing QueryExecutionError across Discover, Errors, and Metrics endpoints. This breaks top events/time series retrieval for production environments within the specified time windows.","tags":["Database","Serialization","API","ClickHouse","Snuba","Type Conversion","QueryExecutionError"],"cluster_size":4,"cluster_min_similarity":0.9641056424443695,"cluster_avg_similarity":0.9707143369241426},{"project_ids":["1"],"cluster_id":50,"group_ids":[6067414280,6617779764,6631057814,6656786701,6691913538,6700608733,6700608737,6730165872,6745241223,6745241236,6798458984],"issue_titles":["QueryExecutionError: DB::Exception: Unknown function isHandled: While processing (isHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-07-08T16:09:52', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-07-15T16:10:52', 'Universal')) AND ((project_i...","QueryExecutionError: DB::Exception: Unknown function notHandled: While processing ((CAST(release, 'Nullable(String)') AS _snuba_release) IN ['com.americanexpress.android.acctsvcs.uk.rc@7.21.1+150583', 'com.americanexpress.android.acctsvcs.us.rc@7.21.1+150574', 'com.american...","QueryExecutionError: DB::Exception: Unknown function isHandled: While processing (isHandled() = 1) AND ((project_id AS _snuba_project_id) = 5671321) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-08-14T09:37:16', 'Universal')) AND (_snuba_finish_ts < toDateTime('2...","QueryExecutionError: DB::Exception: Unknown function notHandled: While processing (notHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-07-07T07:03:50', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-07-14T07:04:50', 'Universal')) AND ((project...","QueryExecutionError: DB::Exception: Unknown function notHandled: While processing (notHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-07-13T07:01:50', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-07-14T07:02:50', 'Universal')) AND ((project...","QueryExecutionError: DB::Exception: Unknown function notHandled: While processing ((environment AS `_snuba_events.environment`) = 'production') AND (((deleted = 0) AND (assumeNotNull(group_id) NOT IN (6617307138, 6581688333, 6495264793, 6011162653, 6779152419, 6018998311, 6...","QueryExecutionError: DB::Exception: Unknown function notHandled: While processing ((client_timestamp AS _snuba_timestamp) >= toDateTime('2025-07-06T17:28:34', 'Universal')) AND (_snuba_timestamp < toDateTime('2025-07-06T18:28:34', 'Universal')) AND ((project_id AS _snuba_pr...","QueryExecutionError: DB::Exception: Unknown function notHandled: While processing ((release AS `_snuba_events.tags[sentry:release]`) IN ['new-moon@2.0.12+225', 'new-moon@2.0.12+60', 'new-moon@2.0.12+61', 'new-moon@2.0.12+62', 'new-moon@2.0.12+63', 'new-moon@2.0.12+64', 'new..."],"root_cause_summaries":["Sentry generates SnQL with `notHandled()` for `error.unhandled:True`, but ClickHouse lacks this function.","Snuba's `HandledFunctionsProcessor` fails to translate `notHandled()` in JOINed queries, sending an unknown function to ClickHouse.","Snuba's transactions dataset lacks the processor to translate `notHandled()` into ClickHouse-compatible SQL, causing query execution failure.","Sentry generates Snuba queries using `notHandled()` function, which ClickHouse does not recognize, causing query execution failure.","Query for `isHandled()` routed to transactions dataset, lacking function definition, causing ClickHouse error.","Sentry generates Snuba queries with non-existent ClickHouse functions `notHandled()` and `isHandled()` for `error.unhandled` filters.","Snuba's transactions dataset lacks `HandledFunctionsProcessor` configuration, causing `notHandled()` to be sent untranslated to ClickHouse, resulting in an unknown function error.","ClickHouse lacks `notHandled` UDF, causing query failure when Sentry translates `error.unhandled:true`.","Snuba's `notHandled` function, generated by Sentry, was not translated to ClickHouse due to processor's narrow column scope.","ClickHouse rejected query because `notHandled()` was untranslated, indicating a missing UDF or Snuba translation.","Snuba SDK fails to translate Sentry's internal `notHandled` function into ClickHouse-compatible SnQL during serialization."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events-meta/","/api/0/organizations/{organization_id_or_slug}/issues-count/","/api/0/organizations/{organization_id_or_slug}/events/","/api/0/organizations/{organization_id_or_slug}/events-stats/"],"title":"ClickHouse rejects isHandled/notHandled functions in Snuba queries","description":"Discover/metrics queries routed through Snuba fail with QueryExecutionError: Unknown function isHandled/notHandled in ClickHouse, indicating unsupported or unregistered UDFs in the query plan. This breaks organization events, stats, and meta endpoints over transactions/errors datasets.","tags":["Database","API","Configuration","ClickHouse","Snuba","Unknown Function","Discover"],"cluster_size":11,"cluster_min_similarity":0.9395465217494605,"cluster_avg_similarity":0.9586618697339298},{"project_ids":["1"],"cluster_id":52,"group_ids":[6165897402,6395607447,6400837299,6538388674,6614888006,6618877476,6621958996,6634136994,6635086777,6645269614,6651204535,6654045326,6662361418,6668218833,6668371640,6671080577,6671667090,6678063278,6678806968,6678806969,6678806972,6678806974,6678806990,6678807319,6678807322,6678807325,6678807327,6678807331,6693065852,6696521538,6702832103,6713072905,6714335722,6725886202,6726553082,6726586234,6727722616,6727723549,6728535332,6736191175,6736191178,6744802472,6744802473,6744802474,6749032392,6752741332,6774171552,6781032580,6792280045,6792465424,6793243103,6793972067,6794198699,6794201570,6794396371,6794457962,6794457963,6794457966,6794457974,6794586191,6795338662,6796085447,6796085449,6796085460,6803496078,6804554136,6812907205],"issue_titles":["RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 104 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 21 exceeds limit of 20', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","Retry: Task can be retried","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 25 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 119 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 154 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 187 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 157 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 131 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 21 exceeds limit of 20', 'policy': 'referrer_guard_rail_policy', 'refer...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 151 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 332 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 185 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 242 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 102 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 31 exceeds limit of 30', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 107 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","SnubaRPCError: code: 400","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'CrossOrgQueryAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 11 exceeds limit of 10', 'storage_key': 'StorageKey.GENERIC_METRI...","SnubaRPCError: code: 500","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 21 exceeds limit of 18', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 18 exceeds limit of 16', 'overrides': {}, 'storage_key': 'S...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 169 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 112 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref..."],"root_cause_summaries":["Snuba's concurrent query limit of 18 for errors dataset was exceeded by 19th query.","Dashboard table widget concurrency, amplified by multi-query dataset splitting and pagination, exceeds Snuba's concurrent query limit.","Concurrent Celery tasks, each with a thread pool, amplify Snuba queries, exceeding its global concurrency limit.","Unrecognized referrer and overfetching queries cause Snuba's concurrent query limit to be exceeded.","High event volume triggers concurrent service hook tasks, each querying Snuba, exceeding its concurrency limit.","Concurrent queries to Snuba's `search_issues` storage exceeded its configured limit, triggering a rate limit exception.","Comparison-based alert rules synchronously trigger additional Snuba queries, causing concurrent query limit exhaustion under high update volume.","Weekly report task enqueues too many concurrent Snuba queries, exceeding the `reports.outcomes` rate limit, causing `RateLimitExceeded` errors.","Multiple concurrent replay deletion tasks query Snuba, exceeding the `StorageKey.REPLAYS` concurrent query limit.","Weekly reports concurrently query Snuba for many projects using the same referrer, exceeding Snuba's concurrent query limit.","Concurrent `events-stats` requests trigger too many Snuba queries for custom measurement metadata, exceeding the concurrent query limit.","Trace endpoint's concurrent `IssueOccurrence.fetch_multi` calls amplify Snuba queries, exceeding concurrent rate limits.","Concurrent API requests trigger EventUser's aggressive Snuba query pattern, exceeding Snuba's concurrent query limit.","Weekly report task scheduling creates excessive concurrent Snuba queries for 'reports.outcomes', exceeding rate limits and triggering retries.","Concurrent Snuba queries from multiple sources exceeded the configured limit of 18.","Concurrent service hook tasks trigger excessive Snuba queries for group serialization, exceeding rate limits.","Concurrent weekly report tasks, using a shared Snuba referrer, exceeded Snuba's concurrent query limit, triggering rate limiting.","Alert rule processing generates too many concurrent Snuba queries for an unrecognized referrer, exceeding its default concurrency limit.","Concurrent weekly report tasks, each querying multiple projects with a shared Snuba referrer, exceed Snuba's concurrent query limit.","Concurrent API requests collectively exceed Snuba's global query limit, causing Snuba to reject queries and Sentry to error.","Concurrent alert rule processing tasks generate too many Snuba queries, exceeding the 100-query `referrer_guard_rail_policy` limit.","Frontend's concurrent requests to /tags/ endpoint, lacking `use_cache` parameter, bypass caching, exceeding Snuba's concurrent query limit.","Concurrent statistical detector tasks issue too many Snuba queries, exceeding the 10-query cross-organization concurrency limit.","Concurrent `prepare_organization_report` tasks, all querying Snuba with the same referrer, exceed Snuba's per-referrer concurrent query limit.","Uncontrolled concurrent AI summary generations flood Snuba, exceeding its concurrent query limit, causing rate limits.","Concurrent replay deletion tasks initiated Snuba queries, exceeding the 22-query concurrent limit on the 'replays' dataset.","Invalid Snuba referrer `tsdb-modelid:300.batch_alert_event_uniq_user_frequency` triggered an overly restrictive default rate-limiting policy, causing query rejection.","High new error group volume triggers excessive concurrent Snuba queries from Sentry App and Seer automation, exceeding concurrency limits.","Escalating issues feature's uncoordinated concurrent Snuba queries for forecasts and escalation checks exceed rate limits.","Multiple concurrent delayed processing tasks collectively exceeded Snuba's shared query concurrency limit for the alert frequency referrer.","Concurrent API requests to event details endpoint exceeded Snuba's 18-query concurrent limit for error data.","Concurrent alert rule processing tasks overwhelm Snuba's referrer-based concurrent query limit.","Multiple 'user.' filters in search queries trigger excessive Snuba query retries, exceeding concurrent query limits.","Service hook processing redundantly queries Snuba for the same group data, exceeding concurrent query limits.","Concurrent trend detection tasks overwhelm Snuba's 10-query concurrency limit, causing rate limit exceptions.","Concurrent query limit for EAP_ITEMS exceeded, due to runtime override enforcing allocation policy.","Concurrent statistical detector tasks independently issue Snuba queries, collectively exceeding the global concurrent query limit due to insufficient cross-task coordination.","Trace endpoint's concurrent Snuba queries overwhelm allocation policies, causing rate limits.","Nested query generation and high Celery worker concurrency overwhelm Snuba's concurrent query limit for a specific referrer.","Concurrent Celery tasks initiated too many Snuba queries for the same referrer, exceeding Snuba's concurrency limit.","Uncontrolled concurrent weekly report tasks overwhelm Snuba's referrer-based query limit, causing `RateLimitExceeded`.","Top events function's N+1 query pattern exceeds referrer's 20-concurrent-query limit by one.","Concurrent digest processing generates too many parallel Snuba queries for issue event counts, exceeding the referrer's concurrent query limit.","Uncoordinated concurrent Celery tasks making Snuba queries with shared referrer exceed Snuba's global concurrent query limit.","EAP items comparison queries exceed Snuba's default concurrent rate limit, causing query rejections.","Digest backend's unbounded record accumulation causes excessive Snuba queries, violating concurrent rate limits.","Default issue search queries all categories concurrently, multiplied by chunking, exceeding Snuba's concurrent query limit.","Concurrent alert rule evaluations generate too many Snuba queries, exceeding the referrer guard rail policy limit.","Concurrent statistical detector tasks exceed Snuba's `generic_metrics_distributions` query limit, causing rate limiting.","Uncontrolled concurrent `run_spike_projection` tasks from different organizations exceed Snuba's referrer-based query limit.","Concurrent service hook tasks, each serializing group data, collectively exceeded Snuba's concurrent query limit.","Concurrent statistical detector tasks collectively exceed Snuba's query concurrency limit for a specific referrer, causing rate limiting.","Weekly report generation's unthrottled concurrent Snuba queries for key transactions exceeded Snuba's referrer rate limit.","Concurrent Snuba queries from parallel forecast tasks exceed the 100-query limit, causing rate limiting.","Uncontrolled concurrent `prepare_organization_report` tasks overwhelm Snuba's `reports.outcomes` query concurrency limit, causing `RateLimitExceeded`.","Unrecognized Snuba referrer `tsdb-modelid:300.batch_alert_event_uniq_user_frequency` triggers restrictive `ReferrerGuardRailPolicy`, causing concurrent query limit exceedance.","Large organizations' spike projection queries, containing many projects, overwhelm Snuba's concurrency limits.","Unbounded concurrent report tasks flood Snuba with queries, exceeding its per-referrer concurrency limit, causing rate limit errors and retries.","Snuba's concurrent query limit of 20 for transactions was exceeded by 21 queries from concurrent dashboard widget requests.","Concurrent Snuba queries from event serialization and adjacent event fetching exceed the 18-query limit.","Snuba's concurrent query limit for 'search_issues' dataset exceeded due to high concurrent trace view requests.","Uncontrolled parallel weekly report tasks concurrently query Snuba, exceeding its referrer-based concurrency limit.","Weekly report generation concurrently schedules too many tasks, causing a burst of Snuba queries for the same referrer, exceeding its concurrent limit.","Concurrent weekly report tasks overwhelm Snuba's referrer guard rail, causing rate limits and retries to amplify the problem.","Delayed processing's concurrent `get_rate_bulk` calls, each chunking Snuba queries, collectively exceed Snuba's global concurrent query limit.","Spike protection tasks concurrently query Snuba, exceeding the concurrent query limit due to insufficient intra-batch concurrency control.","Concurrent statistical detector tasks exceed Snuba's 10-query `CrossOrgQueryAllocationPolicy` limit for `GENERIC_METRICS_DISTRIBUTIONS`."],"transactions":["sentry.tasks.post_process.post_process_group","sentry.rules.processing.delayed_processing","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","/api/0/organizations/{organization_id_or_slug}/issues/","/api/0/organizations/{organization_id_or_slug}/events-stats/","sentry.taskworker.retry in retry_task","sentry.integrations.source_code_management.tasks.pr_comment_workflow","sentry.tasks.weekly_escalating_forecast.generate_forecasts_for_projects","/api/0/issues|groups/{issue_id}/events/{event_id}/","/api/0/organizations/{organization_id_or_slug}/tags/","tasks.spike_protection.run_spike_projection","/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/replays/{replay_id}/","sentry.tasks.summaries.weekly_reports.prepare_organization_report","sentry.utils.snuba in _bulk_snuba_query","query_subscription_consumer_process_message","/api/0/organizations/{organization_id_or_slug}/events/{project_id_or_slug}:{event_id}/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/replays/{replay_id}/summarize/","sentry.sentry_apps.tasks.service_hooks.process_service_hook","sentry.tasks.statistical_detectors.detect_transaction_trends","/api/0/organizations/{organization_id_or_slug}/events/"],"title":"Snuba concurrent query rate limits rejecting requests","description":"Multiple endpoints issuing SnQL queries are being rejected by Snuba due to concurrent query limits and referrer guard rails (e.g., ERRORS_RO, TRANSACTIONS, SEARCH_ISSUES). Impact: API requests for events, traces, and alert calculations are denied when concurrency exceeds configured policies.","tags":["Rate Limiting","API","External System","Snuba","Concurrent Queries","Referrer Guard Rail","Quota Exceeded"],"cluster_size":67,"cluster_min_similarity":0.9159046890423749,"cluster_avg_similarity":0.9524824055513358},{"project_ids":["1"],"cluster_id":56,"group_ids":[6231514672,6614947840,6646064693,6662087212,6673265831,6673265906,6673265919,6673265963,6685953326,6687392777,6708211120,6708217704,6727507289,6775729385,6805538203,6811309728],"issue_titles":["SnubaError: After processing, query is 241600 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 241361 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 667909 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 231612 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 184660 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 241439 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 132465 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 133425 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 279015 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 140118 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 195736 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaRPCError: code: 500","SnubaError: After processing, query is 241459 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 139684 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 238043 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","SnubaError: After processing, query is 180861 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes."],"root_cause_summaries":["Querying all projects for a large organization generates an oversized ClickHouse query, exceeding its byte limit.","Querying all projects in large organizations expands into oversized ClickHouse queries, exceeding the byte limit.","Snuba query for 5000 issues with complex conditions exceeds ClickHouse's maximum query size limit.","SDK updates endpoint queries all organization projects, generating a Snuba query exceeding ClickHouse's size limit.","High release velocity generates excessive `IN` clause in Snuba query, exceeding ClickHouse's query size limit.","Snuba query for organization stats exceeds ClickHouse size limit due to unbounded project ID list in `IN` clause.","Unbounded function name extraction from Go file patch creates excessively large Snuba query, exceeding ClickHouse size limit.","Unbounded project selection for trace queries generates oversized ClickHouse queries, exceeding its byte limit.","High event volume creates oversized Redis digests, leading to Snuba generating ClickHouse queries exceeding byte limits.","Snuba query exceeds ClickHouse's size limit due to many group IDs and extensive function name conditions.","Pendo details endpoint queries all accessible projects, generating an oversized ClickHouse query exceeding its byte limit.","Snuba query for spike projections exceeds ClickHouse size limit due to including all 13,386 project IDs in a single IN clause.","Querying all projects grouped by project explicitly lists all project IDs, exceeding ClickHouse's query size limit.","Snuba query exceeds ClickHouse's size limit because `project_id IN (...)` clause contains too many project IDs.","Unbounded function name extraction from large code changes creates oversized Snuba queries, exceeding ClickHouse's maximum query size limit.","Snuba query size explodes due to many function changes, exceeding ClickHouse's byte limit."],"transactions":["/api/0/organizations/{organization_id_or_slug}/stats_v2/","/api/0/organizations/{organization_slug}/pendo-details/","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","sentry.utils.snuba in _bulk_snuba_query","/api/0/organizations/{organization_id_or_slug}/releases/","/api/0/organizations/{organization_id_or_slug}/traces/","/api/0/organizations/{organization_id_or_slug}/sdk-updates/","/api/0/organizations/{organization_id_or_slug}/issues/","/api/0/organizations/{organization_id_or_slug}/events-stats/","tasks.spike_protection.run_spike_projection","/api/0/organizations/{organization_id_or_slug}/stats/"],"title":"Snuba queries exceed ClickHouse max query size","description":"Several Snuba SNQL requests generated payloads larger than ClickHouses allowed query size, causing SnubaError failures across releases health, outcomes totals/timeseries, and commit context workflows. Impact: API endpoints and Celery jobs depending on these queries fail until queries are constrained or split.","tags":["Database","API","ClickHouse","Snuba","Payload Too Large"],"cluster_size":16,"cluster_min_similarity":0.9378780547947457,"cluster_avg_similarity":0.9597153800834718},{"project_ids":["1"],"cluster_id":58,"group_ids":[6269047879,6270044380,6270530195,6322016670,6532776250,6536481522,6536824994,6661333411,6722073890,6725448385,6751501001,6789839278],"issue_titles":["KafkaException: KafkaError{code=_DESTROY,val=-197,str=\"Commit failed: Local: Broker handle destroyed\"}","KafkaException: KafkaError{code=_DESTROY,val=-197,str=\"Failed to get committed offsets: Local: Broker handle destroyed\"}","ConsumerError: cannot stage offsets for unassigned partitions","KafkaException: KafkaError{code=UNKNOWN_MEMBER_ID,val=25,str=\"Commit failed: Broker: Unknown member\"}"],"root_cause_summaries":["Unhandled exception in `on_partitions_revoked` during Kafka rebalance invalidates consumer's member ID, causing subsequent offset commit to fail.","Multiprocessing pool failed to exit during Kafka rebalance, crashing `on_partitions_revoked`, causing consumer group membership loss.","Span flusher backpressure caused consumer shutdown, leading to Kafka commit failure due to unknown member ID during partition revocation.","Kafka consumer's member ID becomes null during concurrent shutdown and rebalance, causing commit failure.","SpanFlusher backpressure caused consumer to miss Kafka heartbeats, leading to session timeout and UNKNOWN_MEMBER_ID on commit.","Kafka rebalance leaves `RunTaskWithMultiprocessing`'s `__input_blocks` empty, causing `IndexError` on next message and subsequent `KafkaException`.","Kafka consumer lost group membership during rebalance due to exceeding `max.poll.interval.ms`, causing commit failure.","Kafka consumer's broker handle destroyed during rebalance, preventing pending offset commits for revoked partitions.","Frequent consumer restarts trigger Kafka rebalances, causing attempts to commit offsets for unassigned partitions.","Arroyo's `on_partitions_revoked` callback crashed during Kafka rebalance, corrupting consumer state, causing `UNKNOWN_MEMBER_ID` commit failures.","Kafka rebalance callback crashes due to unhandled backpressure, destroying broker handle, preventing clean shutdown.","Kafka consumer's `on_partitions_revoked` callback crashed during rebalance, causing `UNKNOWN_MEMBER_ID` due to prior backpressure-induced inactivity."],"transactions":["sentry.ingest.billing_metrics_consumer in poll","sentry.remote_subscriptions.consumers.result_consumer in commit_offsets","sentry.ingest.billing_metrics_consumer in join","sentry.consumers.dlq in join","sentry.utils.kafka in run_processor_with_signals","sentry.sentry_metrics.consumers.indexer.multiprocess in poll","sentry.consumers.validate_schema in join","replays.consumer.recording_buffered.process_message","sentry.spans.consumers.process.flusher in submit"],"title":"Kafka consumer rebalances causing UNKNOWN_MEMBER_ID errors","description":"Consumers in the basic_consumer pipeline are failing with Kafka UNKNOWN_MEMBER_ID and _DESTROY errors during group rebalances, leading to MessageRejected and downstream join/poll issues. Likely caused by stale group membership or session churn in the consumer group.","tags":["Queueing","Kafka","Consumer Group Rebalance","Unknown Member Id","Message Rejected"],"cluster_size":12,"cluster_min_similarity":0.9092620818827872,"cluster_avg_similarity":0.9464877185861587},{"project_ids":["11276"],"cluster_id":61,"group_ids":[6295096930,6361173395,6670618692],"issue_titles":["InternalServerError: GET /organizations/{orgSlug}/events-facets/ 500"],"root_cause_summaries":["IssuePlatform dataset lacks `get_facets` method, causing `AttributeError` when frontend requests facets for it.","`sentry.snuba.spans_rpc` lacks `get_facets` method, causing `AttributeError` when requested by API endpoint.","Frontend requested facets from backend's 'issue_platform' dataset, which lacks 'get_facets' method."],"transactions":["/insights/backend/summary/","/explore/discover/results/"],"title":"Server 500 on events-facets API for tag facets","description":"Frontend calls to GET /organizations/{orgSlug}/events-facets/ fail with InternalServerError, breaking tag facets loading in tags.tsx. Issue appears server-side while fetching tag facet data.","tags":["API","External System","Upstream Unavailable","HTTP 500","events-facets","Tags UI"],"cluster_size":3,"cluster_min_similarity":0.9759453223422505,"cluster_avg_similarity":0.9767133205456148},{"project_ids":["300688"],"cluster_id":62,"group_ids":[6318721129,6459533148,6587188045,6670944912,6670944944,6804264231],"issue_titles":["ClickhouseError: DB::Exception: Timeout exceeded: elapsed 25.058318138 seconds, maximum: 25. Stack trace:","QueryException: Code: 159. DB::Exception: Timeout exceeded: elapsed 49.170935964 seconds, maximum: 25.\"","QueryException: Code: 159. DB::Exception: Received from snuba-events-analytics-platform-2-1:9000. DB::Exception: Timeout exceeded: elapsed 25.007273758 seconds, maximum: 25.\"","ClickhouseError: DB::Exception: Timeout exceeded: elapsed 25.122292787 seconds, maximum: 25. Stack trace:","QueryException: Code: 159. DB::Exception: Timeout exceeded: elapsed 38.230702472 seconds, maximum: 25.\"","SubscriptionQueryException: Code: 159. DB::Exception: Timeout exceeded: elapsed 49.170935964 seconds, maximum: 25.\""],"root_cause_summaries":["ClickHouse query for EAP items times out due to complex aggregations on large data exceeding the 25-second `max_execution_time`.","ClickHouse query timed out due to excessive weighted percentile calculations for confidence intervals on large datasets.","Complex ClickHouse query on `eap_items` table exceeded its 25-second `max_execution_time` due to expensive map operations and large data volume.","ClickHouse query's complexity and data volume exceed its aggressive 25-second execution timeout.","ClickHouse query's complexity and data volume exceed its 25-second execution time limit, causing a server-side timeout.","Complex subscription query consistently exceeds its 25-second ClickHouse execution timeout, causing connection termination and cascading errors."],"transactions":["[cli init] subscriptions-executor","EndpointTraceItemTable__v1","EndpointTimeSeries__v1","EndpointTraceItemStats__v1"],"title":"ClickHouse query timeouts in Snuba read paths","description":"Multiple Snuba query executions to ClickHouse are exceeding the configured timeout, including remote queries on the events/analytics cluster, causing QueryException propagation through readthrough caching paths.","tags":["Database","External System","Timeout","ClickHouse","Snuba"],"cluster_size":6,"cluster_min_similarity":0.9429989750869087,"cluster_avg_similarity":0.9598388443476147},{"project_ids":["300688"],"cluster_id":66,"group_ids":[6362823872,6362854148,6369522795,6423840801,6459985725,6467395230,6556843867],"issue_titles":["ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 27.94 GiB, current bytes: 28.39 GiB: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 27.94 GiB, current bytes: 28.05 GiB: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 27.94 GiB, current bytes: 28.02 GiB: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 11.92 GiB, current bytes: 12.29 GiB: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 27.94 GiB, current bytes: 28.28 GiB: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 9.83 GiB, current bytes: 9.86 GiB: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Limit for rows or bytes to read exceeded, max bytes: 27.94 GiB, current bytes: 27.95 GiB: While executing Remote. Stack trace:"],"root_cause_summaries":["ClickHouse query exceeded `max_bytes_to_read` limit (11.92 GiB) set by `BytesScannedRejectingPolicy` due to restrictive `scan_limit` for `search_sample`.","Snuba's `BytesScannedRejectingPolicy` set a `max_bytes_to_read` limit too low for the query's data volume, causing ClickHouse to error.","Query exceeded `max_bytes_to_read` set by `BytesScannedRejectingPolicy` due to large data scan.","Query exceeded bytes-scanned quota, triggering Snuba's throttling policy, which set a lower ClickHouse `max_bytes_to_read` limit.","Allocation policy dynamically reduced ClickHouse's `max_bytes_to_read` below query's actual scan needs.","Query scanned more data than allowed by `BytesScannedRejectingPolicy`'s `max_bytes_to_read` limit, causing ClickHouse to error.","Dashboard query exceeded configured `max_bytes_to_read` limit (9.83 GiB) set by allocation policies, causing ClickHouse to abort."],"transactions":["snql_dataset_query_view__events__api.dashboards.widget.line-chart.find-topn","snql_dataset_query_view__events__api.organization-events-facets.top-tags","snql_dataset_query_view__events__tagstore.get_tag_value_paginator_for_projects","snql_dataset_query_view__events__search","snql_dataset_query_view__events__search_sample","snql_dataset_query_view__events__api.discover.query-table","snql_dataset_query_view__events__api.dashboards.tablewidget"],"title":"ClickHouse queries exceed read byte limits","description":"Remote ClickHouse queries are reading more data than the configured max bytes per query, causing ServerException errors during execution in the readthrough caching path. This likely affects large result sets or missing LIMIT/filters.","tags":["Database","Configuration","ClickHouse","Limit Exceeded","Remote Query"],"cluster_size":7,"cluster_min_similarity":0.9619106473283566,"cluster_avg_similarity":0.9701810153894892},{"project_ids":["6178942"],"cluster_id":68,"group_ids":[6378510348,6747618655],"issue_titles":["OperationalError: (psycopg.OperationalError) consuming input failed: server closed the connection unexpectedly"],"root_cause_summaries":["PostgreSQL server connection termination, combined with high-frequency, unhandled database polling in a background thread, causes task crash.","Long-running task's excessive database polling, combined with database connection timeouts, caused connection loss and task failure."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task"],"title":"PostgreSQL connections close during state fetch","description":"Calls to load DbRunState via SQLAlchemy/psycopg intermittently fail with 'server closed the connection unexpectedly', indicating the PostgreSQL backend terminated connections while pipeline steps read state.","tags":["Database","Networking","PostgreSQL","SQLAlchemy","Connection Reset"],"cluster_size":2,"cluster_min_similarity":0.9537861041321922,"cluster_avg_similarity":0.9537861041321922},{"project_ids":["1"],"cluster_id":70,"group_ids":[6423936526,6745973990,6746141553,6746141876,6792300053,6793548070,6798507974],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.rules.processing.delayed_processing","TimeoutError: The read operation timed out","ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.integrations.slack.tasks.post_message","ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.incidents.tasks.handle_trigger_action"],"root_cause_summaries":["Slack API connection timeout, combined with prior operations, exceeded the task's 60-second processing deadline.","Slack API call with large payload exceeded client's 30-second timeout, causing read operation to fail.","Slack integration lacks retry/timeout configuration, causing notification failures when Slack API response exceeds 30 seconds.","Slack API call times out due to large notification payload exceeding `slack_sdk`'s default 30-second timeout.","Slack message payload containing verbose Rust traceback exceeds task processing deadline, causing API call timeout.","Slack API call latency, exacerbated by large message payloads and no client-side timeout, caused task deadline overrun.","Slack API latency combined with a strict 30-second task deadline and no retries causes notification delivery failure."],"transactions":["sentry.rules.processing.delayed_processing","sentry.integrations.slack.tasks.post_message","sentry.incidents.tasks.handle_trigger_action"],"title":"Slack message posting times out in notification pipeline","description":"Calls to Slack chat.postMessage from the notification and delayed processing paths are timing out, leading to ProcessingDeadlineExceeded in Slack post_message and rules delayed processing tasks. This likely impacts issue alert delivery to Slack channels.","tags":["External System","Networking","Queueing","Timeout","Retries Exhausted","Slack","HTTP"],"cluster_size":7,"cluster_min_similarity":0.9329296380526498,"cluster_avg_similarity":0.9546421612770953},{"project_ids":["300688"],"cluster_id":75,"group_ids":[6463885877,6710686223],"issue_titles":["BadSnubaRPCRequestException: Invalid trace item filter, empty 'or' clause"],"root_cause_summaries":["Frontend filter construction generated an empty OR clause, causing Snuba to reject the invalid query.","Sentry's `_resolve_boolean_conditions` generates empty `OrFilter` protobufs, rejected by Snuba's filter validation."],"transactions":["EndpointTimeSeries__v1"],"title":"Snuba RPC rejects empty OR in trace filters","description":"Time-series queries build an invalid Snuba request when trace item filters contain an empty OR clause, causing BadSnubaRPCRequestException during request resolution.","tags":["API","Input Validation","Snuba","Invalid Query","Empty OR Clause"],"cluster_size":2,"cluster_min_similarity":0.9668829186918815,"cluster_avg_similarity":0.9668829186918815},{"project_ids":["1","300688"],"cluster_id":76,"group_ids":[6469038237,6746125289],"issue_titles":["QueryException: Code: 241. DB::Exception: Received from snuba-events-analytics-platform-2-2:9000. DB::Exception: Memory limit (for query) exceeded: would use 18.63 GiB (attempt to allocate chunk of 4289748 bytes), maximum: 18.63 GiB.: (while reading column attributes_s...","SnubaRPCError: code: 500"],"root_cause_summaries":["Snuba's trace query lacks a LIMIT clause, causing ClickHouse to sort all spans, exceeding memory.","ClickHouse query sorting large trace data by timestamp exceeds memory limit."],"transactions":["/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/","EndpointGetTrace__v1"],"title":"ClickHouse query exceeds per-query memory limit","description":"Trace and spans queries against Snuba ClickHouse tables (default.eap_items_1_local) are failing while reading large attribute columns, as the MergeTree read exceeds the configured per-query memory cap. This results in aborted requests for trace views in the analytics platform.","tags":["Database","Resource Limits","ClickHouse","Memory Limit Exceeded","MergeTree","Snuba"],"cluster_size":2,"cluster_min_similarity":0.9646282170536216,"cluster_avg_similarity":0.9646282170536216},{"project_ids":["1"],"cluster_id":79,"group_ids":[6520757747,6626381984,6714198304,6717661088,6765064624,6776183779,6781511608],"issue_titles":["NotImplementedError: Haven't handled all the search expressions yet"],"root_cause_summaries":["Search grammar allows `( and )`, parser creates `ParenExpression(['AND'])`, but resolver lacks logic for raw boolean operator strings.","Search resolver lacks logic for standalone boolean operators, causing `NotImplementedError` when parsing 'AND' or 'OR'.","Resolver lacks handling for single boolean operator queries, causing NotImplementedError.","Search resolver fails on single 'OR' query because it expects a filter or expression, not a standalone boolean operator.","Resolver fails on `( OR )` query; parser allows bare operator, resolver expects structured filter objects.","Search resolver lacks handling for standalone boolean operators, causing `NotImplementedError` when parsing semantically invalid queries.","Search resolver lacks semantic validation for standalone boolean operators, causing NotImplementedError for malformed queries like '( OR )'."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events-stats/","/api/0/organizations/{organization_id_or_slug}/events/","/api/0/organizations/{organization_id_or_slug}/traces/"],"title":"Search resolver not handling all boolean expressions","description":"Requests to organization events and traces endpoints trigger NotImplementedError in the query resolver when parsing complex search expressions, blocking RPC table and timeseries queries. The resolvers boolean-condition handler lacks support for some search terms/structures.","tags":["API","Input Validation","Configuration","Query Parsing","NotImplementedError","Search Resolver","Organization Events/Traces"],"cluster_size":7,"cluster_min_similarity":0.9438424343454506,"cluster_avg_similarity":0.961430938457464},{"project_ids":["300688"],"cluster_id":81,"group_ids":[6529961628,6593180692],"issue_titles":["QueryException: After processing, query is 923086 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes.","QueryException: After processing, query is 330207 bytes, which is too long for ClickHouse to process. Max size is 131072 bytes."],"root_cause_summaries":["Query size exceeds ClickHouse limit due to many project IDs, verbose attribute access, and reliability columns.","Formula reliability feature's recursive query expansion for complex expressions generated SQL exceeding ClickHouse's size limit."],"transactions":["EndpointTraceItemTable__v1","EndpointTimeSeries__v1"],"title":"ClickHouse queries exceed maximum query size","description":"Processed queries for time series and trace item tables are expanding beyond ClickHouses maximum allowed query length, causing QueryTooLongException failures. Likely due to excessive filters, selected columns, or inlined parameter lists during query construction.","tags":["Database","API","ClickHouse","Query Too Large"],"cluster_size":2,"cluster_min_similarity":0.9512059408554898,"cluster_avg_similarity":0.9512059408554898},{"project_ids":["1","300688"],"cluster_id":83,"group_ids":[6534357125,6640349922],"issue_titles":["<unknown>","ConsumerError: KafkaError{code=_MAX_POLL_EXCEEDED,val=-147,str=\"Application maximum poll interval (30000ms) exceeded by 341ms\"}"],"root_cause_summaries":["Kafka consumer exceeded `max.poll.interval.ms` (30s) due to blocking processing, causing broker disconnection and rebalance.","Transaction consumer processing exceeds Kafka's 30-second poll interval, causing group rebalances."],"transactions":["sentry.utils.kafka in run_processor_with_signals"],"title":"Kafka consumer max poll interval exceeded with broker transport failures","description":"Kafka consumers stop after exceeding the max poll interval while librdkafka reports repeated BrokerTransportFailure disconnects to the cluster. Likely connectivity or broker-side instability causes the consumer to miss heartbeats and trigger the max.poll.interval violation.","tags":["Queueing","Networking","Kafka","Broker Transport Failure","Max Poll Exceeded"],"cluster_size":2,"cluster_min_similarity":0.9597745863888849,"cluster_avg_similarity":0.9597745863888849},{"project_ids":["6178942"],"cluster_id":89,"group_ids":[6563595449,6679777526,6706317906],"issue_titles":["OperationalError: sending query and params failed: another command is already in progress","OperationalError: (psycopg.OperationalError) sending query and params failed: another command is already in progress"],"root_cause_summaries":["Autofix state's unbounded growth causes large JSON updates, exceeding Celery's soft time limit, leading to database connection errors.","Celery task's soft time limit (10m) is too short for large JSON database commits, causing `OperationalError` during rollback.","Unbounded progress data in JSON state object causes large database updates to timeout, leading to connection errors during rollback."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task"],"title":"PostgreSQL session reused while command in progress","description":"SQLAlchemy commits fail with psycopg 'another command is already in progress', indicating concurrent use of the same PostgreSQL connection/session during agent state updates and memory storage, often followed by task soft timeouts.","tags":["Database","Concurrency","API","PostgreSQL","SQLAlchemy","Another Command In Progress","Soft Time Limit Exceeded"],"cluster_size":3,"cluster_min_similarity":0.9661913688286989,"cluster_avg_similarity":0.9693742633830268},{"project_ids":["6178942"],"cluster_id":90,"group_ids":[6563597083,6705245544,6705245782,6705991754,6722899610,6735068379],"issue_titles":["OperationalError: (psycopg.OperationalError) connection failed: Connection refused"],"root_cause_summaries":["PostgreSQL server at 127.0.0.1:6432 refused connection, causing autofix task failure.","PostgreSQL server not running or listening on `127.0.0.1:6432`, causing connection refusal.","PostgreSQL server at 127.0.0.1:6432 refused connection, preventing database access for autofix pipeline.","PostgreSQL connection refused at 127.0.0.1:6432 during task execution, indicating database unavailability.","PgBouncer service at 127.0.0.1:6432 was down, preventing database connections for autofix state management.","PostgreSQL server became unreachable at 127.0.0.1:6432 during task execution, causing connection refusal for new database operations."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task","env_py in run_migrations_online"],"title":"PostgreSQL connections refused across Autofix pipeline","description":"Multiple components using SQLAlchemy/psycopg fail to connect to PostgreSQL with connection refused, and some sessions report pool invalidation or abrupt server closes. Likely database service is down or unreachable, impacting state reads/writes and migrations across the Autofix pipeline.","tags":["Database","Networking","Configuration","PostgreSQL","SQLAlchemy","Connection Refused","Invalidate Pool"],"cluster_size":6,"cluster_min_similarity":0.9241809512938447,"cluster_avg_similarity":0.9535426216760496},{"project_ids":["1"],"cluster_id":91,"group_ids":[6566869602,6678733236,6678899680],"issue_titles":["OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: FATAL:  no such database: default_replica"],"root_cause_summaries":["Read replica database `default_replica` at `127.0.0.1:6432` is inaccessible or non-existent, preventing organization lookup.","PostgreSQL server lacks 'default_replica' database, preventing Django's ORM from establishing a connection.","PostgreSQL server lacks 'default_replica' database, preventing application connection due to misconfiguration."],"transactions":["/","/js-sdk-loader/{public_key}{minified}.js"],"title":"PostgreSQL FATAL: missing 'default_replica' database","description":"Multiple Django middleware paths fail when connecting to PostgreSQL because the configured database 'default_replica' does not exist. Likely a misconfiguration or missing replica database causing OperationalError across org lookup and JS SDK loader endpoints.","tags":["Database","Configuration","PostgreSQL","OperationalError","No Such Database"],"cluster_size":3,"cluster_min_similarity":0.969797778326724,"cluster_avg_similarity":0.9710203474169523},{"project_ids":["300688","1"],"cluster_id":92,"group_ids":[6572934020,6724510724,6750580244,6750580263],"issue_titles":["QueryException: Code: 62. DB::Exception: Syntax error: failed at position 2455 ('-'): -message_TYPE_STRING), 'VPN is offline') OR xor(isNull(-message_TYPE_STRING), isNull('VPN is offline'))) AND (notEquals((if(mapContains(attributes_string_33, 'e. Expected identifier.\"","QueryException: Code: 62. DB::Exception: Syntax error: failed at position 1360 ('-'): -message_TYPE_STRING), '') OR isNull(-message_TYPE_STRING) AND isNull('')) AND lessOrEquals((if(mapContains(attributes_float_14, 'sentry.timestamp_precise'), ar. Expected identifier.\"","SnubaRPCError: code: 500","QueryException: Code: 62. DB::Exception: Syntax error: failed at position 119 ('-'): -message_TYPE_STRING) FROM eap_items_1_dist WHERE in(project_id, [11276]) AND equals(organization_id, 1) AND less(timestamp, toDateTime(1755261841)) AND greater. Expected identifier.\""],"root_cause_summaries":["Nested `SubscriptableReference` objects within `isNull` functions are not transformed, leading to invalid SQL rendering with a prepended hyphen.","Query generation creates invalid ClickHouse alias from hyphenated attribute name, causing a syntax error.","Snuba's SQL generation failed due to an unescaped hyphen in the '-message' field name, causing a ClickHouse syntax error.","ClickHouse query failed due to aliased expression `-message_TYPE_STRING` incorrectly used in WHERE clause, causing a syntax error."],"transactions":["EndpointTraceItemTable__v1","/api/0/organizations/{organization_id_or_slug}/events-stats/","AttributeValuesRequest__v1","EndpointTimeSeries__v1"],"title":"ClickHouse syntax errors from hyphenated column alias","description":"Queries referencing a generated field like -message_TYPE_STRING cause ClickHouse to fail with 'Expected identifier', breaking timeseries and trace item queries via Snuba. Likely due to a malformed alias or expression producing a leading hyphen in the column name.","tags":["Database","Serialization","ClickHouse","Syntax Error","Snuba"],"cluster_size":4,"cluster_min_similarity":0.9314224916792586,"cluster_avg_similarity":0.9504996387998604},{"project_ids":["1"],"cluster_id":97,"group_ids":[6580232623,6676253430,6693251294,6694162175,6694166946,6694172297,6766758388,6774521704,6774521980,6774521983,6774522064,6776345220,6776568971,6779524833,6781403230,6789408923,6791127767,6796277129,6803421956,6813041841],"issue_titles":["OSError: Project was not passed and could not be determined from the environment.","TransportError: Failed to retrieve http://metadata.google.internal/computeMetadata/v1/universe/universe_domain from the Google Compute Engine metadata service. Compute Engine Metadata server unavailable"],"root_cause_summaries":["Bigtable client fails to initialize due to missing explicit project ID and unreachable Compute Engine Metadata server.","Google Cloud Storage client failed to initialize due to `project_id=None`, triggering metadata server lookup that failed in non-GCP environment.","Bigtable client initialization fails due to missing GCP project ID, caused by misconfiguration and unavailable metadata server.","Bigtable client initialization fails due to unavailable Compute Engine Metadata server, preventing project ID auto-detection.","Bigtable client initialization fails; Google Cloud Project ID is missing and GCE metadata server is unreachable.","Bigtable nodestore backend lacks Google Cloud project ID, causing client initialization failure and subsequent AttributeError.","Bigtable nodestore backend misconfigured: missing `project` parameter during `BigtableKVStorage` initialization, causing `bigtable.Client` to fail.","Bigtable client fails to initialize; project ID is missing and metadata server is unreachable.","Bigtable client failed due to unreachable Compute Engine Metadata server, preventing project auto-detection.","Bigtable client fails to initialize due to missing project ID and unreachable Compute Engine Metadata server.","Bigtable client initialization fails because `project` is `None` and environment auto-detection fails.","Bigtable client initialization fails due to missing GCP project ID, as environment auto-detection and explicit configuration are absent.","Bigtable client initialization fails in multiprocessing child due to missing explicit project ID in configuration, not inherited from environment.","Bigtable client fails in multiprocessing child due to missing project ID, not inherited from parent's environment.","BigtableKVStorage lacks project ID, causing Bigtable client initialization failure and subsequent AttributeError due to uninitialized table.","Bigtable client failed to initialize; project ID was null and GCE metadata server was unreachable.","Bigtable client failed due to missing Google Cloud project ID configuration, and environment auto-detection failed.","Bigtable client failed to auto-determine project due to unreachable GCE metadata server, as no project was explicitly configured.","Bigtable client initialization failed; `project` ID was missing from `SENTRY_NODESTORE_OPTIONS` and environment."],"transactions":["/extensions/slack/action/","/api/embed/error-page/","sentry.workflow_engine.tasks.process_workflows_event","/organizations/{organization_slug}/projects/{project_id_or_slug}/events/{client_event_id}/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/summarize/","/api/0/issues|groups/{issue_id}/events/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/json/","ingest_consumer.process_event","/extensions/slack/event/","sentry.tasks.post_process.post_process_group","replays.consumer.recording_buffered.process_message","sentry.tasks.store.save_event","/api/0/issues|groups/{issue_id}/","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.utils.kvstore.bigtable in _get_table","/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/","/api/0/issues|groups/{issue_id}/events/{event_id}/"],"title":"GCP project/env missing breaks Bigtable and GCS clients","description":"Clients for Google Cloud Bigtable and GCS fail to initialize because the GCP project cannot be derived from the environment, leading to AttributeErrors when accessing the Bigtable table and metadata fetch failures on GCE. This prevents nodestore reads/writes and recording uploads across workers and web requests.","tags":["Configuration","External System","Storage","Google Cloud Platform","Bigtable","Google Cloud Storage","Missing Project Configuration"],"cluster_size":20,"cluster_min_similarity":0.9176378602506026,"cluster_avg_similarity":0.9612285260721302},{"project_ids":["1"],"cluster_id":99,"group_ids":[6584127694,6713208911,6724067689,6725531989,6760531823,6760562900,6808353372],"issue_titles":["Retriable: Redis Cluster cannot be connected. Please provide at least one reachable node.","RedisClusterException: Redis Cluster cannot be connected. Please provide at least one reachable node."],"root_cause_summaries":["Redis cluster connection failed because `rc-span-buffer.sentry.:6379` was unreachable, indicating an environmental network or host availability issue.","Redis cluster connection failed due to unreachable nodes, likely caused by a misconfigured hostname `rc-span-buffer.sentry.`.","Redis cluster node rc-span-buffer.sentry.:6379 became unreachable, preventing redis-py-cluster from reinitializing, causing connection failure.","Redis cluster `rc-memorystore-relay-quotas.sentry.` is unreachable, preventing quota refunds for discarded events, thus halting event processing.","Redis cluster `rc-span-buffer.sentry.:6379` is unreachable, preventing client connection and causing `RedisClusterException`.","RedisClusterException occurred because `is_redis_cluster: True` was configured, but the Redis cluster was unreachable or not a true cluster.","Sentry cannot connect to Redis cluster due to unreachable nodes, likely from misconfigured or malformed hostname."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook","ingest_consumer.process_event","sentry.utils.redis in cluster_factory","sentry.spans.buffer in done_flush_segments","sentry.spans.buffer in process_spans"],"title":"Redis Cluster connectivity failures across workers","description":"Multiple services fail when creating Redis Cluster pipelines/clients, reporting no reachable nodes. This disrupts buffering, flushing, caching, and webhook request queuing, indicating a Redis Cluster outage or misconfiguration.","tags":["Networking","External System","Caching","Redis","Redis Cluster","Connection Failure","Retries Exhausted"],"cluster_size":7,"cluster_min_similarity":0.9123233117124532,"cluster_avg_similarity":0.9411845992855634},{"project_ids":["1"],"cluster_id":103,"group_ids":[6591547772,6646105349,6678435574,6678537749,6715902423,6722355433,6722358514,6722361330,6722367466,6725352853,6725386635,6725580573,6725665272,6739248272,6739419309,6775316556,6775572003,6796173506,6804684665,6805021304],"issue_titles":["SubscriptionError: Invalid value '['FLEMING-CLIENT-6M6']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['DLSITE-WEB-EMA', 'DLSITE-WEB-ER4', 'DLSITE-WEB-ER3']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['HW-ADMIN-JS-ANX']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['QICARD-ANDRIOD-1PN']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['BOOST-INBOX-4K']' for 'issue:' filter","SubscriptionError: Invalid value '['AIRBORNE-QFD']' for 'issue:' filter","SubscriptionError: Invalid value '['DJANGO-D7-1GN']' for 'issue:' filter","SubscriptionError: Invalid value '['ORIOLY_PRODUCTION-XV']' for 'issue:' filter","SubscriptionError: Invalid value '['OCTOPUS-WINDY-BACKEND-2S']' for 'issue:' filter","SubscriptionError: Invalid value '['NODE-UI-2KF']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['NODE-UI-2KF']' for 'issue:' filter","SubscriptionError: Invalid value '['CHEQME-API-5']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['PD-WEB-DXXD', 'PD-WEB-E407']' for 'issue:' filter"],"root_cause_summaries":["Issue filter's short ID string decodes to an unexpected integer, causing group lookup to fail.","Alert rule's stored query contains malformed issue short IDs, causing parsing failure and Group.DoesNotExist during lookup.","Alert rule query contained unparsed 'BOOST-INBOX-4K' issue filter, causing `Group.objects.by_qualified_short_id_bulk` to fail due to invalid short ID format.","Issue filter value's non-base32 component prevents short ID decoding, causing group lookup failure.","Metric alert query failed due to invalid Base32 character in issue short ID, `BOOST-INBOX-4K`.","Snuba subscription creation failed; referenced issue 'CHEQME-API-5' not found in database for organization.","Snuba subscription creation failed because the 'issue:' filter referenced a non-existent group ID.","Alert rule's issue filter contained a non-base32-decodable short ID, causing parsing failure and query execution halt.","Group not found in database; `Group.DoesNotExist` incorrectly re-raised as `InvalidSearchQuery`.","Alert rule's issue filter contains a non-base32 encoded short ID, causing `base32_decode` to fail and raise `Group.DoesNotExist`.","QuerySubscription's 'issue' filter contains a malformed short ID, causing Base32 decoding to fail.","Subscription task fails because referenced issue 'FRONTEND-N7J' does not exist in the database.","Snuba query's issue filter contains base32-invalid short ID, causing parsing and lookup failure.","Subscription query references non-existent issue, causing Group.DoesNotExist, re-raised as SubscriptionError.","Subscription query references a deleted group; database lookup fails, causing `Group.DoesNotExist` and subsequent `InvalidSearchQuery`.","Subscription query references non-existent issues, causing `Group.DoesNotExist` during lookup, re-raised as `InvalidSearchQuery`.","Issue filter string format mismatch causes group lookup failure during subscription creation.","Query parser fails to split 'issue:PROJECT-ID' into project slug and ID, causing Group.objects.by_qualified_short_id_bulk to fail.","Alert rule's issue filter references a non-existent group short ID, causing query failure.","Subscription creation failed because the referenced Sentry Group 'CHEQME-API-5' did not exist in the database."],"transactions":["sentry.snuba.tasks in create_subscription_in_snuba","/issues/alerts/metric-rules/:projectId/:ruleId/","sentry.snuba.tasks.create_subscription_in_snuba","query_subscription_consumer_process_message","sentry.snuba.tasks in _create_in_snuba","sentry.search.events.datasets.discover in _issue_filter_converter"],"title":"Invalid 'issue:' filter values break subscriptions","description":"Queries for alert/subscription building use 'issue:' with short IDs that do not resolve to Groups, causing InvalidSearchQuery and SubscriptionError during SnQL query construction. Impact: subscriptions fail to create or process updates in Sentry/Discover pipeline.","tags":["API","Input Validation","Configuration","Sentry","InvalidSearchQuery","Group.DoesNotExist","Subscriptions"],"cluster_size":20,"cluster_min_similarity":0.9133880855082377,"cluster_avg_similarity":0.9528927959597249},{"project_ids":["1"],"cluster_id":105,"group_ids":[6603108012,6783693043],"issue_titles":["QueryOutsideRetentionError: Invalid date range. Please try a more recent date range."],"root_cause_summaries":["Group's `first_seen` timestamp is corrupted to a future date, causing query's start time to advance past end time, triggering `QueryOutsideRetentionError`.","`outside_retention_with_modified_start` incorrectly sets query start date to future, causing `start` > `end` and `QueryOutsideRetentionError`."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/events/{event_id}/"],"title":"Queries exceed event retention window in Snuba","description":"API requests are querying events with a start/end range older than the configured retention period in Snuba, causing QueryOutsideRetentionError and blocking event retrieval. Adjusting the date range to recent data resolves the error.","tags":["Configuration","API","Data Integrity","Snuba","Invalid Date Range","QueryOutsideRetentionError"],"cluster_size":2,"cluster_min_similarity":0.956149790775365,"cluster_avg_similarity":0.956149790775365},{"project_ids":["1"],"cluster_id":108,"group_ids":[6603111100,6722535091,6784499232],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: canceling statement due to user request"],"root_cause_summaries":["Inefficient database query with leading wildcard `LIKE` on `sentry_artifactbundleindex.url` caused query timeout.","Inefficient `url__icontains` filter on `TextField` with leading wildcard forces full table scan, causing PostgreSQL query cancellation.","Artifact lookup query with `LIKE '%value%'` on `TextField` causes full table scan, exceeding database timeout."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/artifact-lookup/"],"title":"Artifact bundle lookup query canceled in PostgreSQL","description":"Search queries against sentry_artifactbundle with multiple EXISTS filters are being canceled by the database, likely due to statement timeout or manual cancellation, causing API error handling paths to trigger. Impacted endpoints rely on artifact bundle resolution for projects/releases.","tags":["Database","API","PostgreSQL","Query Canceled","Statement Timeout","Artifact Bundle"],"cluster_size":3,"cluster_min_similarity":0.9732907630289699,"cluster_avg_similarity":0.9779985404021732},{"project_ids":["1"],"cluster_id":110,"group_ids":[6603117848,6603259068],"issue_titles":["AtlassianConnectValidationError: Signature is invalid"],"root_cause_summaries":["Jira's shared secret regenerated, Sentry's stored secret is outdated, causing JWT signature verification to fail.","Jira integration reinstallation fails to update shared secret in Sentry, causing subsequent JWT signature validation to fail."],"transactions":["/extensions/jira/issue-updated/"],"title":"Jira Atlassian Connect JWT signature invalid","description":"Requests to the Jira integration fail JWT verification in Atlassian Connect, indicating an invalid or mismatched shared secret when decoding tokens. This prevents identifying the integration from incoming requests.","tags":["Authentication","API","Configuration","Atlassian Connect","JWT","Invalid Signature","Jira Integration"],"cluster_size":2,"cluster_min_similarity":0.971603633267951,"cluster_avg_similarity":0.971603633267951},{"project_ids":["1"],"cluster_id":111,"group_ids":[6603119285,6784828208,6785419068,6792509045,6794173719],"issue_titles":["OperationalError: canceling statement due to user request","OperationalError: QueryCanceled('canceling statement due to user request\\n')"],"root_cause_summaries":["Database query for monitor stats times out due to expensive aggregation on large `sentry_monitorcheckin` dataset.","Slow database query for release counts exceeds external HTTP request timeout, causing connection termination and query cancellation.","Real-time analytics query on high-volume transactional data, with complex aggregations over large date ranges, causes PostgreSQL statement cancellation due to timeout.","Task's database query exceeded its 10-minute processing deadline, causing the task worker to cancel the statement.","Large organization's weekly report query exceeds 10-minute deadline due to data volume and system instability, causing cancellation."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/current-release/","sentry.tasks.summaries.weekly_reports.prepare_organization_report","/api/0/organizations/{organization_id_or_slug}/monitors-stats/","/api/0/teams/{organization_id_or_slug}/{team_id_or_slug}/release-count/"],"title":"PostgreSQL queries canceled by user-initiated timeouts","description":"Multiple long-running SELECTs across reporting and release endpoints are being canceled with 'canceling statement due to user request', likely from application-side timeouts or query cancel signals, disrupting dashboards and weekly reports.","tags":["Database","API","PostgreSQL","Timeout","Query Canceled","Reporting Endpoints"],"cluster_size":5,"cluster_min_similarity":0.9405663965514753,"cluster_avg_similarity":0.9572266892072557},{"project_ids":["1"],"cluster_id":114,"group_ids":[6603148180,6725487270,6729521164],"issue_titles":["SubscriptionError: Invalid value '['CART-540V']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['AWAY-RESORTS-WEBSITE-QJ']' for 'issue:' filter","InvalidSearchQuery: Invalid value '['ANTHROPIC-6RC49']' for 'issue:' filter"],"root_cause_summaries":["Alert subscription references a deleted/non-existent group, causing group lookup failure during comparison query execution.","Alert rule's Snuba query references a non-existent group short ID, causing `Group.DoesNotExist` during lookup, which is re-raised as `InvalidSearchQuery`.","Alert rule references non-existent issue; subscription creation fails due to missing Group object lookup."],"transactions":["query_subscription_consumer_process_message","sentry.snuba.tasks in _create_in_snuba"],"title":"Subscriptions reject invalid issue: filters with unknown short IDs","description":"Alert/subscription query building fails when the issue: filter includes short IDs that cannot be resolved to Groups, raising InvalidSearchQuery and Group.DoesNotExist during Snuba subscription creation and updates. Impact: affected subscriptions fail to create or process updates for those queries.","tags":["API","Data Integrity","Input Validation","Sentry Subscriptions","InvalidSearchQuery","Group.DoesNotExist","Issue Filter"],"cluster_size":3,"cluster_min_similarity":0.9663644983732758,"cluster_avg_similarity":0.9701789946994462},{"project_ids":["1","300688"],"cluster_id":116,"group_ids":[6604468660,6656884667,6725381326,6738358264,6749089445,6776467976,6776669275,6776701844,6805659553],"issue_titles":["QueryMemoryLimitExceeded: DB::Exception: Memory limit (for query) exceeded: would use 9.32 GiB (attempt to allocate chunk of 33554432 bytes), maximum: 9.31 GiB.. Stack trace:","QueryException: Code: 241. DB::Exception: Received from snuba-events-analytics-platform-2-1:9000. DB::Exception: Memory limit (for query) exceeded: would use 1.86 GiB (attempt to allocate chunk of 4533488 bytes), maximum: 1.86 GiB.: while executing 'FUNCTION toString(t...","QueryException: Code: 241. DB::Exception: Received from snuba-events-analytics-platform-dragon-5-1:9000. DB::Exception: Query memory limit exceeded: would use 1.99 GiB (attempt to allocate chunk of 512.02 MiB bytes), maximum: 1.86 GiB.: While executing MergeTreeSelect(...","QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-4-2:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.32 GiB (attempt to allocate chunk of 4620098 bytes), maximum: 9.31 GiB.: (while reading column transaction_name): (while reading f...","QueryException: DB::Exception: Received from snuba-transactions-tiger-mz-1-2:9000. DB::Exception: Memory limit (total) exceeded: would use 28.72 GiB (attempt to allocate chunk of 0 bytes), maximum: 28.22 GiB. OvercommitTracker decision: Memory overcommit has freed not ...","QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-3-6:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.31 GiB (attempt to allocate chunk of 5278314 bytes), maximum: 9.31 GiB.: while executing 'FUNCTION arrayElement(exception_frames.f...","QueryException: Code: 241. DB::Exception: Memory limit (for query) exceeded: would use 1.87 GiB (attempt to allocate chunk of 15694521 bytes), maximum: 1.86 GiB..\"","QueryException: Code: 241. DB::Exception: Received from snuba-events-analytics-platform-dragon-5-2:9000. DB::Exception: Memory limit (for query) exceeded: would use 1.89 GiB (attempt to allocate chunk of 134250496 bytes), maximum: 1.86 GiB.: While executing MergeTreeSe...","SnubaRPCError: code: 500"],"root_cause_summaries":["ClickHouse query for distinct trace IDs on large dataset exhausted memory due to `DISTINCT` operation and internal `toString` conversion.","Memory-intensive aggregation query in subscription exceeds ClickHouse limits due to insufficient allocation policy constraints.","Snuba query's combinatorial complexity from many function names and group IDs exceeded ClickHouse memory limit.","ClickHouse query processing `attributes_string_0` with complex aggregations on large datasets exceeds memory limits.","ClickHouse memory limit exceeded reading large map column `attributes_string_0` for Logs dataset queries.","Memory limit exceeded due to large query hitting ClickHouse's default `max_bytes_to_read` setting, despite Snuba's allocation policies being inactive.","ClickHouse query for trace item details exhausts memory due to `mapConcat` on 40 large map columns.","ClickHouse query for top tag values per key exhausted memory by grouping all tag combinations before applying per-group limit.","Organization-wide query for common transaction across 100+ projects over 90 days, with `groupUniqArray(100)` aggregation, exhausted ClickHouse memory."],"transactions":["EndpointTimeSeries__v1","/api/0/organizations/{organization_id_or_slug}/events/","EndpointTraceItemDetails__v1","/api/0/organizations/{organization_id_or_slug}/replay-count/","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","[cli init] subscriptions-executor","EndpointGetTraces__v1","EndpointTraceItemStats__v1","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/tags/"],"title":"ClickHouse queries exceed memory limits in Snuba","description":"Multiple Snuba queries against ClickHouse (errors and transactions storages) are failing with perquery and total memory limit exceeded errors during aggregation and column reads, impacting endpoints like replay counts, group tags, commit context, and trace item stats.","tags":["Database","Resource Limits","ClickHouse","Memory Limit Exceeded","AggregatingTransform","Snuba"],"cluster_size":9,"cluster_min_similarity":0.9334005318010999,"cluster_avg_similarity":0.9467260720984719},{"project_ids":["1"],"cluster_id":118,"group_ids":[6612650576,6728433574,6792973245,6800358243],"issue_titles":["ProjectUptimeSubscription.DoesNotExist: ProjectUptimeSubscription matching query does not exist.","Group.DoesNotExist: Group matching query does not exist."],"root_cause_summaries":["Concurrent deletion pathways for `ProjectUptimeSubscription` cause race condition; one path deletes object, other fails to find it.","Concurrent group deletion tasks create a race condition where one task attempts to retrieve a group already deleted by another, causing a `DoesNotExist` error.","Race condition: `broken_monitor_checker` attempts to disable a `ProjectUptimeSubscription` already deleted by a concurrent process.","Concurrent group deletion caused a TOCTOU race condition, leading to `Group.DoesNotExist` when re-fetching an already deleted group."],"transactions":["sentry.uptime.tasks.broken_monitor_checker","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.deletions.tasks.groups.delete_groups","sentry.deletions.tasks.run_deletion"],"title":"Orphaned deletions reference missing DB rows","description":"Background deletion and maintenance tasks are querying Group and ProjectUptimeSubscription rows that have already been removed, causing DoesNotExist exceptions during bulk delete and detector disable flows. Likely race conditions or inconsistent cascading between parent/child deletions.","tags":["Database","Data Integrity","Background Jobs","Django ORM","DoesNotExist","Group Model","ProjectUptimeSubscription"],"cluster_size":4,"cluster_min_similarity":0.9438984883125939,"cluster_avg_similarity":0.9526500994997229},{"project_ids":["1"],"cluster_id":119,"group_ids":[6612721950,6719797850],"issue_titles":["SystemExit: -241"],"root_cause_summaries":["Slack API network timeouts cause Celery task time limit exceedance, leading to SIGTERM process termination.","SlackSdkClient's missing HTTP timeout causes network hangs, leading to Celery task timeouts and SIGTERM-induced process termination."],"transactions":["sentry.rules.processing.delayed_processing"],"title":"SystemExit triggered during Slack message send","description":"Calls to Slack chat_postMessage are invoking a wrapped SDK method that results in SystemExit, cascading into metrics and Sentry reporting paths. This likely indicates improper use of exit semantics in the Slack SDK wrapper during notification handling.","tags":["External System","API","Configuration","Slack","SystemExit"],"cluster_size":2,"cluster_min_similarity":0.9584713424012549,"cluster_avg_similarity":0.9584713424012549},{"project_ids":["1"],"cluster_id":122,"group_ids":[6612844028,6691076976,6738967728],"issue_titles":["UnqualifiedQueryError: Validation failed for entity search_issues: Tag keys (flags_key) not resolved","UnqualifiedQueryError: Validation failed for entity events: Tag keys (profile_id) not resolved","UnqualifiedQueryError: Validation failed for entity events: Tag keys (measurements) not resolved"],"root_cause_summaries":["Column resolution for 'profile.id' uses Discover dataset, but query executes against Events dataset where 'profile_id' is undefined.","SnubaFlagStorage queried 'search_issues' for 'flags_key', but the dataset schema lacked this column, causing a validation error.","Measurement filter applied to error issues, querying Snuba's Events dataset, which lacks measurement columns."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/tags/","/api/0/organizations/{organization_id_or_slug}/issues/"],"title":"Snuba tag key resolution fails in events queries","description":"Queries to Snuba for events and issue search are rejecting tag keys like flags_key, profile_id, and measurements, causing UnqualifiedQueryError during group listing and tag aggregation. Likely due to unqualified or unsupported tag keys in SNQL queries for these entities.","tags":["API","Data Integrity","Sentry Snuba","UnqualifiedQueryError","Tag Key Resolution"],"cluster_size":3,"cluster_min_similarity":0.9458104918776744,"cluster_avg_similarity":0.9511492307394182},{"project_ids":["1"],"cluster_id":123,"group_ids":[6613156323,6616423795,6672769203,6714387394,6739725203],"issue_titles":["InvalidSearchQuery: ca804edb-c160-42fa-86d0-87798d4e99fd is an invalid value for id","InvalidSearchQuery: `trace` must be a valid UUID hex (32-36 characters long, containing only digits, dashes, or a-f characters)","InvalidSearchQuery: `Filter ID` must be a valid UUID hex (32-36 characters long, containing only digits, dashes, or a-f characters)"],"root_cause_summaries":["User searched for issue short ID using 'id:' filter, which expects UUID event IDs, causing validation failure.","Trace ID's 33-character length fails strict 32/36-character UUID validation, causing query rejection.","Traces endpoint's 'id' field uses a span ID validator, rejecting valid 32-character event IDs for 'sentry.item_id'.","User searched for issue short ID using event ID field, failing UUID validation.","Client sends non-UUID string as event ID filter, triggering `InvalidSearchQuery` due to strict validation."],"transactions":["/api/0/organizations/{organization_id_or_slug}/traces/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/events/{event_id}/","/api/0/organizations/{organization_id_or_slug}/trace-meta/{trace_id}/"],"title":"Invalid UUID in search filters for trace/id fields","description":"Search endpoints reject queries where Filter ID or trace parameters are not valid UUID hex values, causing InvalidSearchQuery errors across issue and trace views. User-supplied filters must be normalized to UUID format before querying Snuba/Discover.","tags":["API","Input Validation","Search","UUID Format","InvalidSearchQuery","Sentry Discover"],"cluster_size":5,"cluster_min_similarity":0.9311028124444773,"cluster_avg_similarity":0.9489611569261829},{"project_ids":["1"],"cluster_id":124,"group_ids":[6613167409,6615148838],"issue_titles":["PermissionDenied","KeyError: 'access_token'"],"root_cause_summaries":["GitHub rejected the OAuth authorization code, causing the identity provider to receive an error response instead of an access token, leading to a PermissionDenied exception.","GitHub returned 'bad_verification_code'; Sentry's OAuth callback failed to halt, propagating error payload, causing KeyError when 'access_token' was accessed."],"transactions":["/auth/sso/","/identity/login/{provider}/"],"title":"OAuth2 login pipeline missing access_token causes permission errors","description":"During the OAuth2 authentication pipeline, the provider's build_identity expects an access_token in the payload, but it is absent, leading to a KeyError and subsequent PermissionDenied in the identity step. This breaks login/installation flows using the OAuth2 provider.","tags":["Authentication","API","Input Validation","OAuth2","KeyError","PermissionDenied"],"cluster_size":2,"cluster_min_similarity":0.9584992932914504,"cluster_avg_similarity":0.9584992932914504},{"project_ids":["1"],"cluster_id":126,"group_ids":[6613478263,6615425161,6632499142,6633006200,6638414449,6643954095,6643954097,6643958561,6644183937,6644320206,6644680265,6644730360,6647330539,6647330684,6647330729,6656793919,6658163377,6658163424,6658163434,6658163609,6658163781,6658167031,6658167309,6658173249,6658175269,6658412462,6659241897,6659351110,6659410713,6665586681,6665626738,6666197959,6666202460,6667827285,6668404896,6672965893,6673097878,6673723306,6675129028,6675782295,6677148219,6678002632,6683783925,6692664486,6693844110,6696789901,6702482376,6708345575,6709237728,6712322036,6712727430,6712727452,6713083961,6714844832,6717332069,6717332083,6717462080,6718313952,6721880563,6722336372,6725404370,6726293654,6726293662,6726293844,6727142113,6728021237,6733871079,6733871204,6733871252,6734120739,6741168827,6745748294,6786887317,6787782658,6788631847,6790532482,6791368903,6792416414,6792444893,6792468318,6792948230,6793472562,6793495560,6793495579,6797461617,6799535383,6801117958,6804300361,6806423177,6812650467,6812980112],"issue_titles":["TimeoutException: ","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)","SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)","ReadTimeout: HTTPSConnectionPool(host='api.codecov.io', port=443): Read timed out. (read timeout=10)"],"root_cause_summaries":["Snuba query for tag values timed out due to high data volume and complex aggregations exceeding the 30-second read limit.","Snuba `uniq(tags[sentry:user])` query timed out during Group serialization for SentryApp webhook, blocking task.","Old groups' `first_seen` creates wide Snuba query time windows, causing latest event retrieval to timeout.","Slack unfurling's latest event query defaults to an excessively broad time range, causing Snuba API read timeouts.","Snuba query for hourly event count times out for high-volume groups, causing post-processing failure due to scalability bottleneck.","Snuba query for issues by transaction includes excessive group IDs, causing read timeouts.","Snuba query for latest event of issue 6789012519 timed out due to excessive event volume for that group.","Snuba query for unique user counts during Group serialization exceeds 30-second read timeout.","Expensive Snuba `uniq` aggregation with `ORDER BY` in critical path exceeds 30s timeout, causing webhook delivery failure due to no retries on read timeouts.","Snuba query for 90-day transaction data exceeds 30-second read timeout, causing API endpoint failure.","Snuba query times out due to excessively large `IN` clauses from too many projects in a single request.","Snuba query for high-volume issue statistics timed out, due to expensive aggregations over large dataset.","Snuba query for trace-connected issues times out due to scanning all organization projects over a 2-day window.","Snuba query for latest group event timed out due to performance degradation on large group data.","Codecov API fails to respond to complex GraphQL test analytics query within Sentry's 10-second read timeout.","Snuba query for EAP comparison aggregation exceeds its 25s timeout, causing Sentry's 30s read timeout.","Snuba query for metrics data exceeded 30s timeout, causing Sentry's HTTP client to abort the request.","Snuba query for group user counts lacked time bounds, defaulting to an expensive historical scan that timed out.","Snuba query for unique users since issue creation timed out, exacerbated by ClickHouse issues.","PUT request fails due to synchronous, expensive Snuba query for user stats during response serialization.","Snuba query for user counts timed out due to missing time parameters, causing a default 3-month scan.","Snuba query for top tag values over 3 months timed out due to expensive aggregation and limiting, exceeding 30-second read timeout.","Release records without processed events cause unbounded Snuba queries, leading to read timeouts.","Expensive Snuba query for user counts timed out due to Snuba service instability.","Ineffective `inner_limit` optimization for 'recommended' event query causes Snuba to execute expensive sorts on large datasets, leading to timeouts.","Snuba session count query for high-volume project exceeds 30-second read timeout, failing alert rule evaluation.","Snuba query for 14-day p95 transaction duration data exceeds 30-second read timeout, causing task failure.","Snuba's SNQL parser exceeded 30-second read timeout due to prolonged query string parsing, causing a `ReadTimeoutError`.","Snuba query for high-volume outcomes data exceeded 30-second read timeout due to query complexity and massive data volume.","Snuba query for open PR comments is too complex and broad, exceeding the 30-second read timeout.","Replay segment query's broad timestamp range causes ClickHouse scan, exceeding Snuba's 30-second read timeout.","Snuba query for specific replay ID with excessive events exceeds 30-second read timeout.","Snuba's query planning for event frequency rules on IssuePlatform dataset exceeds client timeout.","Snuba query for replay counts, spanning 90 days of discover data, exceeded 30-second read timeout due to inefficient aggregation.","Dynamic sampling task's large Snuba queries, containing many organizations and projects, exceed 30-second timeout, causing ReadTimeoutError.","Missing release environment data causes unbounded Snuba query, leading to read timeout.","Snuba query for project event counts times out due to high data volume for specific projects.","Snuba API read timed out due to service unresponsiveness or network instability, preventing query completion.","Group serialization triggers unbounded Snuba query for user counts, causing read timeout for large groups.","Snuba's query planner exceeds Sentry's 30-second read timeout due to inefficient processing of fine-grained time-series queries.","Snuba API unresponsiveness caused read timeout during user count query, failing post-processing task.","Complex replay aggregation query, with many functions and filters, exceeded the 30-second Snuba API read timeout.","Snuba query for profiled spans uses inefficient `has` conditions, causing 30-second read timeout on large traces.","Trace tree query, fetching all organization projects, times out due to ClickHouse overload.","Inefficient Snuba query construction with duplicated project IDs and redundant filtering causes read timeout.","Broad 'everything else' query for attribute distributions exceeds Snuba's 30-second read timeout.","Spike protection query for historical data times out, as the system's response mechanism overloads itself during high event volume.","Snuba query for spike projection timed out after 30 seconds due to query complexity/Snuba load, and was not retried.","Broad Snuba query for all sessions over 3 days, grouped by release and project, causes read timeout for high-volume organizations.","Snuba query for distinct users times out due to high cardinality within a 93ms event burst.","Unbounded Snuba query for user tag values with many IPs exceeds 30s read timeout.","Snuba query for hourly event count timed out, indicating Snuba performance degradation.","Codecov API's complex GraphQL query for 30-day test aggregates exceeded Sentry's hardcoded 10-second request timeout.","Complex Snuba query with large `group_id` list and stack frame analysis exceeds 30-second read timeout.","Snuba query for metrics data exceeds 30-second timeout, causing `SnubaError` due to performance bottleneck.","High-volume group's 'get latest event' Snuba query exceeds 30-second timeout, causing digest delivery failure.","Slack notification's unique user count query for high-volume group timed out, due to expensive aggregation over large dataset.","Snuba infrastructure performance degradation causes simple event lookup queries to exceed 30-second read timeout.","Snuba query for tag values on large dataset with high-cardinality grouping exceeded 30-second read timeout.","Snuba query for ~1000 group IDs in `IN` clause exceeds 30s read timeout.","Snuba query for custom dynamic sampling rule timed out due to large date range and complex user-defined filters.","Health data check queries 90 days of session data, ignoring user's 24h request, causing Snuba API read timeout.","Snuba query for 23 groups over 23 hours in production exceeded 30-second read timeout, causing immediate failure.","Snuba query for release statistics across many projects without environment filtering exceeds 30-second read timeout.","Invalid Snuba referrer `tsdb-modelid:300.batch_alert_event_uniq_user_frequency` prevents query optimization, causing read timeouts for complex batch unique user count queries.","Snuba's ClickHouse backend saturates under concurrent digest queries, causing simple event lookups to time out.","Webhook preparation's expensive Snuba query for user counts times out, blocking notification delivery.","No summary found","Snuba query for unique users on a high-volume group over a long period exceeds 30-second read timeout.","High event volume for a snoozed group causes Snuba's event count query to time out, failing the post-processing pipeline.","Slack notification's user count query uses an unbounded time window, causing Snuba to timeout on expensive `uniq` aggregation.","Snuba query for frequency-based snooze validation times out due to high data volume over large window.","Snuba query for trace item details uses epoch start time, causing excessive data scan and exceeding 30-second read timeout.","Snuba's SNQL parser experiences severe performance degradation, causing query parsing to exceed the 30-second read timeout.","Snuba query for issue hashes timed out due to high cardinality `primary_hash` grouping.","Snuba query for dynamic sampling task times out due to excessive data volume and query complexity for large organization batches.","Snuba service instability combined with resource-intensive dynamic sampling queries caused read timeout.","Excessive data retrieval from Snuba for replay details causes network transfer to exceed 30-second timeout.","API endpoint uses an overly complex, wide-ranging Snuba aggregation query for a simple replay lookup, causing Snuba to exceed the 30-second read timeout.","Snuba query for latest event used excessive time window, causing 30-second read timeout.","Group.get_latest_event() without time bounds queries 15+ years of data, causing Snuba API read timeout.","Snuba query for latest event of large group times out due to inefficient ordering and vast data, exceeding 30s limit.","Synchronous Snuba query for user report event data, with wide time range, exceeds read timeout during event serialization.","Expensive Snuba query for 'unhandled' status during group serialization exceeds 30-second read timeout.","Expensive unique count Snuba query with narrow time range times out due to high load and no retries, failing digest delivery.","Snuba query for top issues times out due to large `IN` clause and complex array/boolean conditions on event data.","Snuba query with high-cardinality IN clause and 30-day range exceeds 30-second timeout.","Snuba read timeout occurred due to complex derived metric query on large raw session data for many projects.","Synchronous Snuba query for replay counts times out, blocking Slack notification delivery.","Attachment endpoint's unbounded Snuba query for event IDs on high-volume issues exceeds timeout.","Inefficient Snuba query generation includes all organization projects, causing read timeout for trace-connected issues."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/current-release/","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.dynamic_sampling.tasks.custom_rule_notifications","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/replays/{replay_id}/videos/{segment_id}/","/api/0/organizations/{organization_id_or_slug}/prevent/owner/{owner}/repository/{repository}/test-results-aggregates/","sentry.dynamic_sampling.tasks.boost_low_volume_transactions","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/replays/{replay_id}/viewed-by/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/stats/","tasks.spike_protection.run_spike_projection","/api/0/issues|groups/{issue_id}/events/{event_id}/","sentry.sentry_apps.tasks.sentry_apps.workflow_notification","/api/0/organizations/{organization_id_or_slug}/events/","/api/0/organizations/{organization_id_or_slug}/metrics/data/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/integrations/{integration_id}/","/api/0/issues|groups/{issue_id}/events/","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","/api/0/organizations/{organization_id_or_slug}/projects/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/","/extensions/slack/event/","/api/0/organizations/{organization_id_or_slug}/releases/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/hashes/","sentry.tasks.post_process.post_process_group","/api/0/organizations/{organization_id_or_slug}/replays/","/api/0/organizations/{organization_id_or_slug}/replay-count/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/tags/{key}/values/","sentry.utils.snuba in _snuba_query","/api/0/internal/seer-rpc/{method_name}/","sentry.tasks.digests.deliver_digest","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/rules/preview/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","/api/0/organizations/{organization_id_or_slug}/replays-events-meta/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/tags/{key}/values/","/api/0/organizations/{organization_id_or_slug}/releases/{version}/previous-with-commits/","/api/0/organizations/{organization_id_or_slug}/prevent/owner/{owner}/repository/{repository}/test-results/","getsentry.tasks.quotas.react_to_spike_protection","/api/0/organizations/{organization_id_or_slug}/replays/{replay_id}/","sentry.issues.tasks.post_process.post_process_group","sentry.tasks.autofix.trigger_autofix_from_issue_summary","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/summarize/","sentry.tasks.statistical_detectors.detect_transaction_change_points","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/events/{event_id}/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/","sentry.rules.processing.delayed_processing","/api/0/organizations/{organization_id_or_slug}/trace-items/attributes/ranked/","sentry.dynamic_sampling.tasks.boost_low_volume_projects","/api/0/organizations/{organization_id_or_slug}/trace-meta/{trace_id}/","/api/0/organizations/{organization_id_or_slug}/issues/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/trace-items/{item_id}/","query_subscription_consumer_process_message","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/attachments/"],"title":"Snuba query timeouts across multiple endpoints","description":"Requests from Sentry services to Snuba are timing out during HTTP reads, causing failures in tag queries, group details, replay counts, and post-processing paths. Impact is user-facing pages and alerts that depend on Snuba data, suggesting upstream slowness or overload in Snuba.","tags":["Networking","External System","API","Timeout","HTTP","Snuba","Read Timeout"],"cluster_size":91,"cluster_min_similarity":0.9049591046155199,"cluster_avg_similarity":0.9504965565637985},{"project_ids":["1"],"cluster_id":130,"group_ids":[6613790541,6617754304,6646262047,6646607613,6651254357,6656379290,6656520640,6657408486,6663775650,6671746066,6688185490,6691534629,6716578515,6723477306,6725840728,6725840732,6725840742,6725840746,6725840747,6737253569,6760384999,6778089909,6778089920,6780654743,6784563319,6784663532,6784673324,6784701420,6785097543,6785409246,6786262113,6788746429,6788870342,6789462196,6789563170,6792578888,6792698714,6793021224,6793093846,6794486651,6796039249,6798410124,6799376702,6803306246,6804434396,6815799972],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to statement timeout\\n')","OperationalError: canceling statement due to statement timeout","OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: canceling statement due to user request"],"root_cause_summaries":["Broad issue search across many projects triggers inefficient database post-filtering, generating an overly complex SQL query that times out.","Long-running SQL query for release semver check times out, blocking event processing due to large dataset.","Latest release query times out due to inefficient `RANK()` on large datasets, causing event post-processing failure.","Database query for 10,000 `Group` records exceeds 120-second taskworker processing deadline, causing query cancellation.","Inefficient `get_previous_release` database query, due to nested subqueries and missing chronological filter, causes timeout for large organizations.","Inefficient database query for releases due to `COALESCE` ordering without supporting index, causing timeout.","Inbox search query's complex nested subqueries for ownership filters cause PostgreSQL statement timeout due to inefficient execution.","PostgreSQL database resource exhaustion causes query timeouts, leading to Sentry's client canceling the statement.","Missing database index on `sentry_activity` causes slow query, triggering task worker timeout and query cancellation.","Release pagination queries on large datasets cause PostgreSQL timeouts due to complex filters, joins, and ordering.","RangeQuerySetWrapper's `id`-based pagination combined with complex filters creates inefficient database queries, exceeding task timeouts.","Task's large `project_id__in` database query exceeds processing deadline, causing `QueryCanceled` errors and retries.","Expensive database query with large IN clause and ORDER BY on `sentry_activity` table exceeds PostgreSQL statement timeout.","Taskworker's processing deadline was exceeded, causing process termination and cancellation of the in-progress database query.","Inefficient `get_previous_release` query exceeds Heroku webhook timeout, causing PostgreSQL to cancel the statement.","PostgreSQL query canceled due to `DISTINCT ON` and `ORDER BY` mismatch, causing inefficient full table sort and timeout.","Default issue type inclusion, when no explicit filter is provided, generates an overly broad database query, causing a PostgreSQL timeout.","PostgreSQL query for releases timed out due to index sort order mismatch, forcing costly filesort.","PostgreSQL query for `sentry_groupedmessage` in `weekly_escalating_forecast` task exceeds timeout, causing cancellation and repeated failures.","Database query for `Group` objects exceeds task's 120-second processing deadline, triggering query cancellation.","Inefficient database query on `sentry_groupedmessage` due to missing index on `times_seen` for `ORDER BY` clause, causing query cancellation.","Task's large `project_id__in` query exceeds 120s deadline, causing PostgreSQL to cancel it.","Inefficient database query for latest releases causes statement timeout in rule processing for large projects.","Database query for `sentry_activity` by `project_id`, `datetime`, and `type` is slow due to missing index on `type` column, causing query cancellation.","Database query canceled because uWSGI worker's harakiri timeout was exceeded due to slow query execution.","Complex SQL query generated by `assigned_or_suggested_filter` with many projects causes Postgres statement timeout.","Database query for groups exceeds task timeout, causing PostgreSQL to cancel it, leading to task failure.","Inefficient database query for `GroupOpenPeriod` records, combining JSON field lookup and complex joins, causes PostgreSQL timeout at scale.","Database query canceled due to high contention and resource limits during a large data migration.","Inefficient `GROUP BY`/`HAVING MAX` query on `sentry_grouphistory` with suboptimal indexing exceeds PostgreSQL statement timeout.","Missing database index on `sentry_activity` table caused query cancellation during Redis instability.","Database query for artifact bundle existence times out due to expensive multi-table join on large datasets.","PostgreSQL query with 10,000-item IN clause exceeds 120-second task deadline, causing query cancellation and task failure.","Complex search filters on large datasets generate an expensive database query, exceeding PostgreSQL resource limits and causing cancellation.","Inefficient database query for release versioning scheme causes statement timeout in post-processing task.","Taskworker's processing deadline exceeded, causing cancellation of a long-running, complex PostgreSQL query on large tables.","Inefficient database query with large IN clause due to many-to-many relationship causes query timeout and cancellation.","Database query for 'until escalating' groups exceeds taskworker's 2-minute deadline due to large data volume, causing cancellation.","Inefficient Postgres release filtering, triggered by a Clickhouse timeout, caused query cancellation.","Querying 'for review' issues for large projects causes an inefficient database JOIN, leading to statement timeout.","Task's large project batch size causes database query to exceed processing deadline, leading to query cancellation.","Database query for metrics, using `IN` with `MIN/GROUP BY` subquery, caused timeout due to large data volumes.","Database query for groups exceeds task's 2-minute deadline, causing query cancellation and task failure.","Postgres pre-filter with complex `seer_fixability_score` conditions on large table caused statement timeout.","Database query for recent releases times out due to missing optimal composite index for join, filters, and ordering.","Inefficient database query for release stats, exacerbated by complex joins and filters, exceeded timeout, causing cancellation."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues-count/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/source-map-debug-blue-thunder-edition/","/api/0/organizations/{organization_id_or_slug}/events-meta/","/api/0/organizations/{organization_id_or_slug}/issues/","/api/hooks/release/{plugin_id}/{project_id}/{signature}/","/api/0/organizations/{organization_id_or_slug}/releases/stats/","sentry.utils.query in __iter__","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.tasks.weekly_escalating_forecast.generate_forecasts_for_projects","sentry.models.projectownership in handle_auto_assignment","sentry.tasks.store.save_event","/api/0/organizations/{organization_id_or_slug}/releases/{version}/","/api/0/organizations/{organization_id_or_slug}/releases/{version}/previous-with-commits/","sentry.tasks.schedule_auto_transition_issues_regressed_to_ongoing","sentry.migrations.0925_backfill_open_periods in _backfill_group_open_periods","sentry.integrations.source_code_management.commit_context in get_top_5_issues_by_count_for_file","query_subscription_consumer_process_message","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/{version}/stats/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/","sentry.tasks.post_process.post_process_group","sentry.issues.tasks.post_process.post_process_group","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/files/artifact-bundles/","/api/0/organizations/{organization_id_or_slug}/releases/","/extensions/{provider_id}/setup/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/rules/preview/"],"title":"PostgreSQL timeouts on release and group queries","description":"Multiple queries against release and group tables are being canceled due to statement timeouts or user-request cancellations, impacting event processing, rule evaluation, and release webhooks in Sentry services. Heavy joins and aggregations on sentry_release and related tables likely exceed query time budgets.","tags":["Database","Concurrency","PostgreSQL","Timeout","Long-Running Query","Sentry Release Queries"],"cluster_size":46,"cluster_min_similarity":0.9068691054430339,"cluster_avg_similarity":0.937265497767506},{"project_ids":["1"],"cluster_id":133,"group_ids":[6614374597,6614374599,6620870001,6675809594],"issue_titles":["EventLookupError: Failed to lookup event(720856425ffc45f3b9fb5452e78caaf5) for project_id(4508316709093376)","EventLookupError: Failed to lookup event(001ce79ea21a486fbd4304b6301340a3) for project_id(6573488)","EventLookupError: Failed to lookup event(e518d20b57f94590bb8e3affea18097e) for project_id(4508214121988096)","EventLookupError: Failed to lookup event(28b2f7a7d01a424899ce3d5f1635e061) for project_id(4508450758787072)"],"root_cause_summaries":["Event lookup failed due to race condition: occurrence processed before event persisted to Nodestore.","Bigtable read for event data exceeds 5-second timeout, causing event lookup failure and message processing error.","Bigtable cluster for 'default' app profile unreachable, causing 5-second lookup timeout and event processing failure.","Bigtable `read_row` failed due to `ServiceUnavailable` (\"No zones available\") for the configured app profile, causing event lookup to fail."],"transactions":["issues.occurrence_consumer"],"title":"Bigtable unavailability causes event lookup timeouts","description":"Occurrence processing fails when reading from Nodestore backed by Google Cloud Bigtable, leading to EventLookupError after repeated DeadlineExceeded and ServiceUnavailable responses from Bigtable. This prevents issue occurrences from being correlated with their events.","tags":["External System","Networking","Queueing","Google Cloud Bigtable","Timeout","Upstream Unavailable","Event Lookup"],"cluster_size":4,"cluster_min_similarity":0.9283051786822247,"cluster_avg_similarity":0.9508877726973296},{"project_ids":["1"],"cluster_id":134,"group_ids":[6614378040,6785111346],"issue_titles":["OperationalError: canceling statement due to user request","OperationalError: QueryCanceled('canceling statement due to user request\\n')"],"root_cause_summaries":["Database query canceled; `filter_to_team` generates excessive `IN` clause parameters from large team member lists.","Query for team's unresolved issues generates an excessively large `IN` clause due to many team members, causing PostgreSQL to cancel it."],"transactions":["/api/0/teams/{organization_id_or_slug}/{team_id_or_slug}/all-unresolved-issues/"],"title":"PostgreSQL queries canceled for group assignment filters","description":"Complex SELECTs over sentry_groupedmessage with UNION subqueries on group assignees are being canceled by the server, likely due to exceeding query time limits. This impacts reporting endpoints that aggregate or bucket issues by project and first_seen.","tags":["Database","Query Cancellation","PostgreSQL","Long-Running Query","Sentry ORM"],"cluster_size":2,"cluster_min_similarity":0.9776835964945526,"cluster_avg_similarity":0.9776835964945526},{"project_ids":["1"],"cluster_id":136,"group_ids":[6614620358,6616181936,6661409875,6665247058,6703508960,6790267383,6793537536,6803506420],"issue_titles":["RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'BytesScannedRejectingPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'organization_id 530955 is over the bytes scanned limit of 150000000000 for referrer r...","RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'BytesScannedRejectingPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'organization_id 37442 is over the bytes scanned limit of 1280000000000 for referrer t...","RateLimitExceeded: Query scanned more than the allocated amount of bytes"],"root_cause_summaries":["Replay tag value query's full aggregation on high-volume data exceeds Snuba's bytes scanned limit.","Snuba query for facets with leading wildcard on `exception_frames.abs_path` caused excessive byte scan, triggering rate limit.","Snuba query for PR comment exceeds byte limit due to verbose function matching logic for multiple stack frames.","Dashboard widget query across 70+ projects with expensive string matching over 20 days exceeded Snuba's byte scan limit.","Broad 'has:field' query on large dataset forced extensive ClickHouse scan, exceeding Snuba's byte limit.","Unfiltered queries for large organizations include too many projects, causing Snuba's byte scan limit to be exceeded.","Querying tag values for all projects generates a Snuba query that scans excessive data, exceeding the byte scan limit.","Malformed Snuba query containing 'SocketTimeoutException' as a general search term caused excessive byte scanning, triggering a `RateLimitExceeded` error."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","sentry.data_export.tasks.assemble_download","/api/0/organizations/{organization_id_or_slug}/tags/{key}/values/","/api/0/organizations/{organization_id_or_slug}/events-stats/","/api/0/organizations/{organization_id_or_slug}/events-facets/","/api/0/organizations/{organization_id_or_slug}/issues/"],"title":"Snuba queries exceed byte scan limits and time out","description":"Multiple Sentry endpoints and a Celery task trigger Snuba queries that scan more bytes than allowed, causing RateLimitExceeded errors; some requests also hit read timeouts. Impact includes failed event stats, facets, issue search, and commit context workflows.","tags":["External System","Rate Limiting","Timeout","Snuba","Read Timeout"],"cluster_size":8,"cluster_min_similarity":0.9311446692107537,"cluster_avg_similarity":0.9492003048242348},{"project_ids":["6178942","1"],"cluster_id":137,"group_ids":[6614750279,6657528669,6675492054,6678691981,6726552555,6726552557,6770572535,6785082756,6788794177,6788879388,6789243660],"issue_titles":["IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_commit_repository_id_2d25b4d8949fca93_uniq\"\\nDETAIL:  Key (repository_id, key)=(223106, a6fc072f3c7241ca08dda08ebd45ab666bd1c1ea) already exists.\\n')","MaxTriesExceeded: Max tries (4) exceeded. Last exception: sqlalchemy.exc.IntegrityError: (psycopg.errors.UniqueViolation) duplicate key value violates unique constraint \"repo_level_memory_store_owner_repo_name_provider_version_key\"","IntegrityError: duplicate key value violates unique constraint \"sentry_incidenttrigger_incident_id_6841d5856711c578_uniq\"","IntegrityError: duplicate key value violates unique constraint \"sentry_organization_organizationmember_id_1634015042409685_uniq\"","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_rulesnooze_user_id_alert_rule_id_d9c94e51_uniq\"\\nDETAIL:  Key (user_id, alert_rule_id)=(3488381, 343963) already exists.\\n')","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_regressiongroup_type_project_id_fingerpr_4fcad0e2_uniq\"\\nDETAIL:  Key (type, project_id, fingerprint, version)=(0, 5916696, 18139a01c0a7b415bbba04de2baa85dddb7f2699, 1) already exis...","IntegrityError: duplicate key value violates unique constraint \"sentry_organizationmember_organization_id_email_3d42dbe9_uniq\"","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_commit_repository_id_2d25b4d8949fca93_uniq\"\\nDETAIL:  Key (repository_id, key)=(2002570, 8918118aaaab8b1262a77f28e797371e9b4605f3) already exists.\\n')","IntegrityError: duplicate key value violates unique constraint \"sentry_commit_repository_id_key_7f948336_uniq\"","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_regressiongroup_type_project_id_fingerpr_4fcad0e2_uniq\"\\nDETAIL:  Key (type, project_id, fingerprint, version)=(0, 4508188600172544, 219cf1d4ffc0561608e7da2524169b856a0f1cfe, 3) alr...","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_regressiongroup_type_project_id_fingerpr_4fcad0e2_uniq\"\\nDETAIL:  Key (type, project_id, fingerprint, version)=(0, 5803459, fb7c1f1ed5ad789e8ce280f119a6bf7e131b8369, 4) already exis..."],"root_cause_summaries":["Concurrent consumer processes attempt to create the same IncidentTrigger record, violating a unique database constraint due to a race condition.","Concurrent tasks create same commit due to non-atomic get-or-create, violating unique database constraint.","Concurrent tasks, using group-level locks, race to create the same commit via non-atomic get-or-create logic, causing unique constraint violation.","Concurrent tasks calculate identical `RegressionGroup` versions, leading to unique constraint violation on database insert.","Concurrent requests attempting to create the same unique snooze record cause a race condition, leading to a unique constraint violation.","Concurrent requests pass email validation before database insertion, leading to a unique constraint violation during save.","Concurrent requests bypass `exists()` check, leading to duplicate `OrganizationMemberTeam` creation and `IntegrityError` due to race condition.","Concurrent `Commit.objects.create` calls for the same `(repository_id, key)` pair cause a unique constraint violation.","Concurrent memory store updates cause unique constraint violations; retry logic fails to re-fetch latest version.","Concurrent task workers create duplicate regression groups due to race condition in version increment logic.","Concurrent tasks attempting to create the same initial regression group due to non-atomic check-then-insert leads to unique constraint violation."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/alert-rules/{rule_id}/snooze/","/api/0/organizations/{organization_id_or_slug}/members/","sentry.statistical_detectors.detector in save_regressions_with_versions","sentry.integrations.utils.commit_context in get_or_create_commit_from_blame","/api/0/organizations/{organization_id_or_slug}/members/{member_id}/teams/{team_id_or_slug}/","sentry.tasks.process_commit_context","sentry.statistical_detectors.detector in redirect_escalations","seer.automation.codegen.pr_closed_step.pr_closed_task","query_subscription_consumer_process_message","sentry.tasks.statistical_detectors.detect_transaction_change_points"],"title":"PostgreSQL unique constraint violations in commit and regression inserts","description":"Concurrent workers attempt to create rows that already exist in sentry_commit, sentry_regressiongroup, and sentry_rulesnooze, leading to IntegrityError UniqueViolation and occasional DoesNotExist lookups when racing. Use get-or-create/upsert or deduplicate before bulk_create to avoid races.","tags":["Database","Concurrency","PostgreSQL","Constraint Violation","Duplicate Insert","Bulk Create","Django ORM"],"cluster_size":11,"cluster_min_similarity":0.9239470101783307,"cluster_avg_similarity":0.951958192692961},{"project_ids":["1"],"cluster_id":138,"group_ids":[6614847677,6748936512],"issue_titles":["ApiInvalidRequestError"],"root_cause_summaries":["GitLab integration sends unsupported `include_subgroups` parameter to GitLab API's `/groups/:id/projects` endpoint, causing a 400 Bad Request.","GitLab API returned 400 Bad Request for project search, likely due to integration's access token permissions or group configuration."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/integrations/{integration_id}/","/api/0/organizations/{organization_id_or_slug}/integrations/{integration_id}/repos/"],"title":"Bad Request from integration when searching projects","description":"Integration endpoints calling search_projects return HTTP 400, causing repository/project lookup to fail during repo listing and create-issue config. Likely due to invalid query or request params sent to the external integration client.","tags":["API","External System","Input Validation","Bad Request","HTTP 400","Integration Client"],"cluster_size":2,"cluster_min_similarity":0.9513775448447057,"cluster_avg_similarity":0.9513775448447057},{"project_ids":["1"],"cluster_id":139,"group_ids":[6614866612,6615305191,6615553940,6656575519,6683602237,6705185478,6721094666,6724313132,6783172929,6784708834,6784777741,6784810113,6792653459,6802471399],"issue_titles":["OperationalError: canceling statement due to user request","OperationalError: QueryCanceled('canceling statement due to statement timeout\\n')","OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: canceling statement due to statement timeout"],"root_cause_summaries":["Inefficient SQL query for project deploys, using expensive window functions on large tables, exceeded database timeout.","Unpaginated project fetching triggers an unscalable SQL query for deployment data, causing database query cancellation due to timeout.","Complex SQL query for project deploys times out, causing database statement cancellation during API request.","Querying release stats for all accessible projects in large organizations generates an excessively long SQL IN clause, causing database timeouts.","Inefficient SQL query for latest deploys in `ProjectSummarySerializer` causes database query cancellation for large organizations.","Complex SQL query for project deploy data scales poorly with organization size, causing database query cancellation.","Inefficient SQL query for \"latest\" release, triggered by large project lists, causes Postgres statement timeouts.","Complex SQL query with window functions and semantic versioning logic on large release datasets exceeds PostgreSQL statement timeout.","Complex SQL query for project deploys exceeded database statement timeout, causing QueryCanceled error.","Complex SQL query with flawed `LIMIT` on large release data causes Postgres statement timeout.","Inefficient SQL query for latest deploys times out, causing serialization failure for organization details.","Organization serialization fetches excessive deploy data for all projects, causing a database query timeout due to large project count.","Complex SQL query for project deploys, executed synchronously, exceeds database timeout, causing cancellation.","PostgreSQL query for 'latest' release times out due to inefficient window function on large release datasets."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events-stats/","/api/0/organizations/{organization_id_or_slug}/","/api/0/organizations/{organization_id_or_slug}/projects/","/api/0/organizations/{organization_id_or_slug}/releases/stats/","/api/0/organizations/{organization_id_or_slug}/events/"],"title":"PostgreSQL queries canceled during project serialization","description":"Multiple endpoints cancel database queries while serializing project and organization data, likely due to request aborts or timeouts causing the DB to terminate in-flight statements.","tags":["Database","API","PostgreSQL","Query Canceled","Serialization"],"cluster_size":14,"cluster_min_similarity":0.9327216472288287,"cluster_avg_similarity":0.9540690578551075},{"project_ids":["1"],"cluster_id":140,"group_ids":[6614895047,6646060569,6646226468,6678761506,6684807601,6735568493,6745714511,6753812045,6754203390],"issue_titles":["ApiError: <html>","ApiError: {\"message\":\"403 Forbidden\"}","ApiError: {\"message\":\"403 Forbidden - Your account has been blocked.\"}"],"root_cause_summaries":["GitLab API rejected Sentry's request: integrated user account blocked by GitLab administrator.","GitLab API returned 403 Forbidden because the integrated account was blocked, preventing PR diff fetching.","GitLab API rejected Sentry's request due to the integration's account being blocked, causing an `ApiError`.","GitLab API rejected request with 403, stating the integration's account was blocked.","GitLab API returned 403 Forbidden because the integration's linked GitLab account was blocked.","GitLab API returned 403 Forbidden, indicating the integrated account was blocked, preventing PR diff retrieval.","GitLab integration's access token lacks permissions for merge request API, causing 403 Forbidden.","GitLab integration's access token lacks `ai_workflows` and `read_api` scopes, causing 403 Forbidden when fetching PR diffs.","GitLab user account linked to Sentry integration blocked, causing 403 Forbidden on API requests."],"transactions":["/api/0/organizations/{organization_id_or_slug}/integrations/{integration_id}/repos/","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","sentry.tasks.process_commit_context"],"title":"Git provider blocks account causing 403 on repo/PR APIs","description":"External API requests for repository search and pull request diffs return 403 Forbidden with message 'Your account has been blocked,' affecting both web handlers and Celery tasks using the integration client.","tags":["External System","API","Authorization","HTTP 403","Account Blocked","Celery"],"cluster_size":9,"cluster_min_similarity":0.9180314742140117,"cluster_avg_similarity":0.9616853058330587},{"project_ids":["1"],"cluster_id":141,"group_ids":[6615127889,6789123542],"issue_titles":["IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"auth_user_username_key\"\\nDETAIL:  Key (upper(username::text))=(HIBI-KEITA@KCGRP.JP) already exists.\\n')","IntegrityError: duplicate key value violates unique constraint \"auth_user_username_key\""],"root_cause_summaries":["Concurrent user registration attempts bypass form-level uniqueness check, leading to database unique constraint violation.","Race condition: concurrent requests pass case-insensitive username validation, but database's unique constraint prevents duplicate insertion."],"transactions":["/auth/register/"],"title":"Duplicate username violates auth_user unique constraint","description":"User registration attempts create an auth_user row with a username/email that already exists (case-insensitive), causing a PostgreSQL UniqueViolation during save in the auth login flow.","tags":["Database","Authentication","PostgreSQL","Constraint Violation","Duplicate Username"],"cluster_size":2,"cluster_min_similarity":0.9809212189416493,"cluster_avg_similarity":0.9809212189416493},{"project_ids":["1"],"cluster_id":143,"group_ids":[6615156292,6712752731],"issue_titles":["RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'sso:auth:33226:834f36738d7711ef4fa8a34536e2f233'>> within 4.920 seconds (49 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'sso:auth:33648:a5d349ff22cd2b123ebf42deba1afbf9'>> within 4.929 seconds (51 attempts.)"],"root_cause_summaries":["Redis lock duration (5s) is insufficient for SSO login operations, causing lock expiry and contention, leading to `RetryException`.","Concurrent requests for same user's SSO lock fail because each attempt generates a new, unmatching lock UUID."],"transactions":["/auth/login/{organization_slug}/","/saml/acs/{organization_slug}/"],"title":"SSO login lock acquisition fails on Redis","description":"During SSO login, the app cannot set the Redis-backed lock key for sso:auth, exhausting retries and aborting the pipeline. This prevents users from completing SSO authentication.","tags":["Caching","Authentication","Concurrency","Redis","Lock Acquisition Failure","Retries Exhausted"],"cluster_size":2,"cluster_min_similarity":0.9700135947263049,"cluster_avg_similarity":0.9700135947263049},{"project_ids":["1"],"cluster_id":145,"group_ids":[6615324488,6615324684,6645148777,6683574592,6701639376],"issue_titles":["AssertionError"],"root_cause_summaries":["SSO login fails due to `_handle_membership` asserting invite acceptance when user is already an organization member.","SSO flow for existing member incorrectly routed as new user, causing `accept_invite` to return `None`, failing subsequent assertion.","`_handle_membership` asserts `rpc_om` is not `None` after `accept_invite` returns `None` for existing member.","SSO login for new user fails when an existing user's invite is found and already accepted, causing a null assertion.","SAML 'newuser' operation creates duplicate user, then fails asserting organization member creation when email already exists."],"transactions":["/saml/acs/{organization_slug}/","/auth/sso/"],"title":"SSO login fails on membership creation assertion","description":"During the SSO authentication pipeline, membership handling asserts because rpc_om is missing, causing login to abort. This occurs in the auth provider flow while creating or linking organization membership for new or existing identities.","tags":["Authentication","API","Input Validation","Single Sign-On","AssertionError","Organization Membership"],"cluster_size":5,"cluster_min_similarity":0.9440588078313819,"cluster_avg_similarity":0.9605637884978769},{"project_ids":["1"],"cluster_id":147,"group_ids":[6615427258,6617626628,6633843757,6660162120,6662690811,6664775757,6664775914,6672722015,6678806978,6694145982,6697533450,6717006809,6724487115,6724487138,6730954501,6734289588,6734289714,6734289773,6734289781,6734289791,6734289848,6735423112,6735631851,6735631890,6735652390,6735652395,6736136066,6736136108,6750419689,6755570174,6755570177,6755570204,6755570269,6755570657,6755570664,6755570669,6755570671,6755570687,6755570694,6755570695,6755570697,6755570698,6762800139,6778598387,6778600331,6787772564,6799094801],"issue_titles":["JSONDecodeError: Expecting value: line 1 column 1 (char 0)","SnubaError: Failed to parse snuba error response"],"root_cause_summaries":["Snuba API proxy returned non-JSON 'upstream request timeout' on 504, causing Sentry's JSON parser to fail.","Server clock skew causes `timezone.now()` to return future timestamps, leading to Snuba query timeouts and `JSONDecodeError` on non-JSON responses.","Snuba API returned HTTP 504 Gateway Timeout, causing Sentry to fail JSON decoding its 'upstream request timeout' response.","Tempest service overload caused upstream timeout, returning non-JSON. Sentry's `response.json()` failed to parse plain text, raising `JSONDecodeError`.","Snuba API returned 504 Gateway Timeout with non-JSON, causing `JSONDecodeError` and `SnubaError`.","Envoy proxy timed out due to Snuba instability, returning non-JSON error, causing Sentry's JSON parsing to fail.","Unconstrained Snuba query for all issue tags timed out, causing gateway to return non-JSON error, leading to JSONDecodeError.","Offset-based Snuba query pagination progressively degrades performance, causing timeouts and non-JSON responses.","Snuba unique user count query for high-volume group timed out, causing JSON decode error.","ClickHouse query for performance facets times out, causing Snuba to return non-JSON, leading to Sentry's JSON parsing failure.","Slow Snuba query causes upstream proxy to timeout, returning non-JSON 'upstream request timeout' string, leading to JSONDecodeError.","Upstream proxy timeout returns non-JSON, causing Sentry's `json.loads` to fail on long Snuba queries.","Snuba query for unique users times out due to expensive aggregation and ineffective caching, returning non-JSON response.","Digest records spanning months caused Snuba query timeout, leading to non-JSON proxy response and JSONDecodeError.","Snuba API timed out, returning non-JSON 'upstream request timeout' string, causing JSON decoding failure.","Snuba query timeout yields non-JSON response, causing JSON decoding failure in Sentry's Snuba client.","Upstream proxy timeout returns non-JSON, causing Sentry's JSON parser to fail.","Snuba API timeout returned non-JSON, causing Sentry's JSON parser to fail, raising SnubaError.","Snuba query with 5,000 group IDs causes timeout, returning non-JSON, leading to JSONDecodeError.","Tempest API's non-JSON 504 responses cause `poll_tempest_crashes` to fail parsing, due to missing HTTP response validation.","Snuba query for alert condition data timed out, returning non-JSON response, causing JSON decoding failure.","Snuba API timeout returned non-JSON, causing Sentry's JSON parsing to fail.","Snuba API timeout returns non-JSON, causing Sentry's JSON decoder to fail.","Snuba API timeout returns non-JSON, causing Sentry's JSON parser to fail before status code check.","Snuba query timed out, causing proxy to return non-JSON 'upstream request timeout', leading to JSONDecodeError.","Snuba query timed out, proxy returned non-JSON error, causing Sentry's JSON parser to fail.","Snuba API query timed out, returning non-JSON, causing JSON parsing failure.","Snuba query for issues times out due to excessive breadth when Postgres pre-filtering is bypassed, returning unparseable non-JSON.","Snuba API proxy returned non-JSON 504 timeout, causing Sentry's JSON parser to fail and raise SnubaError.","Intermediary service timeout returns plain text with HTTP 200, causing Sentry's JSON parser to fail.","Snuba's upstream timeout returns non-JSON, causing Sentry's JSON parser to fail.","`_bulk_snuba_query` attempts JSON parsing before checking HTTP status, causing `JSONDecodeError` on non-JSON 504 responses.","Network infrastructure timeout between Sentry and Snuba caused non-JSON 504, leading to JSON parsing failure.","Snuba's allocation policies or ClickHouse queries time out, causing its API gateway to return non-JSON, leading to Sentry's parsing error.","Snuba API timeout returned non-JSON, plain text response, causing Sentry's JSON decoder to fail.","Snuba query timeout returns non-JSON, causing Sentry's JSON decoder to fail.","Snuba API query timeout, returning non-JSON, caused application's JSON decoding failure.","Sentry's `fetch_latest_item_id` task failed to parse non-JSON 504 response from `tempest` service, which returned plain text.","Snuba query for replay selectors times out due to excessive grouping and wide time range, returning non-JSON, causing a misleading JSONDecodeError.","Snuba's backend (ClickHouse) timed out, causing Snuba to return non-JSON 504s, leading to Sentry's JSON parsing failure.","Snuba query for high-cardinality tag values timed out at proxy, returning non-JSON, causing JSON decode error.","Snuba API returns non-JSON on 504 timeout, causing Sentry's JSON decoder to fail.","Nginx's 5s proxy_send_timeout prematurely cuts Snuba queries, causing JSONDecodeError in Sentry.","Snuba API request timed out, returning unparseable plain text, causing JSON decoding and SnubaError.","Snuba's upstream timeout returns plain text, not JSON, causing Sentry's JSON parser to fail.","Snuba API timeout returns non-JSON, causing Sentry's JSON parser to fail and raise a generic SnubaError.","Snuba API timeout returns non-JSON, causing Sentry's JSON parser to fail and raise SnubaError."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","/api/0/organizations/{organization_id_or_slug}/events/","/api/0/organizations/{organization_id_or_slug}/releases/{version}/previous-with-commits/","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","/api/0/organizations/{organization_id_or_slug}/issues-stats/","sentry.issues.tasks.post_process.post_process_group","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/tags/{key}/values/","/api/0/organizations/{organization_id_or_slug}/replay-count/","sentry.tasks.digests.deliver_digest","sentry.dynamic_sampling.tasks.boost_low_volume_projects","sentry.tempest.tasks.fetch_latest_item_id","/api/0/organizations/{organization_id_or_slug}/tags/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/","sentry.utils.snuba in _bulk_snuba_query","ingest_consumer.process_event","/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/","sentry.tasks.summaries.weekly_reports.prepare_organization_report","/api/0/organizations/{organization_id_or_slug}/replay-selectors/","query_subscription_consumer_process_message","/api/0/organizations/{organization_id_or_slug}/stats_v2/","/api/0/organizations/{organization_id_or_slug}/issues/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/attachments/","/api/0/organizations/{organization_id_or_slug}/events-facets/","/api/0/organizations/{organization_id_or_slug}/events-stats/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/tags/","/api/0/organizations/{organization_id_or_slug}/events-facets-performance/","sentry.rules.processing.delayed_processing","sentry.tasks.autofix.start_seer_automation","/api/0/organizations/{organization_id_or_slug}/trace-meta/{trace_id}/","sentry.tempest.tasks.poll_tempest_crashes"],"title":"Snuba responses return nonJSON; parsing fails","description":"Clients calling Snuba receive bodies that are not valid JSON, causing json.loads() to raise JSONDecodeError and leading to 'Failed to parse snuba error response' across multiple endpoints and a Celery task. Impact: event stats, tag values, and performance facets queries fail when Snuba returns malformed or empty error responses.","tags":["API","Serialization","External System","JSONDecodeError","Snuba","Error Response Parsing"],"cluster_size":47,"cluster_min_similarity":0.9126174707659616,"cluster_avg_similarity":0.9560347656043092},{"project_ids":["1"],"cluster_id":149,"group_ids":[6615894302,6743529496],"issue_titles":["ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"],"root_cause_summaries":["Seer anomaly detection service abruptly terminates TCP connections, causing Sentry's requests to fail with connection reset errors.","Seer anomaly detection service or network path actively reset TCP connection, causing Sentry's `ProtocolError`."],"transactions":["query_subscription_consumer_process_message"],"title":"Seer API connections reset during anomaly queries","description":"Consumers calling Seer for anomaly data encounter connection resets from the upstream Seer API, causing processing of subscription updates and detector evaluations to fail.","tags":["Networking","External System","API","Connection Reset","HTTP","Seer"],"cluster_size":2,"cluster_min_similarity":0.9580394747845548,"cluster_avg_similarity":0.9580394747845548},{"project_ids":["1"],"cluster_id":151,"group_ids":[6615925388,6784704969,6792243816],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: canceling statement due to user request"],"root_cause_summaries":["Unindexed `DISTINCT` query on `IncidentActivity.value` (TextField) exceeds PostgreSQL `statement_timeout`, causing message processing failures.","Inefficient `DISTINCT` query on `TextField` `IncidentActivity.value` causes PostgreSQL timeout.","Inefficient DISTINCT query on `TextField` in `IncidentActivity` causes database timeout in multiprocessing worker."],"transactions":["query_subscription_consumer_process_message"],"title":"PostgreSQL queries canceled during incident processing","description":"Incident activity lookups are being canceled mid-execution while processing subscription updates, causing QueryCanceled errors on the SELECT DISTINCT over sentry_incidentactivity. Likely triggered by user-initiated or timeout-driven cancellations in the worker pipeline.","tags":["Database","API","PostgreSQL","Query Canceled","Incident Processing","Background Worker"],"cluster_size":3,"cluster_min_similarity":0.9601424428363653,"cluster_avg_similarity":0.9634095381530469},{"project_ids":["1"],"cluster_id":152,"group_ids":[6616715815,6637559238],"issue_titles":["AtlassianConnectValidationError: Query hash mismatch"],"root_cause_summaries":["Sentry's QSH calculation percent-encodes slashes in URL query parameters, unlike Atlassian's, causing hash mismatch.","Sentry's `get_query_hash` double-encodes URL parameters, causing `qsh` mismatch with Atlassian's JWT."],"transactions":["/extensions/jira/issue/{issue_key}/"],"title":"Jira Atlassian Connect JWT query hash mismatch","description":"Requests to the Jira integration fail JWT validation because the computed qsh (query string hash) does not match, likely due to mismatched path or query parameters during claim verification.","tags":["Authentication","API","Configuration","Atlassian Connect","Jira","JWT Validation","Query Hash Mismatch"],"cluster_size":2,"cluster_min_similarity":0.9681663942420359,"cluster_avg_similarity":0.9681663942420359},{"project_ids":["1"],"cluster_id":157,"group_ids":[6617849667,6647402930,6755482405,6803506878],"issue_titles":["ValueError: user_id is required (cannot be None)"],"root_cause_summaries":["Organization lacks owner, causing analytics event's required user_id to be null.","Organization 543362 lacks a default owner, causing `user_id` to be `None` when recording promotion analytics, triggering a `ValueError`.","Organization 543362 lacks an active owner, causing `default_owner_id` to be null, which is invalid for the required `user_id` analytics event field.","Organization lacks owner, causing `user_id` to be `None` for promotion analytics, triggering `ValueError`."],"transactions":["/api/0/organizations/{organization_id_or_slug}/promotions/trigger-check/","getsentry.tasks.promotions.check_for_completed_promotions"],"title":"Analytics event missing user_id in promotion completion","description":"Promotion completion flow tries to record analytics without a user_id, causing ValueError in both HTTP handler and Celery worker paths when building the event payload. The analytics event schema requires user_id but the claimed promotion lacks it or it is not propagated.","tags":["Input Validation","API","Background Jobs","Analytics","ValueError","Celery"],"cluster_size":4,"cluster_min_similarity":0.9749021599423521,"cluster_avg_similarity":0.9791765546496659},{"project_ids":["1"],"cluster_id":158,"group_ids":[6617879329,6738545985],"issue_titles":["RestrictedIPAddress: (gitlab-new.vndirect.com.vn/10.210.9.91) matches the URL blocklist"],"root_cause_summaries":["GitLab hostname resolves to a private IP within Sentry's configured URL blocklist, preventing connection.","GitLab integration failed: hostname resolved to private IP, blocked by Sentry's `SENTRY_DISALLOWED_IPS` security configuration."],"transactions":["/extensions/{provider_id}/setup/"],"title":"OAuth token request blocked by URL/IP blocklist","description":"Outbound HTTP requests to the OAuth access_token endpoint are being denied because the resolved host IP matches the restricted URL/IP blocklist, preventing token exchange.","tags":["Networking","Security","API","Outbound Request Blocked","OAuth 2.0","URL Blocklist"],"cluster_size":2,"cluster_min_similarity":0.9702161146497488,"cluster_avg_similarity":0.9702161146497488},{"project_ids":["1"],"cluster_id":159,"group_ids":[6618392527,6784998767,6792509405],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: canceling statement due to user request"],"root_cause_summaries":["Synchronous cache invalidation for project ownership triggers an expensive database query on `sentry_groupedmessage` for large projects, causing query cancellation.","Expensive `Group.objects.filter` query for cache invalidation exceeds task deadline, causing database statement cancellation.","Updating code mapping synchronously queries all project issues, causing database timeout for large projects."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/ownership/","sentry.integrations.models.repository_project_path_config in _clear_commit_context_cache"],"title":"PostgreSQL queries canceled during groupedmessage lookups","description":"Queries against sentry_groupedmessage are being canceled by user request, causing OperationalError/QueryCanceled exceptions across web and task paths. This likely stems from explicit timeouts or request cancellations interrupting long-running SELECTs filtered by last_seen and project_id.","tags":["Database","API","PostgreSQL","Query Canceled","OperationalError","sentry_groupedmessage"],"cluster_size":3,"cluster_min_similarity":0.9545051393162011,"cluster_avg_similarity":0.9597092643656396},{"project_ids":["1"],"cluster_id":160,"group_ids":[6618576794,6623462960,6650217255,6657134294,6657142448,6668503039,6668503104,6675268120,6675811366,6675835370,6675996646,6676000496,6676053507,6676187450,6677236268,6683092903,6708645937,6713177010,6719919555,6734977986,6756866270,6777362281,6777363467,6788973894,6789104348,6789674164,6792277093,6794699013],"issue_titles":["SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78cef4b3cdd0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x795bb6e945f0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7979ffb785f0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x790867f2e0f0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78de247704d0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /generic_metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ea47eadae70>: Failed to establish a new connection: [Errno 111] Connection...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e9ebd791370>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5bf02e1fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /search_issues/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b650d6e8710>: Failed to establish a new connection: [Errno 111] Connection r...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c0051a30050>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f234a7d15b0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /discover/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b34142fc510>: Failed to establish a new connection: [Errno 111] Connection refuse...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ad481544d0>: Failed to establish a new connection: [Errno 111] Connection refused'))","MaxRetryError: HTTPConnectionPool(host='vroom', port=80): Max retries exceeded with url: /profile (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x790ad4e60510>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a7ed77f0dd0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7df9a472d910>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e3c74972c30>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b36ac2ce690>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f40dc6cd7f0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /discover/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e7a0c729130>: Failed to establish a new connection: [Errno 111] Connection refuse...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79534badccb0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78ed88ab5fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ed9416a2570>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /generic_metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e9e685017f0>: Failed to establish a new connection: [Errno 111] Connection...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /search_issues/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ed5ab141d90>: Failed to establish a new connection: [Errno 111] Connection r...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /outcomes/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78e37df64170>: Failed to establish a new connection: [Errno 111] Connection refuse...","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e93a6714170>: Failed to establish a new connection: [Errno 111] Connection refused'))","SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /generic_metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f63ff06e9f0>: Failed to establish a new connection: [Errno 111] Connection..."],"root_cause_summaries":["Snuba service refused connections on port 80, causing Sentry's HTTP client to exhaust retries and fail.","Sentry task worker cannot connect to Snuba API at `snuba-api:80`; target service is unreachable or not listening.","Sentry's Snuba client configured for `snuba-api:80` but Snuba service not listening on that port.","Snuba API service unreachable, causing connection refusal and event fetching failure.","Sentry's API failed to connect to Snuba service at `snuba-api:80`, causing connection refused errors and preventing issue count retrieval.","Snuba service connection refused; `SNUBA` environment variable incorrectly set `snuba-api` without port, defaulting to 80, while Snuba listens on 1218.","Snuba service not listening on configured port 80, causing connection refusal and retries to fail.","Snuba service refused connection on port 80; Sentry expected port 80, but Snuba listened on 1218.","Snuba service refused connection on port 80, likely due to misconfigured `SNUBA` environment variable or service unavailability.","Sentry's profiling task failed to connect to 'vroom:80' because the service was unreachable, causing connection refused errors.","Snuba API service unreachable from task worker, causing connection refusal and query failure.","Snuba-api service unreachable on port 80, causing connection refused errors during statistical detector queries.","Sentry failed to connect to Snuba because the `SNUBA` environment variable pointed to `snuba-api:80`, but Snuba was not listening on that port.","Sentry application failed to connect to Snuba API due to network connection refused, indicating Snuba service unavailability.","Sentry attempts connecting to Snuba on port 80, but Snuba listens on port 1218, causing connection refused errors.","Sentry application cannot connect to snuba-api:80; connection refused, indicating service unavailability or network blockage.","Snuba API unavailability for dynamic sampling data causes serialization failure, due to critical path dependency.","Snuba service was unreachable, causing connection refusal and query failures for event data retrieval.","Taskworker's SNUBA environment variable incorrectly set Snuba host to port 80, causing connection refused as Snuba listens on port 1218.","Snuba API service at snuba-api:80 refused connection, preventing Sentry from querying transaction data.","Snuba service at `snuba-api:80` is unreachable, causing connection refused errors during API requests for trace metadata.","Snuba service refused connection to Sentry worker, indicating an infrastructure or network misconfiguration.","Snuba API service unreachable; connection refused at `snuba-api:80` during issue velocity calculation.","Snuba service unreachable from task worker, causing connection refusal during issue data serialization.","Taskworker's `SNUBA` environment variable incorrectly pointed to `snuba-api:80`, causing connection refused errors to Snuba's actual port 1218.","Snuba service unreachable at `snuba-api:80`, causing connection refused errors during query execution.","Snuba API service is unreachable, causing connection refused errors during report generation due to service unavailability or misconfiguration.","Snuba service unreachable at `snuba-api:80`, causing connection refusal during rule evaluation."],"transactions":["sentry.sentry_apps.tasks.service_hooks.process_service_hook","sentry.tasks.summaries.weekly_reports.prepare_organization_report","/api/0/organizations/{organization_id_or_slug}/issues-count/","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.tasks.statistical_detectors.detect_transaction_trends","sentry.rules.processing.delayed_processing","/api/0/organizations/{organization_id_or_slug}/replay-count/","/api/0/organizations/{organization_id_or_slug}/issues-stats/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/{version}/","sentry.tasks.auto_source_code_config","sentry.issues.tasks.post_process.post_process_group","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/events/{event_id}/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/first-last-release/","/api/0/organizations/{organization_id_or_slug}/issues/","/api/0/organizations/{organization_id_or_slug}/","query_subscription_consumer_process_message","/api/0/organizations/{organization_id_or_slug}/trace-meta/{trace_id}/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/","sentry.tasks.statistical_detectors.detect_transaction_change_points","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/attachments/","sentry.profiles.task.process_profile","sentry.utils.snuba in _snuba_query","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/summarize/"],"title":"Snuba HTTP connections to /snql endpoints refused","description":"Sentry web requests that query Snuba (events, search_issues, generic_metrics) are failing because the Snuba HTTP service refuses new connections, leading to urllib3 MaxRetryError and SnubaError. Impact includes failures in organization stats, issues search/count, group stats, and replay counts.","tags":["Networking","External System","API","Connection Refused","Retries Exhausted","HTTP","Snuba"],"cluster_size":28,"cluster_min_similarity":0.926416378375116,"cluster_avg_similarity":0.9546955080627318},{"project_ids":["1"],"cluster_id":163,"group_ids":[6618886266,6649110473],"issue_titles":["PartnerAccount.DoesNotExist: PartnerAccount matching query does not exist."],"root_cause_summaries":["GitHub `plan_changed` webhook received for unprovisioned `PartnerAccount` due to prior `plan_purchased` webhook failing from missing GitHub identity.","PartnerAccount missing because initial provisioning failed, causing subsequent plan change webhook to error."],"transactions":["/remote/github/webhook/"],"title":"Webhook plan_changed missing PartnerAccount record","description":"The plan_changed webhook handler queries PartnerAccount by identifiers that are not present, raising DoesNotExist during request processing. This likely indicates mismatched or missing partner account mapping for incoming webhook events.","tags":["API","Input Validation","Django","DoesNotExist","Webhook"],"cluster_size":2,"cluster_min_similarity":0.9721764866367543,"cluster_avg_similarity":0.9721764866367543},{"project_ids":["1"],"cluster_id":165,"group_ids":[6620012303,6662013446],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\n')"],"root_cause_summaries":["Unbounded database query for all project group IDs caused query cancellation due to excessive execution time.","Unoptimized query for all project group IDs exceeds database timeout, preventing cache invalidation."],"transactions":["sentry.tasks.auto_source_code_config","/api/0/organizations/{organization_id_or_slug}/code-mappings/"],"title":"PostgreSQL queries canceled during code mapping cache clear","description":"SELECTs on sentry_groupedmessage by project_id are being canceled while creating RepositoryProjectPathConfig and clearing commit-context cache on transaction commit, interrupting both worker and API flows.","tags":["Database","API","PostgreSQL","Query Canceled","Django Transaction","RepositoryProjectPathConfig"],"cluster_size":2,"cluster_min_similarity":0.977542364659627,"cluster_avg_similarity":0.977542364659627},{"project_ids":["6178942"],"cluster_id":168,"group_ids":[6621297713,6705193811,6705205294,6705824065,6707911641,6724478613,6725125568,6732544529],"issue_titles":["OperationalError: (psycopg.errors.QueryCanceled) canceling statement due to user request","OperationalError: (psycopg.OperationalError) sending query and params failed: another command is already in progress","OperationalError: (psycopg.OperationalError) consuming input failed: server closed the connection unexpectedly","OperationalError: sending query and params failed: another command is already in progress","ProgrammingError: (psycopg.ProgrammingError) can't change 'autocommit' now: connection in transaction status ACTIVE"],"root_cause_summaries":["Celery task exceeded soft time limit during repository initialization, abruptly closing database connection, causing `OperationalError`.","Celery timeout interrupts DB transaction, leaving connection active; subsequent pre-ping fails to change autocommit.","Root cause task exceeds soft time limit due to repeated, inefficient database queries within agent's iterative loop, causing connection state corruption and rollback failure.","Long-running AI task holding DB lock exceeded timeout, corrupting connection, causing rollback failure.","Root cause analysis task exceeds soft time limit, triggering query cancellation and subsequent database connection errors.","Celery signal interrupted DB query, corrupting connection state; subsequent rollback failed as connection was busy.","Celery soft timeout interrupts long LLM operation within database transaction, leaving connection active, causing `autocommit` change failure during pool ping.","LLM task exceeding soft time limit caused database connection to be busy during rollback, leading to OperationalError."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task"],"title":"psycopg connection reused concurrently in SQLAlchemy session","description":"Multiple concurrent DB operations are issued on the same PostgreSQL connection, causing 'another command is already in progress' and autocommit state errors; some requests also hit soft time limits and dropped connections. Affected components use SQLAlchemy sessions to read/update run state and memory.","tags":["Database","Concurrency","Configuration","PostgreSQL","SQLAlchemy","Another Command In Progress","Autocommit Misuse","Server Closed Connection","Soft Time Limit Exceeded"],"cluster_size":8,"cluster_min_similarity":0.9200592602951876,"cluster_avg_similarity":0.9433537989666958},{"project_ids":["1"],"cluster_id":171,"group_ids":[6622326497,6639664508,6672775351,6678376500,6722475261,6722475265,6722475271,6722475273,6722475281,6722475293,6722475296,6722475298,6722475307,6722475308,6722475313,6722475320,6722475321,6722475330,6722475338,6722475339,6722475342,6722475350,6722475351,6722475353,6722475361,6722475382,6722475393,6722475408,6722475500,6731912419,6735618081,6736227747,6755963921,6756415987,6760582147,6764023355,6764023368,6768733831,6770337481,6770337490,6770337512,6771303682,6777598608,6777598609,6777598613,6777598615,6777598632,6777598703,6777598704,6777598728,6777598730,6777598732,6777598734,6777598736,6777598743,6777598750,6777598767,6777598833,6778034026,6778034064,6782453560,6782454729,6782454755,6782454757,6782454764,6782756510,6782756520,6782756524,6782756549,6782756573,6782756580,6782756603,6782756605,6782756612,6782756623,6782756655,6782756699,6792227927,6792227950,6792227980,6792228003,6792228178,6792904257,6793106943,6794965163,6803951793,6803951797,6803951799,6803951805,6803951817,6803951860,6803951865,6803951867,6803951876,6803951879,6803951887,6808680596,6808680605,6808680608,6808680614,6808680627,6808680630,6808680637,6808680659,6808680671,6810511345,6810511369,6810511370,6810511394,6810511395,6810511438,6812766571,6812882101,6813542903,6813542906],"issue_titles":["MovedError: 11494 192.168.209.21:11130","MovedError: 11782 192.168.240.12:11467","MovedError: 2272 192.168.209.21:11096","MovedError: 13518 192.168.208.5:11113","MovedError: 11697 192.168.208.5:11003","MovedError: 684 192.168.209.22:11081","MovedError: 11598 192.168.208.2:11096","MovedError: 8900 192.168.240.12:11469","MovedError: 13400 192.168.240.12:11461","MovedError: 2435 192.168.209.92:6300","MovedError: 5318 192.168.208.87:11032","MovedError: 7858 192.168.208.5:11133","MovedError: 4623 192.168.208.5:11463","MovedError: 84 192.168.209.22:11093","MovedError: 15375 192.168.208.5:11127","MovedError: 1236 192.168.208.5:11135","MovedError: 1651 192.168.209.92:6300","MovedError: 2959 192.168.208.5:11499","MovedError: 10991 192.168.209.21:11018","MovedError: 3998 192.168.208.40:11495","MovedError: 15954 192.168.208.2:11148","MovedError: 3489 192.168.208.5:11129","MovedError: 5309 192.168.240.12:11449","MovedError: 9752 192.168.208.5:11097","MovedError: 14573 192.168.209.21:11082","MovedError: 1143 192.168.240.12:11063","MovedError: 7383 192.168.209.22:11119","MovedError: 14476 192.168.208.83:11476","MovedError: 2784 192.168.209.21:11016","MovedError: 1745 192.168.208.2:11050","MovedError: 396 192.168.209.22:11093","MovedError: 13857 192.168.208.2:11504","MovedError: 12425 192.168.208.2:11104","MovedError: 76 192.168.209.22:11093","MovedError: 4667 192.168.240.12:11449","MovedError: 11031 192.168.209.21:11018","MovedError: 8416 192.168.209.92:6310","MovedError: 2049 192.168.209.21:11096","MovedError: 15693 192.168.240.11:11464","MovedError: 819 192.168.240.12:11459","MovedError: 10804 192.168.209.92:6310","MovedError: 3940 192.168.208.40:11509","MovedError: 9741 192.168.208.5:11097","MovedError: 2598 192.168.209.92:6300","MovedError: 10001 192.168.240.11:11466","MovedError: 10669 192.168.209.92:6310","MovedError: 14197 192.168.209.22:11449","MovedError: 12541 192.168.209.22:11109","MovedError: 5205 192.168.208.5:11027","MovedError: 1261 192.168.209.22:11085","MovedError: 5276 192.168.240.12:11449","MovedError: 1476 192.168.208.5:11135","MovedError: 3122 192.168.209.74:6300","MovedError: 11334 192.168.208.5:11111","MovedError: 1596 192.168.209.22:11133","MovedError: 3728 192.168.208.40:11509","MovedError: 8911 192.168.209.92:6310","MovedError: 13513 192.168.208.5:11113","MovedError: 8029 192.168.208.93:11031","MovedError: 1041 192.168.208.5:11131","MovedError: 7075 192.168.208.2:11090","MovedError: 3066 192.168.208.5:11065","MovedError: 1130 192.168.209.92:6300","MovedError: 4727 192.168.208.5:11463","MovedError: 9382 192.168.240.12:11469","MovedError: 956 192.168.209.22:11085","MovedError: 6658 192.168.208.5:11083","MovedError: 10524 192.168.240.11:11466","MovedError: 3911 192.168.208.40:11495","MovedError: 4949 192.168.209.21:11120","MovedError: 12580 192.168.208.2:11128","MovedError: 10691 192.168.208.5:11139","MovedError: 13542 192.168.240.12:11461","MovedError: 15863 192.168.240.11:11464","MovedError: 3839 192.168.208.40:11509","MovedError: 4426 192.168.240.12:11449","MovedError: 4807 192.168.208.2:11068","MovedError: 4086 192.168.240.11:11450","MovedError: 12874 192.168.209.165:11051","MovedError: 8649 192.168.209.22:11105","MovedError: 8427 192.168.209.92:6310","MovedError: 8606 192.168.209.22:11105","MovedError: 874 192.168.208.2:11124","MovedError: 10864 192.168.209.92:6310","MovedError: 2627 192.168.209.92:6300","MovedError: 3486 192.168.208.5:11129","MovedError: 3236 192.168.240.11:11454","MovedError: 10176 192.168.208.5:11099","MovedError: 6688 192.168.209.22:11123","MovedError: 109 192.168.209.92:6300","MovedError: 1970 192.168.209.92:6300","MovedError: 3780 192.168.208.40:11509","MovedError: 13515 192.168.208.5:11113","MovedError: 476 192.168.209.22:11081","MovedError: 9960 192.168.208.40:11509","MovedError: 544 192.168.209.92:6300","MovedError: 11136 192.168.240.12:11467","MovedError: 2695 192.168.240.11:11454","MovedError: 10033 192.168.208.40:11509","MovedError: 5368 192.168.209.22:11113","MovedError: 15652 192.168.240.11:11464","TimeoutError: Timeout connecting to server","MovedError: 855 192.168.209.92:6300","MovedError: 8335 192.168.208.2:11060","MovedError: 9795 192.168.209.21:11112","MovedError: 11976 192.168.208.2:11100","MovedError: 1879 192.168.209.21:11096","MovedError: 15353 192.168.208.5:11127","MovedError: 16207 192.168.209.75:6310","MovedError: 9286 192.168.208.83:11508","MovedError: 10440 192.168.209.21:11022","MovedError: 4253 192.168.240.11:11450","MovedError: 10654 192.168.240.11:11466","MovedError: 3725 192.168.208.40:11509"],"root_cause_summaries":["Redis `MovedError` occurs due to network instability preventing client topology updates during cluster reconfigurations.","Redis client's 3-second socket timeout prevents `MOVED` redirection, causing `MovedError` to propagate.","Redis cluster client fails to transparently handle `MovedError` during rebalancing due to concurrent network `TimeoutError`s preventing topology refresh.","Redis cluster instability and network issues cause `MovedError` and `TimeoutError`, preventing client from successfully redirecting and retrying commands.","Redis cluster node redirection failed due to subsequent connection timeout, indicating an unhealthy target node or network issue.","Redis cluster instability and network issues, exacerbated by system overload, prevent `RetryingRedisCluster` from transparently handling `MovedError` and `TimeoutError`.","Redis cluster instability causes `MovedError` retries to fail with `TimeoutError`, preventing command completion.","Redis cluster performance degradation causes timeouts, preventing `RetryingRedisCluster` from handling `MovedError` redirections.","Redis cluster instability and network issues prevent `RetryingRedisCluster` from handling `MOVED` redirections during key deletion.","Redis cluster instability (MovedError, TimeoutError) during DEL operation, triggered by normal HashDiscarded event cleanup.","Redis cluster instability and aggressive 3-second client socket timeout cause frequent `TimeoutError` and `MovedError` exceptions during cache key deletions.","Redis client's `MovedError` due to cluster topology change, exacerbated by network instability preventing successful redirection and retries.","Redis cluster instability, evidenced by timeouts and topology changes, overwhelms `RetryingRedisCluster`'s retry logic during `EVALSHA` lock release, causing `MovedError`.","Persistent network timeouts between application and Redis cluster prevent `RetryingRedisCluster` from handling `MovedError` redirections.","Redis cluster instability causes `MovedError` during cache cleanup after an event matches a group tombstone.","Redis client timed out connecting to new node during cluster redirection, causing `MovedError`.","Redis client's stale cluster topology, due to a preceding connection timeout, caused `MovedError` during key access after rebalancing.","Redis client's stale cluster topology mapping caused `MovedError` during `SMEMBERS` operation, due to high load and cluster rebalancing.","RetryingRedisCluster fails to auto-redirect `MovedError` from Redis cluster, causing transaction data cleanup to fail.","Redis cluster redirection failed; `RetryingRedisCluster` timed out connecting to the new node, indicating network or node health issues.","Redis cluster instability causes `MovedError` and `TimeoutError` during key retrieval, overwhelming client's retry logic.","Redis client fails to redirect `MovedError` due to cluster/network instability, causing transaction deletion to fail.","Redis cluster rebalancing combined with network timeouts prevented `RetryingRedisCluster` from handling `MovedError` during key deletion.","Redis cluster instability combined with insufficient 3-second client socket timeout prevents `RetryingRedisCluster` from handling `MovedError` redirects.","Redis cluster instability and network issues caused stale topology, preventing `RetryingRedisCluster` from handling `MOVED` redirects, leading to `MovedError`.","Redis cluster instability and overload caused `TimeoutError`s, preventing `RetryingRedisCluster` from handling `MovedError` redirections, leading to task failure.","Redis client fails to handle cluster rebalancing due to network saturation and timeouts under high system load.","Redis cluster rebalancing combined with network timeouts prevented `RetryingRedisCluster` from handling `MovedError`.","Redis cluster instability and aggressive 3-second client timeout cause frequent `MovedError` and `TimeoutError` during `ZSCAN` operations.","Redis client fails to follow cluster `MOVED` redirections due to network instability or unhealthy Redis nodes, causing `MovedError`.","Redis connection timeouts caused stale cluster topology, leading to `MovedError` when accessing rebalanced keys.","Redis cluster instability causes `MovedError` during `DEL` command, overwhelming `RetryingRedisCluster`'s retry logic, leading to unhandled exception.","Redis cluster instability and timeouts prevent client from updating topology, causing `MovedError` during lock acquisition.","Redis cluster instability, likely due to reconfigurations, caused `MovedError` and `TimeoutError` during event data deletion.","Redis cluster instability causes connection timeouts, leading to stale cluster maps and subsequent `MovedError` on operations.","Network instability and aggressive 3-second Redis socket timeout prevent `MovedError` handling, causing cleanup failures.","Redis cluster client fails to redirect after slot migration, causing unhandled `MovedError`.","Redis cluster topology instability causes `MovedError` during key deletion, overwhelming client's retry and topology update mechanisms.","Redis cluster instability (network/rebalancing) causes client timeouts, preventing `RetryingRedisCluster` from handling `MOVED` redirections during high-volume cache deletions.","Redis client's stale cluster topology map caused commands to be routed incorrectly, leading to `MovedError` and task failure.","Redis cluster instability and aggressive 3-second socket timeout overwhelm client's retry mechanism, causing `MovedError`.","Redis cluster instability, evidenced by frequent MOVED/Timeout errors, exhausts client retries, causing transaction event processing tasks to exceed their 65-second deadline.","Redis cluster redirection failed due to aggressive 3-second socket timeout during new node connection.","Sentry's `RetryingRedisCluster` lacks `MovedError` handling, causing Redis cluster slot migration errors to propagate.","Redis cluster instability, evidenced by `MovedError` and `TimeoutError`s, prevents storing rate-limited check-in errors.","Redis client failed to re-route `DEL` command after `MOVED` error, due to cluster instability and insufficient retry logic.","Redis cluster instability, exacerbated by client's `skip_full_coverage_check=True`, causes persistent `MovedError` and `TimeoutError` during lock acquisition.","Redis cluster instability combined with aggressive client timeout prevents `RetryingRedisCluster` from handling `MOVED` redirections.","Redis cluster overload/instability causes `MovedError` and `TimeoutError` during event processing, exhausting client retries.","Redis client's `MovedError` handling fails during cluster rebalancing concurrent with network timeouts, preventing transparent key redirection.","Redis Cluster instability and network timeouts prevent `RetryingRedisCluster` from handling `MOVED` redirections, causing `MovedError`.","Redis client failed to follow `MOVED` redirection due to underlying network connectivity issues or unhealthy Redis cluster nodes.","Redis cluster topology instability and network issues prevent client from successfully handling MOVED redirections, causing command failures.","Redis cluster instability and network issues cause client redirects to fail, exhausting retries and leading to event processing errors.","Redis client's aggressive timeout and insufficient retry/reconnection logic fail to handle cluster re-sharding, causing `MovedError` and `TimeoutError`.","Redis cluster instability or network issues prevent client from connecting to redirected nodes after MOVED error.","Redis cluster instability and network timeouts prevent `RetryingRedisCluster` from updating its topology, causing `MovedError` propagation.","Redis cluster instability, marked by frequent timeouts and slot migrations, overwhelms the client's retry logic during batched ZSCAN operations.","Redis cluster instability and network issues overwhelm client's retry mechanism, causing `MovedError` and `TimeoutError`.","Redis cluster instability (rebalancing, network issues) prevents `RetryingRedisCluster` from transparently handling `MovedError`s, causing event processing failures.","Redis client fails to adapt to cluster topology changes due to network instability or server overload causing timeouts.","Redis cluster instability caused client timeouts and failed `MOVED` redirection handling, leading to `MovedError`.","Redis cluster instability and overload overwhelm `RetryingRedisCluster`'s retry mechanism, causing `MovedError` and `TimeoutErrors`.","Redis node unresponsiveness caused a timeout, triggering cluster failover and a subsequent MovedError due to stale client topology.","Redis `MovedError` occurred due to stale cluster topology, unhandled by `RetryingRedisCluster` amidst cluster instability.","Redis cluster instability and high `DEL` operation volume overwhelm client's retry mechanism, causing `MovedError` propagation.","Redis cluster instability and network timeouts prevent client from updating topology and retrying commands after MOVED errors.","Redis cluster instability causes `MovedError` during `GET` operations, overwhelming `RetryingRedisCluster` and failing symbolication tasks.","Redis timeout caused stale cluster topology, leading to `RetryingRedisCluster` sending commands to wrong nodes, failing `MOVED` error handling.","Redis cluster instability and rebalancing cause timeouts, leading to stale client topology and `MovedError` during pipeline execution.","Network instability causes Redis client's stale cluster topology, leading to `MOVED` errors during key deletion.","Redis cluster instability and network issues prevent `RetryingRedisCluster` from completing `MOVED` redirections, causing task failures.","Redis client failed to handle MOVED redirection due to cluster instability and network timeouts, preventing transaction data deletion.","Redis client's stale cluster topology combined with network instability prevents `RetryingRedisCluster` from handling `MovedError`.","Redis Cluster instability causes `MovedError` and `TimeoutError` during `ZSCAN` operations, overwhelming client-side retries.","Redis cluster instability and timeouts prevent `RetryingRedisCluster` from transparently handling `MOVED` redirections, causing `MovedError` propagation.","Redis cluster instability, due to rebalancing and timeouts, caused `MovedError` during transaction data deletion.","Redis client connection timeouts prevent proper cluster redirection handling, causing `MovedError` during event processing.","Redis client's stale cluster topology, due to network timeouts, caused MOVED error to propagate as automatic redirection failed.","Redis cluster instability, with frequent topology changes and unresponsive nodes, overwhelms retry mechanisms, causing lock operation failures.","Redis cluster instability causes frequent MOVED errors; subsequent connection attempts to new nodes time out, failing operations.","Redis client's aggressive full cluster reset on every `MovedError` causes cascading `TimeoutError`s during cluster instability.","Redis cluster instability and unresponsiveness cause `MovedError` and `TimeoutError` during `DEL` operations, failing span buffer cleanup.","Redis cluster instability, indicated by `MovedError` and `TimeoutError`, exhausts `RetryingRedisCluster`'s retries during non-critical cache cleanup.","Redis cluster resharding combined with network timeouts prevented `RetryingRedisCluster` from handling `MovedError`.","Redis client fails to handle MOVED redirections, raising exceptions instead of re-routing commands.","Redis cluster rebalancing causes client to connect to an unreachable/overloaded node, leading to repeated connection timeouts and operational failures.","Redis cluster instability, aggressive client timeouts, and `skip_full_coverage_check=True` prevent lock acquisition, causing task failures.","Redis cluster instability, with frequent slot movements and timeouts, prevents event data retrieval, causing post-processing tasks to skip.","Redis client's connection timeout prevents cluster topology updates, leading to subsequent MovedError due to stale key-node mapping.","Redis cluster instability, frequent rebalancing, or high load causes `TimeoutError` and `MovedError` during transaction event processing.","Redis cluster reconfiguration combined with network instability prevented `RetryingRedisCluster` from handling `MovedError` redirects.","Redis cluster client fails to handle MOVED redirections or times out, causing transaction event processing to fail.","Redis client fails to handle cluster slot migration, propagating `MovedError` due to `RetryingRedisCluster` not updating topology or encountering timeouts.","Redis client's `MovedError` retry fails due to subsequent connection timeouts, indicating underlying cluster instability or network issues.","Redis Cluster redirection failed; client timed out connecting to the new node, causing a `MovedError`.","Redis cluster performance degradation causes client timeouts, leading to stale topology and unhandled MOVED errors.","Redis client's default configuration fails to handle cluster slot migrations amidst network instability, causing `MovedError`.","Redis cluster topology instability, likely from network issues or cluster health, causes `MovedError` and `TimeoutError` during critical `post_process_group` task.","Redis client's stale cluster topology prevents successful `MOVED` redirection, causing `MovedError` during key deletion.","Redis cluster rebalancing causes `MovedError` during event data retrieval, exposing lack of client-side retry/redirection resilience.","Redis cluster instability and network issues prevented client from connecting to correct node after MOVED redirection.","Redis cluster node 192.168.208.40:11495, owning slot 3911, is unreachable, causing repeated MOVED errors followed by connection timeouts.","Network instability caused Redis cluster failover, leading to stale client topology and `MovedError` during `SET` operation.","Redis cluster instability causes `MovedError`s; aggressive 3-second client socket timeout leads to `TimeoutError`s during re-connection attempts.","Redis cluster instability caused `MovedError` during cache deletion; `RetryingRedisCluster` failed to handle redirection.","Redis cluster instability and aggressive client timeouts cause `MovedError` redirection failures, leading to `TimeoutError` and consumer crashes.","Redis client's `skip_full_coverage_check=True` prevents robust cluster topology updates, causing `MovedError` on re-sharding.","Redis cluster topology change combined with network timeouts prevents `RetryingRedisCluster` from completing `MOVED` redirections.","Redis cluster instability causes `MovedError` retries to timeout, propagating the error.","Redis cluster client's redirection failed due to timeout, propagating `MovedError` and crashing taskworker.","Redis cluster instability causes excessive `MOVED` redirections and timeouts, overwhelming `RetryingRedisCluster`.","Redis client fails to handle cluster slot migrations and network instability during cache cleanup, causing task failures.","Redis operation timed out, preventing `RetryingRedisCluster` from handling `MovedError` redirection.","Redis cluster instability during resharding/failover causes timeouts, preventing client from handling MOVED redirections for key deletion."],"transactions":["sentry.tasks.store.process_event","sentry.tasks.store.save_event","sentry.tasks.process_suspect_commits","sentry.utils.kvstore.redis in delete","sentry.tasks.symbolicate_jvm_event","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.tasks.symbolicate_js_event","sentry.spans.buffer in _load_segment_data","sentry.tasks.store.symbolicate_event","sentry.tasks.relay.invalidate_project_config","sentry.issues.tasks.post_process.post_process_group","sentry.spans.buffer in process_spans","monitors.monitor_consumer","sentry.spans.buffer in done_flush_segments","sentry.utils.locking.backends.redis in acquire","sentry.tasks.process_commit_context","sentry.spans.buffer in flush_segments","sentry.profiles.task.process_profile","sentry.tasks.store.save_event_transaction","sentry.utils.kvstore.redis in get","sentry.models.featureadoption in get_all_cache","/api/0/relays/projectconfigs/","getsentry.billing.tasks.usagebuffer.flush_usage_buffer"],"title":"Redis cluster MovedError across feature, quota, and cache reads","description":"Multiple services (monitors quota checks, feature adoption cache, project config cache, and processing store) are hitting Redis cluster MovedError, indicating slot migration or misrouted keys during cluster reconfiguration. This disrupts check-in processing, metrics/features recording, project config fetches, and event processing cache operations.","tags":["Caching","External System","Configuration","Redis","Cluster Redirection","MovedError"],"cluster_size":115,"cluster_min_similarity":0.9069411469374579,"cluster_avg_similarity":0.9464295301257534},{"project_ids":["1"],"cluster_id":174,"group_ids":[6623343452,6723602454,6802448656],"issue_titles":["OutboxDatabaseError: Failed to process Outbox, USER_UPDATE due to database error","OutboxDatabaseError: Failed to process Outbox, PROJECT_UPDATE due to database error","OutboxDatabaseError: Failed to process Outbox, TEAM_UPDATE due to database error"],"root_cause_summaries":["Bulk project deletions cause `PROJECT_UPDATE` outbox contention, leading to `SELECT FOR UPDATE` query cancellations due to prolonged lock waits.","Concurrent outbox processing deadlocks when transactions acquire overlapping locks on `sentry_controloutbox` entries in conflicting orders.","Concurrent outbox processing for the same organization shard causes deadlocks, leading to query cancellation."],"transactions":["/api/0/users/{user_id}/authenticators/{auth_id}/","/api/0/organizations/{organization_id_or_slug}/teams/","sentry.deletions.tasks.run_deletion"],"title":"Outbox processing hits Postgres deadlocks and cancellations","description":"Draining Outbox shards for USER_UPDATE, PROJECT_UPDATE, and TEAM_UPDATE triggers Postgres deadlocks on controloutbox deletes and query cancellations on regionoutbox row locks (FOR UPDATE). Concurrency during on_commit processing causes contention, interrupting outbox delivery across multiple models.","tags":["Database","Concurrency","Queueing","PostgreSQL","Deadlock","Query Canceled","Outbox"],"cluster_size":3,"cluster_min_similarity":0.9525362560293353,"cluster_avg_similarity":0.9590465549907085},{"project_ids":["1"],"cluster_id":181,"group_ids":[6636638904,6745239016,6745239017,6745239018,6784501226],"issue_titles":["AssertionError"],"root_cause_summaries":["BillingHistory's aggregate ondemand_spend diverged from sum of BillingMetricHistory records, triggering an assertion.","Concurrent billing calculations cause `BillingHistory.ondemand_spend` to temporarily lag behind sum of `BillingMetricHistory.ondemand_spend`.","BillingHistory's aggregate ondemand_spend temporarily lags behind the sum of its constituent BillingMetricHistory records during concurrent updates.","BillingHistory.ondemand_spend lags behind sum of BillingMetricHistory.ondemand_spend, causing assertion failure during quota check.","Concurrent billing updates cause `history.ondemand_spend` to trail sum of `BillingMetricHistory.ondemand_spend`, triggering assertion."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/monitors/{monitor_id_or_slug}/","monitors.monitor_consumer","getsentry.billing.tasks.usagebuffer.flush_usage_buffer"],"title":"Assertion in ondemand spend reconciliation for monitor seats","description":"On assigning or validating monitor seats, ondemand cost calculation asserts that total historical spend equals the cumulative tally, causing failures in both API updates and background usage flushing. The mismatch in spend reconciliation within ondemand.calculate_per_category_ondemand triggers AssertionError across quota and billing seat assignment paths.","tags":["Billing","Quota Management","Data Integrity","Sentry Monitors","AssertionError","Ondemand Spend Calculation"],"cluster_size":5,"cluster_min_similarity":0.9392555401162856,"cluster_avg_similarity":0.959643066829343},{"project_ids":["1"],"cluster_id":183,"group_ids":[6639092757,6646223797,6646224008,6724644147,6777033295,6784855129,6784856737,6785662927,6790978785,6790978815,6792400662,6793431141],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: canceling statement due to user request"],"root_cause_summaries":["PostgreSQL query for `sentry_grouphash` records timed out, cancelling group deletion due to large table or inefficient indexing.","Project deletion blocked by `Incident` protecting `AlertRule` via `models.PROTECT`, preventing `SnubaQuery` deletion, causing database transaction timeout.","Django ORM's implicit cascade deletion of `GroupHash` via `Project` FK generates unchunked, large queries, causing database timeouts.","BulkModelDeletionTask used for GroupHashMetadata, triggering large, unhandled cascading UPDATEs, causing query cancellation.","Long-running `sentry_grouphash` SELECT query during group deletion exceeds task timeout, causing `QueryCanceled` error.","High concurrency cleanup tasks cause database contention, leading to query cancellations on `sentry_grouphash` table.","Large project deletion triggers a single, massive `UPDATE` query on `sentry_grouphashmetadata` due to `SET_NULL` cascade, exceeding task timeout.","PostgreSQL `statement_timeout` cancels `sentry_grouphash` query during deletion, causing incomplete cleanup and foreign key violations.","Un-chunked `on_delete=SET_NULL` cascade on `GroupHashMetadata` during `GroupHash` deletion creates large `UPDATE` query, exceeding database timeout.","Project deletion fails due to an unchunked `sentry_grouphash` query timing out on high volume.","Django's implicit `SET_NULL` for `Activity` on `Group` deletion causes slow `SELECT` queries, exceeding PostgreSQL's timeout.","Deletion task timeout due to large IN clause in `sentry_grouphash` query during project group cleanup."],"transactions":["sentry.deletions.base in delete_instance","sentry.utils.query in __iter__","sentry.deletions.tasks.run_deletion","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/"],"title":"PostgreSQL queries canceled during deletion tasks","description":"Bulk project/group deletion workflows are triggering long-running queries against sentry_activity, sentry_grouphash, and sentry_grouphashmetadata that are being canceled by user request, interrupting cleanup and group delete operations.","tags":["Database","API","Concurrency","PostgreSQL","Query Canceled","Bulk Deletion","Sentry Grouphash"],"cluster_size":12,"cluster_min_similarity":0.9107040681815148,"cluster_avg_similarity":0.9468610709372591},{"project_ids":["1"],"cluster_id":184,"group_ids":[6639601116,6672775383,6672803234,6678376456,6714232636,6716127891,6722475254,6722475255,6722475256,6722475267,6722475304,6722475322,6722475335,6722475336,6722475337,6722475346,6722475355,6722475357,6722475365,6722475376,6722475492,6731912417,6731912420,6770337461,6770337467,6770337470,6770337477,6770337480,6770337484,6770337505,6770337507,6770337509,6771303654,6775169793,6777451935,6777598610,6777598612,6777598620,6777598622,6777598623,6777598625,6777598626,6777598634,6777598638,6777598640,6777598653,6777598663,6777598664,6777598691,6777598692,6777598719,6777598724,6777598735,6777598738,6777598741,6777598758,6777598797,6777598911,6778033996,6778034005,6778034023,6778034024,6778034032,6778034072,6778034096,6778034130,6782453554,6782453558,6782453565,6782453567,6782453568,6782453576,6782454720,6782454815,6782756553,6782756572,6782756578,6782756579,6782756590,6782756596,6782756630,6782756646,6782756662,6782756673,6782756697,6782756706,6782756711,6782756810,6782867846,6792227897,6792227921,6792227972,6792228387,6792904254,6792976638,6793006847,6794965205,6800495907,6803951822,6803951870,6807841705,6808008666,6808680588,6808680592,6808680603,6808680629,6808680649,6808680657,6808680658,6808680672,6810511344,6810511351,6810511362,6810511368,6810511406,6810511411,6810511440,6812766566,6812882166,6813542902],"issue_titles":["ConnectionError: Error 32 while writing to socket. Broken pipe.","TimeoutError: Timeout reading from socket","TimeoutError: Timeout connecting to server"],"root_cause_summaries":["No summary found","Redis cluster instability causes `MovedError`, but aggressive 3-second socket timeout prevents successful re-routing, leading to `TimeoutError`.","Redis cluster instability and performance degradation caused socket read timeouts, overwhelming application retry mechanisms.","Redis cluster instability, network latency, and connection drops cause persistent timeouts during socket reads.","Sequential, unbatched Redis GETs in `invalidate_project_config` task, combined with Redis cluster instability under high load, cause socket read timeouts and task failure.","Redis cluster instability causes `MovedError` and delays, exceeding client's 3-second socket timeout during lock acquisition.","Redis socket timeout during cluster redirection, due to insufficient 3-second client timeout.","Redis cluster instability causes socket closures, leading to read timeouts during event data retrieval.","Large event payload, Redis cluster instability, and short socket timeout cause Redis SET operation to time out.","Redis cluster instability causes socket closure, leading to `TimeoutError` during `get_last_upload`.","Redis socket timeout too short for cluster re-sharding, preventing `RetryingRedisCluster` from handling `MovedError`.","Redis client's 3-second socket timeout is too short for transaction event data retrieval, causing connection closure and read failures.","Redis cluster instability triggers client redirection; 3-second socket timeout prevents client from re-establishing connection and completing data retrieval.","Redis timeout during non-critical cache lookup blocks critical symbolication, due to lack of error handling.","Aggressive 3-second Redis socket timeout combined with cluster instability causes `TimeoutError` during event storage.","Redis client fails to handle cluster redirection, prematurely closing socket, causing subsequent operations to timeout.","Redis cluster instability and network issues, combined with an aggressive 3-second socket timeout, cause Redis GET operations to fail.","Redis cluster instability, due to re-sharding and network latency, caused GET operation timeouts for project configurations.","Redis client misconfiguration: Sentry uses single-node client for Redis Cluster, causing timeouts on MOVED redirections.","Redis cluster instability causes socket timeouts during non-critical cache cleanup, leading to task failure.","Redis cluster instability, marked by `MovedError` and `TimeoutError`, prevents distributed lock acquisition and release, causing task failures.","Redis cluster instability/high load combined with aggressive 3-second socket timeout causes `post_process_group` task failures.","Redis timeouts occur due to cluster overload and connection saturation from high event processing volume and cascading seer automation.","Concurrent Redis connection validation disconnects active connections, causing subsequent operations to fail on closed sockets.","Redis connections drop mid-operation due to network instability or server overload, causing socket read timeouts on closed sockets.","Redis Cluster rebalancing causes `MOVED` redirection, leading to client socket closure and subsequent read timeout during reconnection.","Redis cluster instability and network issues cause `MovedError` and connection closure, leading to `TimeoutError` during data retrieval.","Redis cluster instability, indicated by `MovedError` and high latency, caused socket read timeouts during similarity indexing operations.","Redis cluster instability causes `MovedError`s, leading to client redirection attempts that exceed socket read timeouts.","Redis cluster instability triggers `MovedError`s; `RetryingRedisCluster`'s recovery exceeds 3-second socket timeout, causing `TimeoutError`.","Network instability and aggressive 3-second Redis socket timeout cause `BrokenPipeError` during `SMEMBERS` operation in `post_process_group` task.","Redis cluster instability causes `MovedError` and connection closure; client attempts I/O on closed socket, leading to `TimeoutError`.","Redis client's 3s socket timeout is too aggressive for cluster `MOVED` redirection, causing `TimeoutError` during cache deletion.","Redis cluster instability causes socket closures, leading to `TimeoutError` when reading from terminated connections.","Redis cluster instability, evidenced by `MovedError`s, causes socket timeouts during client redirection due to network latency or node unresponsiveness.","Redis cluster instability causes `delete` operation to time out, exceeding task's processing deadline.","Redis client's 3s socket timeout during cluster reconfigurations causes cleanup failures.","Redis cluster instability causes `MovedError`, triggering connection re-establishment exceeding the 3-second socket timeout, resulting in `TimeoutError`.","Redis cluster rebalancing combined with aggressive 3-second socket timeout causes transaction cleanup failures.","Redis cluster instability, large event payloads, and a 3-second socket timeout cause retry failures.","Redis cluster instability caused socket closure during symbol source retrieval, leading to a TimeoutError in the symbolication task.","Redis cluster instability causes connection closures, which `RetryingRedisCluster` fails to handle, leading to `TimeoutError` during data retrieval.","Redis connections drop during minhash indexing due to server overload, causing client-side socket read timeouts on already closed connections.","Redis cluster instability, high load, or reconfigurations cause socket read timeouts during event data retrieval.","High-frequency, non-critical Redis calls for feature adoption block event processing due to Redis cluster instability and short client timeouts.","Redis cluster instability causes `MovedError`; `RetryingRedisCluster` fails to reconnect within timeout, resulting in `TimeoutError`.","Redis connection closed due to system strain; subsequent read attempt on closed socket timed out.","Redis cluster instability causes `MOVED` errors, leading to socket read timeouts during event data retrieval.","Redis cluster instability combined with an aggressive 3-second socket timeout prevents `RetryingRedisCluster` from recovering connections, causing `TimeoutError`.","RetryingRedisCluster fails to recover from Redis cluster topology changes, leading to closed sockets and subsequent timeouts.","Redis Cluster `MOVED` redirection timed out during socket read, indicating cluster instability or network latency preventing timely key retrieval.","Redis client fails to handle cluster topology changes within 3s timeout, triggered by dynamic sampling tasks.","Redis cluster instability, evidenced by `MovedError` and socket timeouts, prevents `get_last_upload` from completing, causing symbolication task failure.","Redis cluster instability causes `MovedError` during large data retrieval, leading to socket closure and subsequent `TimeoutError`.","Redis cluster instability causes `MovedError` during key deletion, leading to `TimeoutError` during client's retry and re-discovery.","Redis connection pool exhaustion, exacerbated by cluster instability, caused socket read timeouts during event storage.","Redis cluster consistently fails to respond within 3s socket timeout, despite client-side retries, indicating infrastructure performance degradation.","Redis cluster instability, aggressive 3-second timeout, and large event payloads cause socket read timeouts during retries.","Redis cluster rebalancing closes client socket; client attempts read on closed socket, causing TimeoutError.","Redis cluster instability combined with aggressive 3-second client timeout prevents successful `DEL` operation.","Redis Cluster instability or network issues caused socket timeouts during data retrieval, leading to task failure.","Redis cluster instability causes `MovedError`, leading client to attempt reads on closed sockets, resulting in `TimeoutError`.","Redis cluster instability causes `MovedError`, `RetryingRedisCluster` retries, but 3s socket timeout insufficient during reconfigurations.","Redis cluster instability and lack of client-side timeouts cause socket read failures during event data retrieval.","Redis cluster instability and network latency caused connection timeouts during event data retrieval, failing the `save_event_transaction` task.","Redis cluster instability causes MOVED errors, leading to socket timeouts during client redirection attempts for cache key retrieval.","Redis cluster topology changes, combined with an aggressive 5-second socket timeout and lack of retry logic, cause `save_event` task failures.","Redis Cluster connection management fails during topology changes, causing socket closure and subsequent read timeout.","Redis cluster instability causes socket closures during `DEL` operations, leading to `TimeoutError` when client attempts to read from dead connections.","Redis cluster instability causes client socket read timeouts during cache deletion due to redirection failures.","Redis cluster instability causes socket timeouts during event cache deletion, triggered by normal event discarding.","RetryingRedisCluster lacks TimeoutError handling, causing immediate task failure during Redis cluster instability.","Redis cluster instability, evidenced by `MovedError` and network timeouts, prevents `symbolicate_js_event` from completing its Redis operations.","Redis cluster instability during topology reconfiguration causes `MovedError`, leading to persistent socket `TimeoutError` despite client retries.","Redis cluster instability/overload causes socket timeouts during event data storage, despite client retries.","Transaction processing Redis cluster instability causes socket read timeouts during event data retrieval.","Redis cluster instability causes socket closure during topology changes, leading to client read timeouts.","Redis cluster client fails to re-establish connection after MOVED error, leading to attempts to read from a closed socket and subsequent timeout.","Redis cluster timeouts during event cache deletion, triggered by normal HashDiscarded exceptions, indicate infrastructure performance issues.","Redis cluster instability and network latency cause socket timeouts during GET operations, preventing event data retrieval.","Redis cluster slot migration closes socket; subsequent operation on same key attempts to use closed socket, causing TimeoutError.","Redis cluster instability causes socket timeouts during event data retrieval, failing symbolication tasks due to absent retry logic.","Redis client's missing `socket_connect_timeout` combined with cluster instability caused connection timeouts during key deletion.","Redis cluster instability combined with a 3-second socket timeout causes connection failures.","Redis cluster instability and high system load caused socket read timeouts during event data retrieval, leading to task failure.","Redis Cluster instability causes socket closure, leading to `TimeoutError` during cache cleanup.","Redis cluster instability causes `MovedError`s, leading to excessive client redirections that exceed the 3-second socket timeout during cache cleanup.","Redis cluster reconfigures; client's connection attempts to new nodes time out due to aggressive 3-second socket timeout.","Redis cluster instability and topology changes cause socket timeouts during pipeline execution, exceeding the 3-second client timeout.","Redis cluster instability combined with aggressive 3-second socket timeout causes `TimeoutError` during cache cleanup.","Redis cluster instability or network issues cause socket closure, leading to `TimeoutError` during `get` operation.","Redis client's lack of robust cluster topology refresh and retry logic causes timeouts during cleanup operations amidst cluster instability.","Redis cluster instability and performance degradation cause socket read timeouts during event post-processing.","Redis client times out on closed socket during cluster reconfigurations, due to aggressive timeout and retry logic limitations.","Redis client's connection pool uses stale connections to a changing cluster, causing socket read timeouts on already-closed sockets.","Redis cluster instability, evidenced by `MovedError` and unresponsiveness, causes socket read timeouts during symbolication's Redis `GET` operation.","Redis cluster instability and network issues cause socket timeouts during event storage, exceeding client retry capabilities.","Redis cluster instability and network latency caused socket read timeouts during symbolication data retrieval.","Redis cluster instability and overload caused socket timeouts and `MovedError` during lock release, leading to task failure.","Outdated Redis client fails to handle cluster `MOVED` redirection, causing premature socket closure and subsequent read timeout.","Redis cluster instability caused `MovedError` and subsequent socket read timeouts during key deletion, despite client retries.","Redis cluster rebalancing combined with aggressive 3-second socket timeouts causes `TimeoutError` during cache cleanup.","Redis cluster rebalancing caused socket closure, leading to `TimeoutError` when `symbolicate_js_event` tried to read event data.","Redis cluster instability combined with aggressive 3-second client socket timeout causes `TimeoutError` during `DEL` operations.","Redis cluster instability and network issues cause socket timeouts during event data deletion, overwhelming the single retry mechanism.","Redis client timed out connecting to a cluster node after a MOVED redirect, indicating the target node was unreachable.","Redis socket connection timeout during cluster instability, due to aggressive 3-second timeout preventing retry mechanism completion.","Redis cluster instability and unresponsiveness caused socket timeouts during critical event data retrieval, leading to task failure.","Redis cluster instability combined with aggressive 3-second client socket timeout causes persistent `TimeoutError`.","Redis cluster instability and network issues cause socket timeouts during transaction data retrieval, failing the task.","Redis cluster instability, likely due to re-sharding or node issues, caused connection closure and subsequent read timeout during symbolication.","Redis client's 3-second socket timeout is too short for cluster instability, causing timeouts during MOVED error handling.","Redis cluster instability causes `MovedError`, leading `RetryingRedisCluster` to timeout during socket read while attempting retry/redirection.","Redis server overload or network issues caused socket read timeouts during feature adoption data retrieval, impacting event processing.","Redis cluster instability and network issues to a specific node cause connection timeouts during span data loading.","Redis cluster instability, evidenced by `MovedError` and socket timeouts, prevents timely `DEL` operations during event discard cleanup."],"transactions":["sentry.models.featureadoption in get_all_cache","sentry.tasks.store.save_event","sentry.issues.tasks.post_process.post_process_group","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","sentry.tasks.store.symbolicate_event","sentry.tasks.symbolicate_js_event","sentry.relay.projectconfig_cache.redis in get","/api/0/relays/projectconfigs/","sentry.tasks.store.save_event_transaction","sentry.tasks.symbolicate_jvm_event","sentry.spans.buffer in _load_segment_data","sentry.tasks.store.process_event","sentry.utils.kvstore.redis in delete","sentry.tasks.process_commit_context","sentry.profiles.task.process_profile","sentry.tasks.relay.invalidate_project_config","sentry.lang.native.sources in get_last_upload","sentry.utils.kvstore.redis in get","sentry.tasks.process_suspect_commits"],"title":"Redis socket timeouts in project config and feature cache","description":"Multiple code paths that read from Redis (projectconfig cache, debounce checks, and feature adoption sets) are timing out on socket reads, causing request handling and background processing to stall. Impacted components include project configuration build/invalidation and feature adoption caching, indicating Redis cluster read latency or connectivity issues.","tags":["Caching","External System","Networking","Redis","Timeout","Socket Read","Project Config Cache"],"cluster_size":120,"cluster_min_similarity":0.9055206591213764,"cluster_avg_similarity":0.9488941784327675},{"project_ids":["300688"],"cluster_id":186,"group_ids":[6640349916,6752846655,6752847559],"issue_titles":["<unknown>"],"root_cause_summaries":["Kafka consumer's processing strategy blocked during rebalance revocation, exceeding session timeout, causing group coordinator disconnection.","Kafka consumer's `join()` blocks too long during rebalance, exceeding `max.poll.interval.ms`, causing broker timeout.","Kafka GroupCoordinator unresponsiveness caused consumer rebalance timeouts, leading to a 40-hour client-side operation timeout."],"transactions":[],"title":"Kafka consumer broker transport failures and timeouts","description":"librdkafka reports BrokerTransportFailure and OperationTimedOut while talking to the GroupCoordinator for multiple topics, leading to disconnects despite the connection being UP. This suggests intermittent broker/network issues impacting Kafka consumer coordination.","tags":["Networking","Queueing","Kafka","Connection Reset","Timeout","librdkafka","Group Coordinator"],"cluster_size":3,"cluster_min_similarity":0.9431014729676227,"cluster_avg_similarity":0.9538044324169498},{"project_ids":["1"],"cluster_id":192,"group_ids":[6645390854,6647084861,6654721083,6722344332,6722363997,6722364712,6722364920,6722422683,6722450323,6722453956],"issue_titles":["ApiError: {\"message\":\"404 Project Not Found\"}","IntegrationError: Error Communicating with GitHub (HTTP 404): If this repository exists, ensure that your installation has permission to access this repository (https://github.com/settings/installations). Please also confirm that the commits associated with the following...","ApiError: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/commits/commits#list-commits\",\"status\":\"404\"}"],"root_cause_summaries":["GitHub API returned 404 for commit comparison, indicating invalid or inaccessible commit SHAs in the repository.","Sentry's internal integration proxy returned 404, indicating misconfiguration or unavailability of the proxy service.","Sentry's database contains a stale GitHub repository record, causing 404 API errors when fetching commits.","GitHub API 404s on commit fetch due to repository/commit non-existence or permission issues.","GitHub API returned 404 for commit, indicating repository access or existence issue for the Sentry App.","Sentry's internal integration proxy returned a 404, preventing GitHub API access, leading to a misleading IntegrationError.","GitLab project inaccessible; Sentry's background task failed fetching commits due to a 404 from GitLab API.","GitHub API returned 404 for commit comparison, indicating repository or commits are missing/inaccessible.","Sentry's database contains stale repository data; GitHub API returns 404 for non-existent/inaccessible repository, causing task failure.","Sentry's database holds a stale GitHub repository reference; subsequent commit fetching attempts fail due to GitHub's 404 response."],"transactions":["sentry.shared_integrations.client.base in _request","sentry.tasks.commits.fetch_commits"],"title":"GitHub repo/commit lookups return 404 Not Found","description":"Calls to GitHub compare/commits endpoints are returning 404s, causing IntegrationErrors when fetching commit data. Likely causes include missing repository access/permissions for the installation or commits not yet pushed to GitHub.","tags":["External System","API","Authentication","GitHub","HTTP 404","IntegrationError"],"cluster_size":10,"cluster_min_similarity":0.9138376924106687,"cluster_avg_similarity":0.9541293828995259},{"project_ids":["1"],"cluster_id":195,"group_ids":[6645907695,6646064078,6646179202,6647132217,6681593508,6681629674,6682729285,6690989471,6722363401,6722365880,6722376057,6722404401,6722422914,6722437677,6725532103,6726096159,6726096161,6726201855,6729935341,6731269704,6732284370,6732556918,6735256979,6735902130,6739785018,6747643418,6750326362],"issue_titles":["ApiError: {\"$id\":\"1\",\"innerException\":null,\"message\":\"TF401029: Couldn't find Git commit with ID ccc4ce867b1077fab46b5486557247956b796655.\",\"typeName\":\"Microsoft.TeamFoundation.Git.Server.GitCommitDoesNotExistException, Microsoft.TeamFoundation.Git.Server\",\"typeK...","IntegrationError: Error Communicating with Bitbucket (HTTP 404): Commit not found","ApiError: {\"type\": \"error\", \"error\": {\"message\": \"Commit not found\", \"data\": {\"shas\": [\"df49175000000000000000000000000000000000\"]}}, \"data\": {\"shas\": [\"df49175000000000000000000000000000000000\"]}}","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID 7f9d96d7d6254d9131268f9ad1df7a186b8b13a5.","ApiError: {\"$id\":\"1\",\"innerException\":null,\"message\":\"TF401029: Couldn't find Git commit with ID f5f0c106761729d69ca5633a476e36bc7d31fda6.\",\"typeName\":\"Microsoft.TeamFoundation.Git.Server.GitCommitDoesNotExistException, Microsoft.TeamFoundation.Git.Server\",\"typeK...","ApiError: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/commits/commits#compare-two-commits\",\"status\":\"404\"}","ApiError: {\"message\":\"404 Project Not Found\"}","ApiError: {\"$id\":\"1\",\"innerException\":null,\"message\":\"TF401029: Couldn't find Git commit with ID 400056ff86c4e5703f39cac885b0d8533e9b3ad2.\",\"typeName\":\"Microsoft.TeamFoundation.Git.Server.GitCommitDoesNotExistException, Microsoft.TeamFoundation.Git.Server\",\"typeK...","IntegrationError: Error Communicating with Bitbucket (HTTP 404): unknown error","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID 231ebdd078928f2840f4a93165a6eb9735613e4c.","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID cbe4be78a3a1ffc9832e3cff90611045104f042a.","ApiError: {\"type\": \"error\", \"error\": {\"message\": \"Commit not found\", \"data\": {\"shas\": [\"35c55bd4e91ba81b358c8b2702aa9346aaa42b07\"]}}, \"data\": {\"shas\": [\"35c55bd4e91ba81b358c8b2702aa9346aaa42b07\"]}}","ApiError: {\"type\": \"error\", \"error\": {\"message\": \"Commit not found\", \"data\": {\"shas\": [\"3a6363762a4db92d97a699b792e8ae18b528546d\"]}}, \"data\": {\"shas\": [\"3a6363762a4db92d97a699b792e8ae18b528546d\"]}}","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID fc888642c2fc939d51c097e4c2259849f1f93216.","IntegrationError: Error Communicating with GitHub (HTTP 404): If this repository exists, ensure that your installation has permission to access this repository (https://github.com/settings/installations). Please also confirm that the commits associated with the following...","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID dca7783f2595150eb34e01510d987d6f88620cfc.","IntegrationError: Error Communicating with GitLab (HTTP 404): 404 Ref Not Found","ApiError: {\"message\":\"404 Ref Not Found\"}","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID 494c4e446e407dee6fb2979f1d0760437c81f73a.","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401029: Couldn't find Git commit with ID 2963fdee90f6fa6ed4c0de23ec30eb045ccebc56.","ApiError: {\"type\": \"error\", \"error\": {\"message\": \"Commit not found\", \"data\": {\"shas\": [\"a9f5d2fb41c9de837d0967de898228c97b37cf77\"]}}, \"data\": {\"shas\": [\"a9f5d2fb41c9de837d0967de898228c97b37cf77\"]}}"],"root_cause_summaries":["API request provided non-existent commit SHA, causing Bitbucket 404, leading to Sentry's `ApiError` during commit fetch.","Bitbucket API returned 404 for provided commit SHA, causing Sentry's fetch_commits task to fail.","Commit SHA not found in one of the referenced repositories, causing Azure DevOps API to return 404.","Sentry's stored commit SHA for a previous release became invalid in GitLab, causing a '404 Ref Not Found' error during commit comparison.","Bitbucket commit SHA became inaccessible due to git history rewriting, causing Sentry's fetch to fail.","GitLab API returned 404 for requested commit SHA, indicating the commit does not exist in the repository.","Bitbucket API returned 404 for non-existent commit, causing `fetch_commits` task to fail.","Bitbucket API reported commit '0965943a096172a26d76fbcfc49f056b3c5276b4' not found in 'Codemind-team/leiregister' repository.","Bitbucket API returned 404 for a commit SHA, indicating the commit does not exist in the specified repository.","Release '138b783' was associated with a non-existent commit SHA '138b783000000000000000000000000000000000', causing Bitbucket API 404.","Release data contained same commit ID for two distinct repositories, causing Azure DevOps to report commit not found for both.","Sentry's `fetch_commits` task failed because the release's commit SHA was not found in the Bitbucket repository.","Bitbucket API returned 404 for commit SHA, because the referenced commit does not exist in the repository.","Sentry's database incorrectly stored a commit ID for 'Generative_AI' repository, causing Azure DevOps API calls to fail.","Sentry's `ReleaseHeadCommit` table stores stale Git commit references, causing API calls to external Git providers to fail when history is rewritten.","Release creation incorrectly mapped a commit to a repository where it didn't exist, causing Azure DevOps to return a 404.","GitLab API returned '404 Ref Not Found' for provided commit SHAs, causing Sentry's commit fetching to fail.","GitHub API 404 due to Sentry using a non-existent `start_sha` from its database for commit comparison.","GitHub API returned 404 for commit SHA, indicating the commit `0847d8a4673e3df2278d5480d7ee2dd76f98df2d` does not exist in `0-1-no/Norea-APP`.","Sentry's stored Git commit SHA for a previous release became invalid in Azure DevOps due to history rewrite, causing API lookup failure.","Release defined same Git commit SHA for multiple independent repositories; one repository lacked the specified SHA.","GitHub API returns 404 for commit comparison due to stale `start_sha` from Sentry's database, caused by repository history rewriting.","Azure DevOps reported commit not found for 'yuki-app' repository, despite the same commit existing in 'yuki' repository.","GitLab API returned 404 because the `start_sha` from a previous release no longer exists in the repository's history.","Sentry's stale commit reference from a previous release caused a 404 when fetching from Azure DevOps after Git history rewrite.","Sentry's stored previous commit SHA was not found in Azure DevOps due to Git history rewrite, causing commit fetch failure.","GitHub API returned 404 because Sentry's stored previous commit SHA was invalidated by Git history rewrite."],"transactions":["sentry.tasks.commits.fetch_commits","sentry.shared_integrations.client.base in _request"],"title":"Commit comparison fails due to missing SHAs in VCS","description":"Fetching commit ranges in the commits worker triggers 404s from Bitbucket and Azure DevOps because the referenced commit SHAs are not found upstream. This likely stems from invalid or pruned commits, incorrect repo/branch mapping, or stale references in our integration clients.","tags":["External System","API","Data Integrity","Upstream Unavailable","Bitbucket","Azure DevOps","Commit Not Found"],"cluster_size":27,"cluster_min_similarity":0.910046375661909,"cluster_avg_similarity":0.9494153361552181},{"project_ids":["1"],"cluster_id":196,"group_ids":[6645924599,6709177701,6805493282],"issue_titles":["SentryAppSentryError: event_not_in_servicehook"],"root_cause_summaries":["ServiceHook lacks 'issue.unresolved' event subscription, causing webhook dispatch failure.","System attempted sending 'issue.unresolved' webhook to Sentry App not subscribed to that event type.","ServiceHook for 'Canva Issue Assigner' lacked 'issue.unresolved' event, causing webhook dispatch failure."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.workflow_notification"],"title":"Sentry App webhook sent for unsupported event","description":"Workflow notifications attempt to send webhooks for an event not listed in the apps service hook configuration, triggering SentryAppSentryError: event_not_in_servicehook in Celery workers. This prevents the webhook from being delivered for that event type.","tags":["API","Configuration","Celery","Sentry Apps","Unsupported Event"],"cluster_size":3,"cluster_min_similarity":0.9642721335049423,"cluster_avg_similarity":0.9704146953752897},{"project_ids":["1"],"cluster_id":197,"group_ids":[6645949938,6646263329,6725560110,6739115477],"issue_titles":["ApiError: status=400 body={'detail': ErrorDetail(string=\"Invalid value '['KRAKEN-CORE-1SF1']' for 'issue:' filter\", code='parse_error')}","ApiError: status=400 body={'detail': ErrorDetail(string='user.email is not a tag in the metrics dataset', code='parse_error')}","ApiError: status=400 body={'detail': ErrorDetail(string='host is not a tag in the metrics dataset', code='parse_error')}","ApiError: status=400 body={'detail': ErrorDetail(string='Cannot query apdex with a threshold parameter on the metrics dataset', code='parse_error')}"],"root_cause_summaries":["Performance alert chart generation incorrectly maps 'generic_metrics' to 'metrics' dataset, causing 'user.email' tag error.","Alert rule configured `http.url` query for `metrics` dataset, which lacks `http.url` tag, causing API rejection.","Apdex(threshold) on generic_metrics dataset fails chart generation due to API incompatibility with 'metrics' dataset.","Alert rule's query tag incompatible with its configured dataset's API mapping."],"transactions":["sentry.incidents.tasks.handle_trigger_action"],"title":"Metric alert chart fetch fails in notifications pipeline","description":"Alert rule notifications fail when fetching timeseries data for metric alert charts via the internal client, raising ApiError and aborting email/notification delivery. Impact: incident fire/resolve emails and message attachments lack charts or fail to send.","tags":["API","Notifications","Alerting","HTTP Error","Timeseries Data","Metric Alerts"],"cluster_size":4,"cluster_min_similarity":0.9445475304669149,"cluster_avg_similarity":0.9563960804223179},{"project_ids":["1"],"cluster_id":199,"group_ids":[6645950709,6753787646],"issue_titles":["ApiInvalidRequestError"],"root_cause_summaries":["GitLab API returned 400 for PR diffs, indicating Sentry's stored project/PR IDs are invalid or non-existent in GitLab.","GitLab API returned 400 for PR diffs, indicating Sentry's stored PR/project IDs are invalid or inaccessible in GitLab."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow"],"title":"Bad Request when fetching PR diffs blocks comment workflow","description":"The open_pr_comment_workflow fails while calling client.get_pr_diffs, which returns an HTTP 400 and raises ApiInvalidRequestError/HTTPError. This prevents determining safe files for commenting and aborts the PR comment flow.","tags":["API","External System","Input Validation","HTTP 400","Pull Request Diffs","Celery"],"cluster_size":2,"cluster_min_similarity":0.9674692407438021,"cluster_avg_similarity":0.9674692407438021},{"project_ids":["1"],"cluster_id":200,"group_ids":[6645954958,6646038479,6646958211,6646958219,6650608445,6679017310,6679017314,6689527288,6712951101,6722325441,6722353062,6722354059,6722421080,6722421956,6722426456,6722438059,6724451109,6731616583,6754287556,6792396723,6803496843],"issue_titles":["ApiRetryError: SafeHTTPConnectionPool(host='sentry-rpc-control.psc.de.sentry.internal', port=8999): Max retries exceeded with url: /api/0/internal/integration-proxy/ (Caused by ResponseError('too many 503 error responses'))","IntegrationError: Error Communicating with GitLab (HTTP 503): unknown error","ApiRetryError: SafeHTTPConnectionPool(host='sentry-rpc-prod-control.us.sentry.internal', port=8999): Max retries exceeded with url: /api/0/internal/integration-proxy/ (Caused by ResponseError('too many 503 error responses'))"],"root_cause_summaries":["GitHub GraphQL API returned 503s to Sentry's proxy, causing taskworker's retries to exhaust.","Sentry's internal integration proxy returned repeated 503s, exhausting retries for GitLab API calls.","Sentry's Control Silo failed to connect to the external GitLab instance, causing the integration proxy to return 503s and exhausting retries.","External GitLab service `awakeninggit.e2enetworks.net` returned 503s, exhausting Sentry's retries, preventing PR diff retrieval.","External GitLab API consistently returns 503 Service Unavailable, causing Sentry's integration proxy to exhaust retries and raise `ApiRetryError`.","Internal `sentry-rpc-control` proxy returned 503s, exhausting retries, preventing GitLab communication.","External GitLab instance repeatedly returns 503s to Sentry's integration proxy, exhausting retries and causing `ApiRetryError`.","Control Silo integration proxy service returned persistent 503s, exhausting retries, preventing GitLab API access.","Sentry's internal integration proxy service consistently returns 503 errors, preventing external API calls and causing `ApiRetryError`.","Control silo failed connecting to external GitLab API, causing repeated 503s and MaxRetryError, propagating to region silo as IntegrationError.","GitLab API returned 503 \"Service Unavailable\" to Sentry's integration proxy, exhausting retries and causing an IntegrationError.","Sentry's internal integration proxy service returned too many 503 errors, exhausting retries for GitLab API requests.","Control silo returns 503s for proxied GitHub API requests, exhausting retries.","Integration proxy service at `sentry-rpc-control` repeatedly returned 503s, exhausting retries and failing GitLab commit fetch.","Control silo's integration proxy failed to connect to GitLab API, causing repeated 503s and subsequent `ApiRetryError` in region silo.","Sentry's control silo integration proxy service returned 503s, exhausting retries for GitLab API calls, causing `ApiRetryError`.","Internal proxy service `sentry-rpc-prod-control` repeatedly returned 503s, exhausting retries for GitHub API requests.","Internal integration proxy service consistently returned 503s, exhausting retries and causing API request failure.","Control silo service `sentry-rpc-control.psc.de.sentry.internal:8999` repeatedly returned 503s, exhausting retries.","Control silo integration proxy returned HTTP 503s, exhausting retries, preventing GitLab API access.","GitHub API rejects requests from Sentry's control silo with 503s, exhausting retries and causing task failure."],"transactions":["sentry.tasks.process_commit_context","sentry.tasks.commits.fetch_commits","sentry.shared_integrations.client.base in _request","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow"],"title":"GitLab integration proxy returns repeated errors","description":"Calls to the internal integration-proxy for GitLab (compare commits, blame, PR diffs) are repeatedly failing with too many error responses, exhausting retries in Celery workers.","tags":["External System","API","Queueing","Retries Exhausted","HTTP","GitLab","Integration Proxy"],"cluster_size":21,"cluster_min_similarity":0.9477231183812649,"cluster_avg_similarity":0.9652638323040649},{"project_ids":["1"],"cluster_id":201,"group_ids":[6645955036,6683082299,6726976292,6733859347,6740656369,6745998189],"issue_titles":["IntegrationError: Error Communicating with GitLab (HTTP 400): unknown error","ApiInvalidRequestError"],"root_cause_summaries":["GitLab API returned 400 Bad Request because a provided commit SHA did not exist in the repository.","GitLab API rejected commit comparison due to a non-existent `start_sha` from a previous release, caused by Git history rewrite.","GitLab API returned 400 for commit's merge requests, likely due to non-existent commit or insufficient permissions.","GitLab API returned 400 Bad Request for a specific commit's diff, causing Sentry's `fetch_commits` task to fail.","GitLab API returned 400 for commit comparison, likely due to invalid SHAs or project access.","GitLab API rejects commit comparison when start and end SHAs are identical, causing an IntegrationError."],"transactions":["sentry.tasks.commits.fetch_commits","sentry.tasks.process_commit_context"],"title":"GitLab API bad requests in commit fetching","description":"Workers calling GitLab endpoints for compare_commits, diffs, and merge-commit lookups are receiving 4xx Bad Request responses, surfaced as ApiInvalidRequestError and IntegrationError. This likely indicates invalid request parameters (e.g., SHAs or project_id) or API contract mismatch causing commit retrieval to fail.","tags":["API","External System","Input Validation","GitLab","Bad Request","IntegrationError","ApiInvalidRequestError"],"cluster_size":6,"cluster_min_similarity":0.93201409368453,"cluster_avg_similarity":0.9491096347255414},{"project_ids":["1"],"cluster_id":202,"group_ids":[6645961059,6696577106,6751806337],"issue_titles":["ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"],"root_cause_summaries":["Incident chart generation fails because `organizations:incidents` feature flag is disabled for the organization, causing a 404 from the incidents API endpoint.","Incident chart generation failed because the required 'organizations:incidents' feature flag was not enabled for the organization.","Incident chart generation failed due to disabled `organizations:incidents` feature flag."],"transactions":["sentry.incidents.tasks.handle_trigger_action"],"title":"Metric alert email fails fetching incident periods","description":"Alert rule resolution throws ApiError when charts.fetch_metric_issue_open_periods issues a GET via the client to build metric alert charts, causing incident email notifications to fail.","tags":["API","External System","Alerting","HTTP Error","Metric Alerts","Email Notifications"],"cluster_size":3,"cluster_min_similarity":0.9607818410477111,"cluster_avg_similarity":0.9644410627614678},{"project_ids":["1"],"cluster_id":203,"group_ids":[6646033609,6709178653,6770882747,6770882750,6770882756,6770882763,6770882764,6803878240],"issue_titles":["SentryAppSentryError: missing_servicehook"],"root_cause_summaries":["SentryApp installation lacked ServiceHook because webhook_url was initially null, causing subsequent webhook dispatch failure.","ServiceHook was deleted when webhook URL became null, not recreated when URL restored, causing missing_servicehook error.","ServiceHook record missing for Sentry App installation due to creation failure, preventing webhook delivery.","SentryAppInstallation exists without corresponding ServiceHook, preventing webhook dispatch due to missing configuration.","ServiceHook deletion on webhook URL clear, without recreation on URL restore, causes missing_servicehook error.","Sentry App installation lacks a corresponding ServiceHook database record, causing webhook delivery failure.","ServiceHook missing for installation, preventing webhook processing due to data integrity failure.","ServiceHook deleted when webhook URL temporarily cleared, not recreated upon restoration, causing subsequent webhook delivery failures."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook","sentry.sentry_apps.tasks.sentry_apps.workflow_notification"],"title":"Sentry app webhooks failing: missing servicehook","description":"Webhook dispatch in Sentry Apps raises SentryAppSentryError (MISSING_SERVICEHOOK), indicating installations lack required service hooks for workflow/resource change events, causing webhook notifications to fail.","tags":["API","Configuration","Sentry","Webhook","Missing Servicehook"],"cluster_size":8,"cluster_min_similarity":0.9377727229067816,"cluster_avg_similarity":0.9641341748005802},{"project_ids":["1"],"cluster_id":208,"group_ids":[6646044455,6765627430],"issue_titles":["IntegrationError: Error Communicating with Azure DevOps (HTTP 400): unknown error"],"root_cause_summaries":["Azure DevOps API request failed due to outdated API version 4.1, causing a 400 Bad Request.","Azure DevOps integration fails due to `VstsApiClient` using outdated API version `4.1` for `get_work_item` endpoint."],"transactions":["sentry.integrations.tasks.sync_status_outbound","sentry.tasks.commits.fetch_commits"],"title":"Azure DevOps API returns 400 on commits and work items","description":"Requests to Azure DevOps for commit ranges and work items are failing with HTTP 400 Bad Request, causing IntegrationError in fetch_commits and sync_status_outbound paths. Likely due to invalid request payloads or parameters sent to the Azure DevOps REST API.","tags":["External System","API","Azure DevOps","HTTP 400","Bad Request","IntegrationError"],"cluster_size":2,"cluster_min_similarity":0.9665892709365943,"cluster_avg_similarity":0.9665892709365943},{"project_ids":["1"],"cluster_id":209,"group_ids":[6646066320,6646524862,6656766648,6671770559,6672049898,6674670273,6675798999,6675807636,6675814347,6675814372,6675819571,6675841661,6675907944,6725814159,6726137616,6730411124,6735569978,6741155229,6741549188,6745062715,6750152099,6766982777,6767448618,6782550449,6782727473,6783084592,6784672384,6785505629,6786593110,6792163774,6792611781,6793874696,6794700348,6796224647,6796765267,6799866345],"issue_titles":["RetryError: Timeout of 60.0s exceeded","RetryError: Timeout of 5.0s exceeded","ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.workflow_engine.processors.delayed_workflow"],"root_cause_summaries":["Bigtable nodestore unavailable, causing read timeouts and `RetryError` during event data fetch.","Bigtable read timeout due to aggressive 5s limit on background task, exacerbated by network latency.","Bigtable read timeout of 5s, optimized for ingestion, prematurely fails background task event fetching.","Bigtable read operation exceeded the explicit 5-second timeout, causing event fetching to fail.","Bigtable read timeouts occur because system overload causes Bigtable to respond slowly, exceeding the aggressive 5-second read timeout.","Bigtable `get_many` lacked explicit timeout, defaulting to 60s, blocking task until its 60s timeout, due to unresponsive Bigtable service.","Bigtable read operation for event data exceeded its 5.0-second timeout, causing post-processing task failure.","Bigtable read operations timed out due to network connectivity issues, causing gRPC connection failures.","Bigtable read for event data exceeded 5-second timeout, causing RetryError and API 500 response.","Bigtable read operation for event data exceeded its 5-second timeout, due to underlying Bigtable performance degradation.","Bigtable read timeout of 5.0s is too short for large IssueOccurrence data, causing DeadlineExceeded and RetryError.","Bigtable read timeout of 5s, optimized for ingestion, fails API requests for large events.","Bigtable reads time out due to an aggressive 5.0s timeout, causing `DeadlineExceeded` and `RetryError` during event data retrieval.","Bigtable read timeout due to large `IssueOccurrence` evidence data, exceeding the 5-second ingestion pipeline limit.","Bigtable read for event data exceeds 5-second timeout, causing `RetryError` due to aggressive timeout applied to background task.","Bigtable read for event data exceeded 5-second timeout, causing RetryError and task failure.","Bigtable read timeout due to aggressive 5s limit on non-critical forecast fetch, causing post-processing failure.","Bigtable `get_many` operation lacks reduced timeout, causing 60-second default to be exceeded when fetching event data.","Bigtable read timeout of 5.0s is too short for large IssueOccurrence data, causing `RetryError` and task failure.","Bigtable gRPC calls failed with 502 Bad Gateway, causing the 5-second read timeout to be exceeded.","Bigtable read for event data exceeded its 5-second timeout, causing a `DeadlineExceeded` error, which was re-raised as a `RetryError`.","Bigtable returned 502 errors; aggressive 5-second timeout prevented successful retries, causing task failure.","Bigtable connection timed out due to network failure, preventing forecast retrieval and causing task failure.","Bigtable gRPC connection timed out during nodestore read, due to network or Bigtable instance unresponsiveness.","Bigtable read operations exceed 5-second timeout, likely due to infrastructure performance degradation, causing `RetryError`.","Bigtable read for event data times out due to 5s limit, triggered by HEAD request unnecessarily fetching full payload.","Bigtable read of oversized event data exceeds 5-second timeout, causing workflow engine task failure.","Bigtable read operation for event data exceeded its 5-second timeout, causing a `RetryError` in the webhook task.","Bigtable `get_many` default 60s timeout matches task deadline, causing `ProcessingDeadlineExceeded` on slow reads.","Bigtable nodestore gRPC call to fetch full event data exceeded its 5.0s deadline, causing a RetryError.","Bigtable read operation timed out due to network connectivity issues exceeding the configured 5-second retry limit.","Bigtable read operations exceed 5s timeout due to system-wide infrastructure stress, causing task processing failures.","Bigtable read timeout due to aggressive 5s limit for large event data in background task.","Bigtable multi-row read defaulted to 60s timeout, exceeding acceptable latency, unlike single-row reads configured for 5s.","Bigtable read for event data exceeded 5s timeout, causing webhook delivery failure.","BigtableKVStorage.get_many lacks timeout optimization, causing batch reads of large event data to exceed 60s."],"transactions":["sentry.tasks.auto_source_code_config","sentry.tasks.post_process.post_process_group","sentry.workflow_engine.tasks.process_workflows_event","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/","sentry.sentry_apps.tasks.service_hooks.process_service_hook","/api/0/organizations/{organization_id_or_slug}/events/{project_id_or_slug}:{event_id}/","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/events/","sentry.utils.kvstore.bigtable in get","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2","sentry.workflow_engine.processors.delayed_workflow","/organizations/{organization_slug}/projects/{project_id_or_slug}/events/{client_event_id}/","sentry.issues.tasks.post_process.post_process_group"],"title":"Bigtable reads timing out via gRPC in nodestore","description":"Celery workers and web requests are timing out when reading nodes from the Bigtable-backed nodestore, with gRPC calls hitting DEADLINE_EXCEEDED. This causes post-processing and event retrieval to fail due to read timeouts and exhausted retries.","tags":["External System","Networking","Queueing","API","Timeout","Google Cloud Bigtable","gRPC","Celery"],"cluster_size":36,"cluster_min_similarity":0.909855183008694,"cluster_avg_similarity":0.952815577912363},{"project_ids":["1"],"cluster_id":210,"group_ids":[6646067487,6650595260,6651253545,6673412861,6707969055,6720053788,6724748849,6725435635,6726165735],"issue_titles":["IntegrationError: Gitlab requires an integration id.","IntegrationError: Bitbucket requires an integration id.","IntegrationError: Bitbucket Server requires an integration id."],"root_cause_summaries":["Repository 'sbs / mono' has a NULL `integration_id` in the database, preventing GitLab API calls.","Repository with 'integrations:bitbucket' provider had NULL integration_id, causing `get_installation` to fail.","Repository's GitLab provider requires an integration ID, but the database record has a null integration ID.","Bitbucket Server repository record lacks required `integration_id`, causing `IntegrationError` during commit fetch.","Repository's `integration_id` is NULL, but its provider is Bitbucket, which requires a non-NULL `integration_id`.","GitLab repository configured as integration-managed but lacks `integration_id` in database, causing `IntegrationError` during commit fetch.","Repository's `integration_id` is null, preventing Gitlab integration from fetching commits.","Repository's integration_id became NULL after integration removal, causing Bitbucket provider to fail requiring an ID.","Repository's integration_id became NULL after integration disassociation, causing fetch_commits to fail when requiring a valid integration."],"transactions":["sentry.tasks.commits.fetch_commits"],"title":"Commit fetch fails: missing integration ID","description":"Background commit-compare tasks for Bitbucket, Bitbucket Server, and GitLab fail because repositories lack an integration_id when resolving the installation in the provider. This blocks commit retrieval for affected repos until the integration is properly linked.","tags":["Configuration","API","Background Jobs","IntegrationError","Bitbucket","GitLab","Bitbucket Server"],"cluster_size":9,"cluster_min_similarity":0.9513959833831005,"cluster_avg_similarity":0.9666269332369877},{"project_ids":["1"],"cluster_id":211,"group_ids":[6646072468,6782868501],"issue_titles":["ApiInvalidRequestError: {\"text\":\"Incorrect index\",\"code\":7,\"invalid-event-number\":1}"],"root_cause_summaries":["Splunk integration failed due to an invalid or inaccessible index 'main' configured in Sentry.","Splunk API rejected event due to invalid index 'main', indicating a misconfiguration of the Splunk integration's index setting."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"Data forwarding fails with 400 Bad Request from plugin API","description":"Plugin-based data forwarding during post-process is sending an invalid event index, causing the upstream API to return ApiInvalidRequestError ('Incorrect index') and HTTP 400. This blocks plugin.post_process from completing in worker jobs.","tags":["API","Configuration","External System","Upstream Unavailable","Bad Request","Plugin","Data Forwarding"],"cluster_size":2,"cluster_min_similarity":0.9787996906154303,"cluster_avg_similarity":0.9787996906154303},{"project_ids":["1"],"cluster_id":213,"group_ids":[6646074554,6646421086,6684066154,6709306872,6715167251,6722329300,6722444895,6722447280,6735839509,6781194339,6792296919,6792643525,6792744042,6792744388,6793012233,6793019190,6793021693,6793047073,6793079082],"issue_titles":["ConnectionError: Error 32 while writing to socket. Broken pipe.","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4507644678569984.16'>> within 4.905 seconds (48 attempts.)","ConnectionError: Error 104 while writing to socket. Connection reset by peer.","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'usagebuffer.usage_flush_lock:4507974924238928'>> within 4.995 seconds (51 attempts.)","UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509060130144256:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509060130144256:IssueOwners::ActiveMembers'","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'usagebuffer.usage_flush_lock:4509469074522193'>> within 4.910 seconds (51 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'subscription:uptime_monitor:665d11bed7d9488fb2018937fee2ae75'>> within 9.896 seconds (99 attempts.)","UnableToAcquireLock: Unable to acquire <Lock: 'queue_comment_task:174730441'> due to error: Could not set key: 'l:queue_comment_task:174730441'","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4507528335982592.2'>> within 4.951 seconds (51 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'usagebuffer.usage_flush_lock:4507367898284032'>> within 4.998 seconds (47 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4504037644697600.16'>> within 4.988 seconds (50 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'deploy-notify:80919424'>> within 9.962 seconds (93 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'usagebuffer.usage_flush_lock:4503921285857280'>> within 4.897 seconds (14 attempts.)","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4507528335982592.2'>> within 4.935 seconds (50 attempts.)","UnableToAcquireLock: Unable to acquire <Lock: 'queue_comment_task:166454569'> due to error: Could not set key: 'l:queue_comment_task:166454569'","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4506667534254080.1'>> within 4.869 seconds (14 attempts.)","UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509106591039568:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509106591039568:IssueOwners::ActiveMembers'","RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4507427284123728.1'>> within 4.942 seconds (48 attempts.)","UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4508675955097680:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4508675955097680:IssueOwners::ActiveMembers'"],"root_cause_summaries":["Redis `maxclients` reached due to excessive connections from multiple task workers, causing lock acquisition failures and retry exhaustion.","Redis locks are not explicitly released, causing contention and acquisition failures until their TTL expires.","Redis server instability or resource exhaustion causes `client.set` to fail, preventing lock acquisition.","Concurrent project tasks contend for the same organization-level Redis lock, timing out due to a short retry window versus the lock's longer duration.","Concurrent tasks race for Redis lock; late `notified` check causes repeated lock acquisition failures and timeout.","Redis connection instability, exacerbated by high lock contention during billing operations, causes socket timeouts and broken pipes.","Redis lock duration (30s) is too short for task execution (20m), causing contention and failures during Redis overload.","Redis overload/network issues prevent lock acquisition, causing `RetryException` during spend allocation updates.","Digest processing exceeds lock duration, allowing rescheduling before completion, causing subsequent tasks to fail acquiring the same Redis lock.","Redis lock contention for spend allocation recording causes retry timeout due to long-running critical section.","Single task processing multiple usage items for same organization/category repeatedly fails to acquire non-reentrant Redis lock due to key persistence.","Redis lock acquisition failed repeatedly due to contention or unresponsiveness, indicating underlying Redis infrastructure issues.","Redis connection pool exhausted; `max_connections=16` insufficient for concurrent lock acquisition demand.","Lock contention on organization/category-level Redis lock due to concurrent project-level tasks, exceeding retry timeout.","Redis server or network infrastructure abruptly terminates connections during write operations, preventing lock acquisition.","Concurrent `process_commit_context` tasks for different issues linked to the same pull request contend for a shared Redis lock, causing `UnableToAcquireLock`.","Concurrent tasks for same pull request race for short-lived lock, failing before debouncing cache is set.","Taskworker concurrency and per-process Redis connection pools exceed Redis cluster's `maxclients` limit, causing connection exhaustion and lock acquisition failures.","Crashed task leaves Redis lock orphaned; subsequent attempts fail due to retry timeout being shorter than lock expiry."],"transactions":["getsentry.billing.tasks.usagebuffer.scan_usage_buffer","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","sentry.utils.retries in __call__","sentry.tasks.process_commit_context","sentry.utils.locking.lock in acquire","sentry.tasks.commits.fetch_commits"],"title":"Redis-backed locks fail to set key for tasks","description":"Multiple Celery tasks cannot acquire distributed locks because Redis fails to set the lock keys (e.g., spend_allocations, queue_comment_task, digests IssueOwners). This prevents workers from proceeding and triggers retries/timeouts.","tags":["Caching","Queueing","Concurrency","Redis","Distributed Lock","Could Not Set Key","Celery"],"cluster_size":19,"cluster_min_similarity":0.9069446166424625,"cluster_avg_similarity":0.9400853755646182},{"project_ids":["1"],"cluster_id":214,"group_ids":[6646105310,6646181615,6646384697,6678435494,6689961421,6689961440,6722361285,6722361332,6722394994,6722427582,6722458910,6722458917,6739091884,6786082523],"issue_titles":["IncompatibleMetricsQuery: Cannot query apdex with a threshold parameter on the metrics dataset","SubscriptionError: Cannot query apdex with a threshold parameter on the metrics dataset"],"root_cause_summaries":["Apdex function with threshold parameter used on metrics dataset, which only supports tag-based apdex.","Database `SnubaQuery` with `apdex(400)` for metrics dataset causes query builder to fail due to incompatible apdex parameter.","AlertRuleSerializer lacks validation for aggregate/dataset compatibility, allowing invalid rules that fail during later processing.","Apdex query with threshold parameter used on metrics dataset, which only supports tag-based apdex.","Metrics dataset apdex query with threshold was persisted, causing failure during subscription deletion.","API allows apdex(threshold) on metrics dataset, but backend rejects it, causing query failure.","Alert rule creation allows `apdex(threshold)` for metrics dataset, which is incompatible with its tag-based apdex implementation.","Apdex query with threshold failed because metrics dataset's apdex is tag-based, not threshold-based.","API allows apdex with threshold on metrics dataset, but backend's metrics dataset implementation rejects it.","Apdex queries with satisfaction thresholds are incompatible with the metrics dataset, causing subscription creation to fail.","Apdex query with threshold parameter fails on metrics dataset due to incompatible function implementation.","Query creation lacks aggregate-to-dataset compatibility validation, allowing invalid queries to persist and fail during later processing.","A `QuerySubscription` with `apdex(300)` on the metrics dataset cannot be processed, as metrics apdex is tag-based, not threshold-based.","Apdex query with threshold parameter routed to metrics dataset, which only supports tag-based apdex, causing incompatibility."],"transactions":["sentry.snuba.tasks in delete_subscription_from_snuba","sentry.search.events.datasets.metrics in _resolve_apdex_function","sentry.snuba.tasks.delete_subscription_from_snuba","sentry.snuba.tasks.update_subscription_in_snuba","sentry.snuba.tasks in _create_in_snuba"],"title":"Apdex queries with threshold rejected by metrics","description":"Alert subscription tasks building Snuba metrics queries fail when apdex is requested with a threshold parameter, which the metrics dataset does not support. This blocks create/update/delete of subscriptions that reference such queries.","tags":["API","Configuration","Data Integrity","Snuba","Metrics Dataset","Apdex","Invalid Parameter"],"cluster_size":14,"cluster_min_similarity":0.942249981986142,"cluster_avg_similarity":0.9637703859091872},{"project_ids":["1"],"cluster_id":215,"group_ids":[6646105333,6798150081],"issue_titles":["SubscriptionError: The functions provided do not match the requested metric type","IncompatibleMetricsQuery: The functions provided do not match the requested metric type"],"root_cause_summaries":["Metric function 'sum' lacks counter-specific implementation, failing metric type validation for 'c:' prefix.","MetricsFunction for 'sum' lacks 'snql_counter' implementation, failing validation for 'c:spans/ai.total_cost@usd' metric."],"transactions":["sentry.snuba.tasks in delete_subscription_from_snuba","sentry.snuba.tasks.delete_subscription_from_snuba"],"title":"Snuba metrics query uses mismatched function/type","description":"AlertMetricsQueryBuilder rejects a subscription delete query because selected functions are incompatible with the metric type, bubbling up as SubscriptionError during Celery worker execution.","tags":["API","Configuration","Sentry","Snuba","Metrics","SubscriptionError","Incompatible Metrics Query"],"cluster_size":2,"cluster_min_similarity":0.9698025930355452,"cluster_avg_similarity":0.9698025930355452},{"project_ids":["1"],"cluster_id":216,"group_ids":[6646105502,6678537766,6722357667,6722361223,6722458914,6725487273,6732124642,6784137450],"issue_titles":["SubscriptionError: Invalid query. Project(s) application-monitoring-springboot do not exist or are not actively selected.","InvalidSearchQuery: Invalid query. Project(s) application-monitoring-springboot do not exist or are not actively selected."],"root_cause_summaries":["Subscription query's project slug 'application-monitoring-springboot' is outdated; project 6090082's current slug differs or project is inactive, preventing query parsing.","Query string references project not included in subscription's limited project scope, causing resolution failure.","Project slug change invalidated stored query's project filter, causing query builder to fail.","SnubaQuery's project slug validation fails during subscription deletion because the slug is not associated with the subscription's project ID.","QuerySubscription's project context doesn't include the project slug specified in its query string.","Subscription query references project slug not found in organization's active projects, causing query validation failure.","Query builder's SnubaParams lacked Project objects, causing project slug resolution to fail during query validation.","Query subscription's project context limits query builder, causing project filter for external project to fail validation."],"transactions":["sentry.snuba.tasks.delete_subscription_from_snuba","sentry.snuba.tasks in _create_in_snuba","sentry.snuba.tasks in delete_subscription_from_snuba","sentry.search.events.datasets.filter_aliases in project_slug_converter"],"title":"Snuba subscriptions fail on unknown project slug","description":"Subscription create/delete tasks error when building queries that reference project slug 'application-monitoring-springboot', which is not selected or does not exist, leading to InvalidSearchQuery wrapped as SubscriptionError.","tags":["Configuration","API","Sentry","Snuba","Invalid Query","SubscriptionError","Project Slug"],"cluster_size":8,"cluster_min_similarity":0.9346628810251627,"cluster_avg_similarity":0.9564671707643537},{"project_ids":["1"],"cluster_id":217,"group_ids":[6646113007,6646547763,6656493768,6671358819,6683159775,6709202476,6710063818,6711283984,6721964725,6754696654],"issue_titles":["IntegrationError: Identity not found.","Identity.DoesNotExist"],"root_cause_summaries":["GitLab integration fails due to orphaned `default_auth_id` reference after its linked `Identity` record was deleted.","GitLab integration's `default_auth_id` references a non-existent identity, causing `Identity.DoesNotExist` when fetching API client.","OrganizationIntegration's default_auth_id references a non-existent Identity record, preventing GitLab API client instantiation.","OrganizationIntegration's default_auth_id points to a deleted Identity record, causing lookup failure.","OrganizationIntegration's default_auth_id references a non-existent Identity record, likely due to identity reattachment during setup.","VSTS integration failed due to missing Identity record referenced by `OrganizationIntegration.default_auth_id`.","Dangling `default_auth_id` in `OrganizationIntegration` points to a non-existent `Identity` record, causing integration authentication failure.","GitLab integration failed because `org_integration.default_auth_id` referenced a non-existent `Identity` record, causing `identity_service.get_identity` to return `None`.","GitLab integration's required Identity object was deleted, causing `default_identity` lookup to fail and `IntegrationError` during commit context processing.","OrganizationIntegration references non-existent Identity record, causing authentication failure during GitLab API client initialization."],"transactions":["sentry.integrations.gitlab.integration in get_client","sentry.tasks.process_commit_context","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/stacktrace-coverage/","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","/api/0/internal/integration-proxy/"],"title":"Integration client fails due to missing identity","description":"Calls to fetch an integration client raise 'Identity not found' because the installation has no default identity, breaking PR comment workflows and stacktrace linking. Ensure the integration installation has a valid identity bound before client access.","tags":["Configuration","Authentication","API","Integration","Identity Missing","Celery","Stacktrace Linking"],"cluster_size":10,"cluster_min_similarity":0.9378421477905549,"cluster_avg_similarity":0.9654934268212088},{"project_ids":["1"],"cluster_id":218,"group_ids":[6646129641,6646749903,6717908878,6732321603,6741374843],"issue_titles":["ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"ef75c5bc1f284bc6be061ea8acf47ade\"}","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"55ab76915fdf468e9fd4dad7c4525728\"}","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"8ba7375f9b324d1b8c5f8592faf60aaa\"}","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"293dba9396284b2787d69899e7edfec7\"}","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"f595df3afa854f5ebae1407980073b42\"}"],"root_cause_summaries":["GitHub organization's IP allowlist blocks Sentry's token refresh, causing API calls to fail and the PR comment workflow to error.","GitHub organization's IP allowlist blocked Sentry's access, preventing token refresh and API calls.","GitHub organization's IP allowlist blocked Sentry's token refresh, causing an internal proxy error masking the access denial.","GitHub organization's IP allow list blocked Sentry's outbound request for pull request files.","GitHub organization's IP allow list blocks Sentry's token refresh, causing API access failure."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow"],"title":"PR file fetch fails with upstream 500 errors","description":"The open_pr_comment_workflow task fails when the Git providers /repos/{repo}/pulls/{pull_number}/files endpoint returns 500, leading our client to raise ApiError from HTTPError. This blocks determining safe-for-comment files during PR processing.","tags":["External System","API","Upstream Unavailable","HTTP 5xx","Pull Request Files Endpoint","Celery"],"cluster_size":5,"cluster_min_similarity":0.952900074719459,"cluster_avg_similarity":0.9661003471960443},{"project_ids":["1"],"cluster_id":219,"group_ids":[6646150522,6646294627],"issue_titles":["TypeError: Task sentry.sentry_apps.tasks.sentry_apps.workflow_notification was called with a parameter that cannot be JSON encoded (datetime.datetime(2025, 6, 16, 15, 0, 32, 86778, tzinfo=datetime.timezone.utc), reason is do not pickle stdlib classes) in argument 1","TypeError: Task sentry.sentry_apps.tasks.sentry_apps.workflow_notification was called with a parameter that cannot be JSON encoded (datetime.datetime(2025, 5, 23, 4, 56, 8, 796741, tzinfo=datetime.timezone.utc), reason is do not pickle stdlib classes) in argument 1"],"root_cause_summaries":["Task arguments mutated with non-JSON-serializable datetime objects, causing retry re-serialization to fail.","GroupSerializer includes raw `datetime` objects in task payload, causing Celery's JSON serialization validation to fail."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.workflow_notification"],"title":"Sentry app webhook notifications time out and 502","description":"Celery task sentry_apps.workflow_notification fails when sending webhooks: outbound HTTP requests either time out or receive 502 Bad Gateway responses. Retries also surface a Celery arg serialization error due to a non-JSON-serializable date being passed to the task.","tags":["API","Queueing","Serialization","External System","Celery","Webhook","Timeout","Upstream Unavailable","JSON Serialization Error"],"cluster_size":2,"cluster_min_similarity":0.9546414573265288,"cluster_avg_similarity":0.9546414573265288},{"project_ids":["1"],"cluster_id":221,"group_ids":[6646222445,6711114721,6731877244],"issue_titles":["IntegrityError: update or delete on table \"sentry_groupedmessage\" violates foreign key constraint \"group_id_refs_id_9603f6ba\" on table \"sentry_grouphash\"","IntegrityError: update or delete on table \"sentry_incident\" violates foreign key constraint \"incident_id_refs_id_74273623\" on table \"sentry_incidentseen\""],"root_cause_summaries":["IncidentDeletionTask lacks IncidentSeen child relation, causing foreign key violation during incident deletion.","Database retains `sentry_incidentseen` with foreign key, but `IncidentDeletionTask` lacks its cleanup, causing deletion failure.","Incomplete `sentry_grouphash` deletion leaves orphaned references, violating foreign key constraint during `sentry_groupedmessage` deletion."],"transactions":["sentry.deletions.tasks.run_deletion","sentry.deletions.tasks.groups.delete_groups"],"title":"Cascade deletes fail due to FK constraints in Sentry","description":"Deletion tasks for incidents and groups attempt to remove parent rows while child records still exist (IncidentSeen, GroupHash), causing PostgreSQL foreign key violations. This blocks scheduled cleanup jobs in Celery until dependent rows are deleted or cascades are configured.","tags":["Database","Data Integrity","API","PostgreSQL","Foreign Key Violation","Sentry Deletion Task","Cascade Delete Missing"],"cluster_size":3,"cluster_min_similarity":0.943640875619103,"cluster_avg_similarity":0.9573192832513668},{"project_ids":["1"],"cluster_id":222,"group_ids":[6646222513,6711913108,6712840292],"issue_titles":["ProtectedError: (\"Cannot delete some instances of model 'SnubaQuery' because they are referenced through protected foreign keys: 'AlertRule.snuba_query'.\", {<Incident at 0x7ec89ed276c0: id=11272194>, <Incident at 0x7ec89f3a3040: id=15728644>, <Incident at 0x7ec89f38c7a...","ProtectedError: (\"Cannot delete some instances of model 'SnubaQuery' because they are referenced through protected foreign keys: 'AlertRule.snuba_query'.\", {<Incident at 0x7a0f283517e0: id=21823489>, <Incident at 0x7a0f2833f380: id=22085634>, <Incident at 0x7a0f282d571...","ProtectedError: (\"Cannot delete some instances of model 'SnubaQuery' because they are referenced through protected foreign keys: 'AlertRule.snuba_query'.\", {<Incident at 0x7f7758931d90: id=11272194>, <Incident at 0x7f7758f85710: id=15728644>, <Incident at 0x7f7758f62dd..."],"root_cause_summaries":["Incident.alert_rule's PROTECT constraint and IncidentDeletionTask's lack of AlertRule/SnubaQuery cascade prevent SnubaQuery deletion.","AlertRuleDeletionTask fails to delete its associated SnubaQuery, causing protected foreign key violations during other deletions.","QuerySubscription deletion fails because AlertRule.objects manager excludes snapshot AlertRules, leading to a ProtectedError when deleting referenced SnubaQuery."],"transactions":["sentry.deletions.tasks.run_deletion"],"title":"SnubaQuery deletions blocked by AlertRule FK protection","description":"Deletion tasks for SnubaQuery objects fail because they are still referenced by AlertRule.snuba_query, triggering Django ProtectedError during scheduled cleanup. Incidents referencing these queries prevent cascading deletes.","tags":["Database","Data Integrity","Configuration","Django ORM","Foreign Key Constraint","ProtectedError","Alerting"],"cluster_size":3,"cluster_min_similarity":0.9549771138286898,"cluster_avg_similarity":0.9624732288012817},{"project_ids":["1"],"cluster_id":227,"group_ids":[6646327766,6731709120],"issue_titles":["ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"],"root_cause_summaries":["API call to incidents endpoint failed due to invalid `project: -1` parameter, causing a 404.","Incident chart API request failed due to invalid `project: -1` parameter not supported by endpoint."],"transactions":["sentry.incidents.tasks.handle_trigger_action"],"title":"Metric alert chart fetch fails with API error","description":"Building metric alert emails and notifications fails when charts.py fetch_metric_issue_open_periods calls the metrics API via client.get, raising ApiError. This blocks both fire and resolve alert notifications from completing.","tags":["API","Notifications","Alerting","Client Error","Metric Charts","Celery"],"cluster_size":2,"cluster_min_similarity":0.9619755170848878,"cluster_avg_similarity":0.9619755170848878},{"project_ids":["1"],"cluster_id":228,"group_ids":[6646412253,6646412368,6682729283,6685734907,6689750620,6690982665,6690982707,6698156454,6726920646,6733893043,6738892476,6738892482,6770916801],"issue_titles":["IntegrationError: Error Communicating with GitLab (HTTP 502): unknown error","IntegrationError: Error Communicating with GitHub (HTTP 500): unknown error","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"ab302e1f048141f0ad4875a60cbd8e66\"}","ApiError: <!DOCTYPE html>","ApiError: {\"message\":\"403 Forbidden\"}","ApiError: GitLab is not responding","ApiError: <html>","IntegrationError: Error Communicating with GitLab (HTTP 500): unknown error"],"root_cause_summaries":["GitLab API intermittently returned 500 Internal Server Error for commit diff requests, causing Sentry's integration proxy to propagate the failure.","Cloudflare returned 502 Bad Gateway to Sentry's internal proxy when fetching a GitLab commit diff, causing task failure.","IntegrationProxyClient's retry logic excludes 502 errors, causing task failures on transient proxy issues.","GitLab API returned 500 for merge request query, causing Sentry's `ApiError` and task retry exhaustion.","GitLab commit diff retrieval failed due to `IncompleteRead` from network connection drop between Sentry's proxy and GitLab API.","GitLab API returned incomplete chunked response, causing proxy to error 500, leading to Sentry's ApiError.","Large GitLab commit diff exceeded Cloudflare/proxy limits, causing a 502 Bad Gateway error during fetch_commits task.","GitLab server returned HTTP 502 to Sentry's Control silo during commit diff fetch, causing error propagation.","Sentry's control silo integration proxy received a 502 Bad Gateway from external GitLab, preventing commit context retrieval.","GitHub API returned HTTP 500 for commit details request, causing Sentry's integration proxy to propagate the error.","GitLab API returned 502 Bad Gateway for a commit diff request, causing Sentry's integration proxy to fail.","GitLab API returned 502 to Sentry's internal proxy during commit diff fetch, causing `IntegrationError`.","Cloudflare returned 502 Bad Gateway to Sentry's GitLab API request, causing an IntegrationError."],"transactions":["sentry.tasks.process_commit_context","sentry.tasks.commits.fetch_commits"],"title":"GitLab/GitHub commit fetch fails with 5xx responses","description":"Background tasks fetching commit diffs from GitLab and GitHub are failing due to upstream 502/500 errors, leading to IntegrationError and ApiError during compare_commits and get_diff/get_commit calls.","tags":["External System","API","Networking","Upstream Unavailable","HTTP 5xx","GitLab","GitHub"],"cluster_size":13,"cluster_min_similarity":0.9141915292297943,"cluster_avg_similarity":0.9461632887199725},{"project_ids":["1"],"cluster_id":230,"group_ids":[6646446447,6725749886],"issue_titles":["InvalidQueryError: query must have at least one expression in select"],"root_cause_summaries":["Query builder's `resolve_function` fails to convert `DiscoverFunction` aggregates to Snuba SDK expressions, resulting in an empty `select` clause.","`count_unique` defined as `DiscoverFunction` prevents `snql_aggregate` population, yielding empty Snuba `select` clause."],"transactions":["sentry.tasks.on_demand_metrics.process_widget_specs"],"title":"SNQL query missing SELECT expressions in metrics check","description":"On-demand metrics widget cardinality checks are issuing SNQL requests without any select expressions, causing InvalidQueryError during query serialization via Snuba.","tags":["API","Input Validation","Snuba","SNQL","InvalidQueryError"],"cluster_size":2,"cluster_min_similarity":0.9775367115532965,"cluster_avg_similarity":0.9775367115532965},{"project_ids":["1"],"cluster_id":232,"group_ids":[6646502916,6722426516,6722438096],"issue_titles":["IntegrationFormError: {'parent': ['Could not find issue by id or key.']}"],"root_cause_summaries":["Jira API rejected issue creation due to non-existent or inaccessible parent issue key `TECHH-98`.","Jira API rejected issue creation due to a non-existent or inaccessible parent issue key 'EDU-126' in the request payload.","Jira alert rule's configured parent issue 'EDU-126' is inaccessible, causing API request failure."],"transactions":["sentry.shared_integrations.client.base in _request","sentry.tasks.post_process.post_process_group"],"title":"Jira issue creation fails: invalid parent key","description":"Post-processing rules fail when creating Jira issues because the provided parent issue ID/key is not found, resulting in a 400 Bad Request from the Jira API and IntegrationFormError. This blocks automated issue creation in the integration pipeline.","tags":["External System","API","Configuration","Jira","Bad Request","Invalid Reference"],"cluster_size":3,"cluster_min_similarity":0.9625878506893502,"cluster_avg_similarity":0.9713805168777369},{"project_ids":["1"],"cluster_id":234,"group_ids":[6646659663,6775500824,6807639014,6807644923],"issue_titles":["ApiError: {\"message\":\"This installation has been suspended\",\"documentation_url\":\"https://docs.github.com/rest/reference/apps#create-an-installation-access-token-for-an-app\",\"status\":\"403\"}","ApiError: {\"message\":\"Although you appear to have the correct authorization credentials, the `SallaApp` organization has an IP allow list enabled, and your IP address is not permitted to access this resource.\",\"documentation_url\":\"https://docs.github.com/rest/app...","ApiError: {\"message\":\"Although you appear to have the correct authorization credentials, the `mailplug-inc` organization has an IP allow l"],"root_cause_summaries":["GitHub organization's IP allow list blocked Sentry's API request, as Sentry's IP was not whitelisted.","GitHub organization's IP allow list blocks Sentry's IP, preventing token refresh despite valid credentials.","GitHub organization's IP allow list blocks Sentry's API request, despite valid credentials, causing a 403 Forbidden error.","GitHub organization's IP allow list blocked Sentry's API request, despite valid credentials."],"transactions":["/extensions/{provider_id}/setup/","/api/0/internal/integration-proxy/","getsentry.tasks.verify_github_subscriptions"],"title":"GitHub API 403s due to org IP allowlist and suspended app","description":"Requests to GitHub installations and marketplace endpoints are failing with 403 Forbidden when organizations have IP allow lists or the installation is suspended, breaking integration setup and subscription verification flows.","tags":["External System","API","Authentication","Upstream Unavailable","GitHub","Forbidden 403","IP Allowlist"],"cluster_size":4,"cluster_min_similarity":0.9518726419781584,"cluster_avg_similarity":0.9688294311426752},{"project_ids":["1"],"cluster_id":235,"group_ids":[6646707867,6799328440],"issue_titles":["InvalidSearchQuery: invalid status value of '404'","SubscriptionError: invalid status value of '404'"],"root_cause_summaries":["Invalid 'status:404' query, intended for HTTP status, was stored in subscription, causing parsing failure during deletion.","Invalid 'status:404' query stored in SnubaQuery due to missing input validation, causing `InvalidSearchQuery` during later processing."],"transactions":["sentry.snuba.tasks.delete_subscription_from_snuba","sentry.snuba.tasks in delete_subscription_from_snuba"],"title":"Invalid status value in issue search filters","description":"Subscription deletion tasks fail when parsing the querys status filter, which provides an integer-like placeholder ('<int>') that the issue search parser rejects. This propagates as InvalidSearchQuery and SubscriptionError during Snuba entity query building.","tags":["API","Input Validation","Query Parsing","Snuba","InvalidSearchQuery","SubscriptionError"],"cluster_size":2,"cluster_min_similarity":0.9540419364267042,"cluster_avg_similarity":0.9540419364267042},{"project_ids":["1"],"cluster_id":236,"group_ids":[6646804514,6725550710],"issue_titles":["IncompatibleMetricsQuery: Metric: c:custom/business_successfully_fetched@none could not be resolved"],"root_cause_summaries":["Metric resolution during subscription deletion fails because the custom metric is not found within the default 90-day lookup window.","Subscription deletion fails because its custom metric is unresolvable, indicating the metric no longer exists."],"transactions":["sentry.search.events.datasets.metrics in resolve_metric","sentry.snuba.tasks.delete_subscription_from_snuba"],"title":"Alert metrics query fails to resolve metric key","description":"Deletion of a Snuba alert subscription triggers AlertMetricsQueryBuilder to resolve a metric like c:<email>, which is not recognized, causing IncompatibleMetricsQuery. This impacts alert subscription cleanup tasks that parse metrics-based queries.","tags":["API","Configuration","Sentry Alerts","Snuba","Incompatible Metrics","Query Parsing"],"cluster_size":2,"cluster_min_similarity":0.9775700864277566,"cluster_avg_similarity":0.9775700864277566},{"project_ids":["1"],"cluster_id":237,"group_ids":[6646883597,6674948935,6685373553,6710200116,6710677053,6726430752,6736000592,6759843271,6774804635,6780329252,6781173676,6793150825,6796518522,6802254709],"issue_titles":["Group.DoesNotExist: Group matching query does not exist.","Subscription.DoesNotExist: Subscription matching query does not exist."],"root_cause_summaries":["Async task `process_suspect_commits` fails because its target `Group` is deleted by a concurrent operation before task execution.","Group deletion raced event post-processing, causing `Group.DoesNotExist` when task tried to retrieve non-existent group.","Group deletion raced post-processing task, causing `Group.DoesNotExist` when task attempted to retrieve non-existent group.","Concurrent group deletion by merge task causes post-processing to fail due to missing group.","Digest processing attempts to retrieve a group that was deleted by a concurrent background task.","Asynchronous task failed to retrieve group from database; group was deleted between task enqueueing and execution.","Race condition: Group deleted by merge task before digest notification delivery, causing `Group.DoesNotExist`.","Asynchronous group deletion by merge task creates race condition, causing post-processing task to fail fetching non-existent group.","Asynchronous task failed to retrieve a Group object because it was deleted from the database after the task was queued.","Group deletion race condition: concurrent tasks attempt to retrieve already-deleted groups, causing `DoesNotExist` errors due to non-idempotent task initiation.","Asynchronous task fails to retrieve Group object from database because it was deleted after task enqueued.","Asynchronous task failed to find subscription for organization deleted concurrently, due to missing defensive checks for stale data.","Asynchronous task fails to retrieve Group due to deletion occurring between task queuing and execution.","Task processed deleted Group ID, creating orphaned GroupOwner, then failed accessing non-existent Group due to `db_constraint=False`."],"transactions":["getsentry.tasks.stats.sync_outdated","sentry.models.group in get_group_with_redirect","sentry.deletions.tasks.groups.delete_groups","sentry.tasks.commit_context in process_commit_context","sentry.tasks.activity.send_activity_notifications","sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2","sentry.tasks.process_suspect_commits","sentry.tasks.digests.deliver_digest","sentry.issues.tasks.post_process.post_process_group"],"title":"Missing Group/Subscription records break Celery post-processing","description":"Background workers fail when fetching Group or Subscription by ID from cache/DB, causing post-processing, ownership inference, notifications, and stats sync to error. Likely stale references or deleted objects not handled defensively.","tags":["Data Integrity","Caching","API","Django ORM","DoesNotExist","Celery","Sentry"],"cluster_size":14,"cluster_min_similarity":0.9286269203771114,"cluster_avg_similarity":0.9566828668265244},{"project_ids":["1"],"cluster_id":239,"group_ids":[6647198005,6684498668,6701109795,6753301787,6755796666,6762473610,6792401517,6795155168],"issue_titles":["Project.DoesNotExist: Project matching query does not exist.","Organization.DoesNotExist: Organization matching query does not exist.","Subscription.DoesNotExist: Subscription matching query does not exist."],"root_cause_summaries":["Deleted organization IDs persist in Redis uptime buckets, causing `DoesNotExist` errors during processing.","Redis usage buffer contains stale project IDs for deleted subscriptions, causing `DoesNotExist` errors during processing.","Project deletion creates stale Redis references, causing uptime detection tasks to fail when fetching non-existent projects.","Project deletion does not clear corresponding Redis uptime data, causing `Project.DoesNotExist` when tasks retrieve stale IDs.","Stale project ID in Redis caused `Project.DoesNotExist` when task attempted to retrieve a deleted project for cleanup.","Stale project IDs in Redis, due to task timeouts preventing cleanup, caused `Project.DoesNotExist` when fetching deleted projects.","Stale organization IDs in Redis buckets, due to missing cleanup after database deletion, cause `DoesNotExist` errors.","Taskworker attempts to retrieve a deleted project from database due to stale Redis cache lacking invalidation upon project deletion."],"transactions":["sentry.uptime.detectors.tasks.process_organization_url_ranking","getsentry.billing.tasks.usagebuffer in flush_usage_buffer","sentry.uptime.detectors.tasks in process_organization_url_ranking"],"title":"Cache lookup returns missing ORM objects in Celery task","description":"Celery worker tasks resolving Organization, Project, and Subscription via get_from_cache are raising DoesNotExist, indicating stale or invalid cache entries or deleted records referenced by tasks. This impacts process_organization_url_ranking and usage buffer flushing when IDs no longer exist.","tags":["Caching","Queueing","Data Integrity","Celery","Django ORM","DoesNotExist","Stale Cache"],"cluster_size":8,"cluster_min_similarity":0.9418195640453648,"cluster_avg_similarity":0.9611711562601633},{"project_ids":["1"],"cluster_id":240,"group_ids":[6647516613,6755036291],"issue_titles":["ApiError: {\"error\":{\"code\":\"TenantNoPermission\",\"message\":\"Tenant access is denied.\"}}"],"root_cause_summaries":["Microsoft Teams API denied conversation creation due to insufficient Azure AD application permissions for the tenant.","Microsoft Teams API denied conversation creation due to insufficient permissions for the integration's access token within the specified tenant."],"transactions":["sentry.tasks.digests.deliver_digest","sentry.tasks.digests.schedule_digests"],"title":"Microsoft Teams API denies tenant access during digests","description":"Digest notifications to Microsoft Teams fail with TenantNoPermission, returning HTTP 403 when fetching user conversation IDs. Likely misconfigured tenant or insufficient app permissions on the Teams integration.","tags":["External System","Authorization","API","Microsoft Teams","HTTP 403","TenantNoPermission"],"cluster_size":2,"cluster_min_similarity":0.9736979428602571,"cluster_avg_similarity":0.9736979428602571},{"project_ids":["11276"],"cluster_id":241,"group_ids":[6647539092,6653572758],"issue_titles":["Error: Failed to create sample event"],"root_cause_summaries":["User lacks `event:write` or `event:admin` permissions required by `ProjectCreateSampleTransactionEndpoint` for sample event creation.","User's project permissions lacked `event:write` scope, causing a 403 on sample event creation."],"transactions":["/insights/backend/","/onboarding/:step/"],"title":"Sentry sample event creation failing in UI","description":"Clicks on onboarding and sample-event buttons trigger Sentry.captureException('Failed to create sample event'), indicating the UI cannot generate a sample event via Sentry scope handling.","tags":["API","Configuration","Sentry","Client-Side","CaptureException"],"cluster_size":2,"cluster_min_similarity":0.9744094820249003,"cluster_avg_similarity":0.9744094820249003},{"project_ids":["1"],"cluster_id":244,"group_ids":[6647843695,6789207236,6789207830,6789207842,6789207932,6789207934,6789207967,6789207977,6789207979,6789208000,6789208002,6789208024,6789208075,6789208133],"issue_titles":["ReadTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Read timed out. (read timeout=6)","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4504718419886080 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x785ee98a3890>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505803031707648 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x78b28137f1d0>, 'Connection to symbolicat...","ConnectionError: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4506189799358464 (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7c7e78124050>: Failed to resolve ...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1244074 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x78b88b763950>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4504962044526592 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x785ff22eb530>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4505046108733440 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7ee287233950>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4504413835034624 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7d05dc7b31d0>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504731502247936 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7cd6082d9c70>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505803031707648 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a86b81fec30>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1236774 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x794d99771eb0>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1234324 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a01344048a0>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4507582597693440 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7901584ee9f0>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=4504074185408512 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7c361819d490>, 'Connection to sy..."],"root_cause_summaries":["Malformed `symbolicator-js.sentry.` URL in configuration prevents connection, exhausting retries and failing profile symbolication tasks.","Symbolicator service URL misconfiguration prevents connection; hostname is malformed, port is incorrect, leading to network timeouts.","Symbolicator hostname `symbolicator-js.sentry.` with trailing dot prevents DNS resolution, causing connection failures.","Symbolicator connection fails due to malformed hostname with trailing dot in configuration.","Symbolicator hostname `symbolicator-js.sentry.` (with trailing dot) misconfiguration causes connection timeouts, preventing JavaScript symbolication.","Symbolicator service unreachable due to misconfigured hostname (trailing dot) and incorrect port 80, exhausting retries.","Symbolicator hostname `symbolicator-js.sentry.` is malformed, causing DNS resolution and connection timeouts.","Symbolicator hostname misconfigured with trailing dot, causing DNS resolution and connection failures.","Symbolicator hostname `symbolicator.sentry.` is misconfigured, causing connection timeouts to an unresponsive IP address after DNS resolution.","Symbolicator service unreachable due to DNS resolution failure of `symbolicator-js.sentry.` hostname.","Symbolicator connection failed due to malformed hostname `symbolicator-jvm.sentry.` preventing DNS resolution.","Symbolicator hostname configured with trailing dot, causing DNS resolution failures and connection timeouts in internal network.","Malformed `symbolicator.sentry.` hostname prevents HTTP connection, causing repeated timeouts and MaxRetryError.","Symbolicator service URL misconfiguration in deployment settings caused connection attempts to incorrect port 80, leading to timeouts."],"transactions":["sentry.tasks.symbolicate_jvm_event","sentry.profiles.task.process_profile","sentry.tasks.symbolicate_js_event","sentry.tasks.store.symbolicate_event"],"title":"Symbolicator DNS and connect timeouts from Sentry workers","description":"Sentry processing and profiling tasks fail to contact the Symbolicator services (symbolicator-js.sentry, symbolicator.sentry) due to DNS resolution failures and connection/read timeouts, causing symbolication to stall. This impacts JS and native stacktrace processing in worker tasks.","tags":["Networking","External System","API","DNS Resolution Failure","Connect Timeout","Read Timeout","Sentry Symbolicator"],"cluster_size":14,"cluster_min_similarity":0.9344188323088524,"cluster_avg_similarity":0.9666499337889529},{"project_ids":["11276"],"cluster_id":246,"group_ids":[6648125338,6782267858],"issue_titles":["Error: useOrganization called but organization is not set."],"root_cause_summaries":["User lacks access to URL's organization; API 401 causes null organization state, crashing `useOrganization` hook.","Authentication failure prevents organization context population, causing `useOrganization` to error when accessed by the Onboarding component."],"transactions":["/onboarding/:step/"],"title":"useOrganization called outside OrganizationContext","description":"React components like Onboarding invoke useOrganization before OrganizationContextProvider is set, causing ErrorBoundary crashes during routing and lazy load. Likely a missing or misordered provider in the app/router composition for organization-scoped routes.","tags":["Configuration","API","React","Context Provider Missing","Hook Misuse"],"cluster_size":2,"cluster_min_similarity":0.9709950377269544,"cluster_avg_similarity":0.9709950377269544},{"project_ids":["1"],"cluster_id":247,"group_ids":[6648172613,6649897773],"issue_titles":["AvataxException: GetTaxError: 'Invalid or missing state/province code (TEL).'","AvataxException: GetTaxError: 'Address not geocoded.'"],"root_cause_summaries":["Avatax transaction fails for non-US customers because `region` is `None` when using credit card location.","Avatax API rejects tax calculation due to missing city/region in non-US addresses derived from credit card data."],"transactions":["getsentry.tasks.create_invoices.process_subscription"],"title":"Avalara tax failures during invoice creation","description":"AvaTax transaction creation fails in the billing worker when customer addresses are invalide.g., bad state/province code (TEL) or ungeocodable addressesblocking sales tax computation during subscription invoicing.","tags":["External System","API","Input Validation","Avalara AvaTax","Invalid Address","Geocoding Failure"],"cluster_size":2,"cluster_min_similarity":0.9620188034066681,"cluster_avg_similarity":0.9620188034066681},{"project_ids":["1"],"cluster_id":248,"group_ids":[6648177709,6656160648,6657784991,6692117118,6732866491,6794581046],"issue_titles":["OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 2667645 waits for ShareLock on transaction 4236218338; blocked by process 2652944.\\nProcess 2652944 waits for ShareLock on transaction 4236218329; blocked by process 2667645.\\nHINT:  See server log f...","OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 2653428 waits for ShareLock on transaction 3322502587; blocked by process 2672438.\\nProcess 2672438 waits for ShareLock on transaction 3322502687; blocked by process 2653428.\\nHINT:  See server log f...","OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 859239 waits for ShareLock on transaction 4067465074; blocked by process 878204.\\nProcess 878204 waits for ShareLock on transaction 4067464979; blocked by process 859239.\\nHINT:  See server log for q...","OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 1478992 waits for ShareLock on transaction 1304801583; blocked by process 1479002.\\nProcess 1479002 waits for ShareLock on transaction 1304802082; blocked by process 1478978.\\nProcess 1478978 waits f...","OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 1519371 waits for ShareLock on transaction 810499632; blocked by process 1519369.\\nProcess 1519369 waits for ShareLock on transaction 810499612; blocked by process 1519371.\\nHINT:  See server log for...","OperationalError: deadlock detected"],"root_cause_summaries":["Concurrent processes updating `accounts_billingmetrichistory` acquire row locks in conflicting orders due to inconsistent, non-mutually exclusive coarse-grained and granular locking strategies.","Concurrent `ServiceHook` updates without `select_for_update()` cause deadlocks due to shared-to-exclusive lock contention.","Concurrent `flush_usage_buffer` tasks for same subscription cause deadlock updating shared `accounts_billinghistory` record.","Concurrent monitor check-in processing for same organization causes deadlock due to inconsistent lock acquisition order on shared billing metric history records.","Concurrent billing transactions acquire database locks in inconsistent orders, causing circular wait conditions and deadlocks.","Concurrent `tally_usage` calls for different categories on same subscription deadlock, as Redis lock is too granular, allowing conflicting database row updates."],"transactions":["/api/0/customers/{organization_id_or_slug}/ondemand-budgets/","monitors.monitor_consumer","/api/0/internal/rpc/{service_name}/{method_name}/","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","getsentry.tasks.create_invoices.process_subscription"],"title":"PostgreSQL deadlocks updating billing history","description":"Concurrent workers update accounts_billingmetrichistory and accounts_billinghistory rows, causing PostgreSQL to detect deadlocks during UPDATEs. Impact includes failed invoice processing, usage tally, and monitor seat assignment until retries succeed.","tags":["Database","Concurrency","PostgreSQL","Deadlock","UPDATE Statement","BillingMetricHistory","BillingHistory"],"cluster_size":6,"cluster_min_similarity":0.9486154209700135,"cluster_avg_similarity":0.9629413184714612},{"project_ids":["1"],"cluster_id":249,"group_ids":[6648254173,6674255220,6725863964,6792840963,6809502897],"issue_titles":["ValueError: Integration with id 14666 not found","ValueError: Integration with id 139287 not found","ValueError: Integration with id 112536 not found","ValueError: Integration with id 125880 not found","ValueError: Integration with id 109571 not found"],"root_cause_summaries":["Race condition: `SlackSdkClient` re-fetches integration by ID, but it's concurrently removed/deactivated, causing `ValueError`.","Billing enforcement disables Slack integration, then attempts to send notification using the now-disabled integration, causing a ValueError.","Slack integration deactivation between notification action retrieval and client initialization causes lookup failure.","Race condition: Integration disabled before its asynchronous notification task could retrieve it as active.","System attempts to notify Slack integration disabled, but fails because integration is already disabled."],"transactions":["sentry.integrations.slack.sdk_client in __init__","sentry.integrations.slack.tasks.post_message","getsentry.tasks.quotas.deactivate_db_spike"],"title":"Slack client init fails: integration ID not found","description":"Celery tasks attempting to send Slack notifications instantiate SlackSdkClient with a missing or invalid integration ID, causing ValueError and preventing alerts from being delivered.","tags":["External System","Configuration","API","Slack","Integration Not Found","Celery"],"cluster_size":5,"cluster_min_similarity":0.9344743546310746,"cluster_avg_similarity":0.9492980677308921},{"project_ids":["11276"],"cluster_id":251,"group_ids":[6648647389,6694491225,6694570420,6731624609,6744547839,6804082298],"issue_titles":["Error: The data does not contain any plottable values."],"root_cause_summaries":["No cache span data exists for `span.op:[cache.get_item,cache.get]`, causing empty API response and widget error.","Backend returned no data, causing all chart plottables to be empty, triggering a client-side error.","Missing destination parameter leads to unfiltered data query, returning all null values, causing chart to error due to no plottable data.","API returned no log data for the requested period, causing all plottables to be empty, triggering the error.","Backend returned zero-count time series data, causing all plottables to be empty, triggering the visualization error.","API returns no data for HTTP domain, causing chart widgets to receive empty series, leading to \"no plottable values\" error."],"transactions":["/insights/backend/queues/destination/","/insights/backend/caches/","/explore/traces/compare/","/insights/backend/http/domains/","/explore/traces/"],"title":"Timeseries widgets throw on empty plottables","description":"React ErrorBoundaries trigger when TimeSeriesWidgetVisualization receives no plottable data, causing charts on Traces, Insights, Queues, and Logs pages to error instead of handling empty results gracefully.","tags":["UI","Input Validation","React","Time Series","Empty Dataset","Error Boundary"],"cluster_size":6,"cluster_min_similarity":0.935937539501609,"cluster_avg_similarity":0.96031444900539},{"project_ids":["1"],"cluster_id":252,"group_ids":[6648653555,6733406946,6804021277],"issue_titles":["IntegrityError: insert or update on table \"sentry_grouphash\" violates foreign key constraint \"project_id_refs_id_6f0a9434\"","IntegrityError: insert or update on table \"sentry_releaseprojectenvironment\" violates foreign key constraint \"project_id_refs_id_dc770857\"","IntegrityError: insert or update on table \"sentry_environmentproject\" violates foreign key constraint \"project_id_refs_id_cf2e01df\""],"root_cause_summaries":["Race condition: Project deleted after being cached but before GroupHash creation, causing foreign key violation on insert.","Project deletion during asynchronous event processing causes foreign key violation when creating environment-project link.","Race condition: Project deleted after cache retrieval but before foreign key creation, causing IntegrityError."],"transactions":["sentry.tasks.store.save_event","sentry.models.releaseprojectenvironment in _get_or_create_impl","ingest_consumer.process_event"],"title":"Foreign key violations referencing missing Sentry projects","description":"Multiple write paths attempt to create rows (GroupHash, ReleaseProjectEnvironment, EnvironmentProject) with project_id values that do not exist in sentry_project, causing IntegrityError and subsequent DoesNotExist errors. This suggests orphaned references or out-of-order processing where project creation is not completed before dependent records are written.","tags":["Database","Data Integrity","Configuration","PostgreSQL","Foreign Key Violation","Sentry","Orphaned References"],"cluster_size":3,"cluster_min_similarity":0.9712050186322593,"cluster_avg_similarity":0.9733243227329069},{"project_ids":["1"],"cluster_id":253,"group_ids":[6648658103,6653862170,6654175914,6656075359,6665609361,6696855105,6758542608,6791816148,6793528566,6794988959,6794994692,6796732022],"issue_titles":["Project.DoesNotExist: Project matching query does not exist.","DeleteAborted: Failed to delete events from nodestore. We won't retry this task.","MonitorIncident.DoesNotExist: MonitorIncident matching query does not exist.","SentryAppInstallation.DoesNotExist: SentryAppInstallation matching query does not exist."],"root_cause_summaries":["Profile processing failed because its associated project was deleted while the task was queued.","Event processing fails when its project is deleted before task execution.","Kafka message with stale project ID processed after project deletion, causing `Project.DoesNotExist` due to lack of pre-validation.","MonitorIncident deleted/resolved after Kafka message queued, causing consumer's `get` to fail due to race condition.","Project deletion between profile queuing and processing causes `Project.DoesNotExist` due to race condition.","Asynchronous task attempts to retrieve project deleted after scheduling, causing `Project.DoesNotExist` due to race condition.","Project deletion after event ingestion but before database save causes `Project.DoesNotExist` error.","Project deletion race condition: task queued for existing project, project deleted, then task fails fetching non-existent project.","SentryAppInstallation soft-deletion caused subsequent RPC call to fail retrieving non-existent active installation.","Project deletion's synchronous record removal races asynchronous event cleanup's project lookup.","Event processed for a project hard-deleted from the database, causing lookup failure.","Project deletion during asynchronous event processing causes `Project.DoesNotExist` due to stale `project_id` in Redis."],"transactions":["issues.occurrence_consumer","monitors.incident_occurrence_consumer","ingest_consumer.process_event","sentry.deletions.tasks.nodestore in delete_events_for_groups_from_nodestore_and_eventstore","sentry.tasks.store.save_event_transaction","/api/0/internal/rpc/{service_name}/{method_name}/","sentry.tasks.auto_resolve_project_issues","sentry.profiles.task in process_profile_task"],"title":"Missing Project/MonitorIncident records during background processing","description":"Celery and consumer workers attempt to fetch Project and MonitorIncident records that no longer exist, causing DoesNotExist errors during event ingest and incident processing. Likely stale IDs or race conditions between deletion and async processing lead to failed lookups.","tags":["Data Integrity","API","Configuration","Django ORM","DoesNotExist","Celery","Kafka"],"cluster_size":12,"cluster_min_similarity":0.9139618557803838,"cluster_avg_similarity":0.9489770777052761},{"project_ids":["1"],"cluster_id":254,"group_ids":[6648840715,6648840725,6735618000,6753343583,6775190702,6775190736,6775190781,6780305251,6780305274,6780305280,6780305283,6780305285,6780305291,6780305292,6780305293,6780305295,6780305297,6780305298,6780305309,6780305312,6780305325,6780305328,6780305330,6780305333,6780305339,6780305341,6780305357,6780305364,6780305365,6780305366,6780305398,6780305421,6780305426,6780305430,6780305441,6780305442,6780305444,6780305447,6780305462,6780305463,6780305464,6780305467,6780305470,6780305476,6780305477,6780305482,6780305487,6780305498,6780305505,6780305521,6780305523,6780305526,6780305533,6780305543,6780305881,6782453581,6792976939,6794060406,6794060420,6794060430,6794060431,6794060444,6794060456,6794060463,6794060496,6794060499,6794060510,6794060517,6794060519,6794060548,6794060556,6794060558,6794060798,6794063208,6798842618,6798851220,6798876758,6803951788,6803951810,6803951813,6803951831,6803951832,6803951833,6803951839,6803951842,6803951861,6803951868,6803951871,6803951873,6803951881,6803951882],"issue_titles":["TimeoutError: Timeout connecting to server","TimeoutError: Timeout reading from socket"],"root_cause_summaries":["Redis server unresponsiveness during connection attempts, likely due to overload or network issues, causes connection timeouts.","Redis buffer connection attempts time out, indicating network or server unresponsiveness, preventing taskworkers from fetching group data.","Redis connection to `redis-buffer-3` timed out after 3s, likely due to server overload or network issues, blocking event buffering.","Redis server `redis-buffer-1.sentry` is unresponsive/overloaded, causing client connection timeouts during task processing.","Redis connection to uptime autodetection service timed out, indicating network or Redis server performance issues.","Taskworker failed to establish TCP connection to Redis buffer server within 3 seconds, indicating network or server unresponsiveness.","Redis server at 10.12.146.116:6379 is unresponsive/overloaded, causing TCP connection attempts to time out.","Redis buffer instance cannot handle current operation volume within 3-second timeout, causing socket closures.","Redis instance `redis-buffer-1.sentry` is unresponsive, causing socket connection timeouts during rule processing.","Global 5-second socket timeout prematurely closes Redis connections during pipeline execution under load, causing TimeoutError.","Redis buffer instance `redis-buffer-3.de.sentry.internal` connection timeout due to overload from task worker connection attempts.","Redis network instability or resource exhaustion causes socket read timeouts due to default, unconfigured client timeouts.","Redis connection attempts time out, indicating systemic Redis instance overload or network instability, causing task failures.","Redis buffer operations fail due to socket connection timeouts, indicating Redis unavailability or overload, exposing a lack of resilience in event processing.","Redis connection to `redis-buffer-3` timed out after 3 seconds due to server unresponsiveness or network issues.","Redis server instability causes connection closures and subsequent connection attempts to time out.","Redis connection timeouts occur due to server overload or network latency, preventing TCP handshake during concurrent task processing.","Redis connection attempts block indefinitely due to `redis-py`'s default `socket_connect_timeout=None`, causing `TimeoutError`.","Redis connection to `10.123.203.92:6379` timed out during TCP handshake, indicating network or Redis server unavailability.","Redis server at `redis-buffer-3.de.sentry.internal` failed to accept TCP connections, causing taskworker Redis operations to time out.","Redis cluster instability during rebalancing or high load causes connection timeouts and slot redirection errors, disrupting span buffer operations.","Redis server at 10.12.146.116:6379 failed to accept new TCP connections within 3 seconds, causing TimeoutError during task processing.","Redis buffer server unresponsiveness or premature connection closure causes taskworker connection timeouts.","Redis connection exhaustion occurs when multiple post-processing workers concurrently attempt new TCP connections to an overloaded Redis buffer, exceeding its capacity.","Redis server overload or network instability caused connection timeouts during buffer operations, preventing event processing.","Redis server `redis-buffer-3.de.sentry.internal` connection timed out, preventing buffer updates for event processing.","Redis cluster instability and missing `socket_connect_timeout` cause connection timeouts during span data processing.","Redis cluster node unresponsiveness causes connection timeouts during command redirection, disrupting span flushing.","Redis connection timed out due to server overload or network issues, exacerbated by frequent operations and short timeout.","Redis buffer server `redis-buffer-3` timed out connecting, indicating network saturation or server overload preventing new connections.","Redis connection timed out during socket connect, indicating network or Redis server availability issues.","Redis instance `redis-buffer-3.de.sentry.internal` connection timeout due to network instability or overload.","Redis cluster instability and network issues cause connection timeouts during batched cleanup operations, failing span segment flushing.","Redis server is unresponsive or network degraded, causing connection timeouts during pool validation and reconnection attempts.","Redis cluster instability, likely due to high load or misconfiguration, causes connection timeouts and slot migrations, preventing span buffer flushing.","Redis pipeline execution times out due to high volume of N+1 Redis operations from occurrence processing, indicating Redis overload.","Redis connection pool exhaustion, exacerbated by server overload, caused TCP connection timeouts during buffer processing.","Redis server at 10.123.203.92:6379 is unreachable or unresponsive, causing connection timeouts.","Redis server overload prevents new TCP connections, causing socket timeouts during event buffering.","Redis client attempts socket read on already-closed connection, due to underlying library's connection management failure.","Redis buffer server `redis-buffer-3.de.sentry.internal:6379` connection timed out after 3 seconds.","Redis server `redis-buffer-3.de.sentry.internal:6379` is overloaded, failing to accept new TCP connections within the 3-second timeout.","Redis connection attempts to `redis-buffer-3.de.sentry.internal:6379` time out after 3 seconds, due to server unresponsiveness or network issues.","Redis connection timed out due to a 3-second client-side timeout and server unresponsiveness.","Redis server connection timed out due to network or server unresponsiveness, preventing TCP handshake within 3 seconds.","Redis connection attempts timed out due to server overload or network issues, preventing task processing.","Redis buffer server unresponsive to connection requests, causing task worker timeouts.","Redis cluster node `192.168.208.40:11495` is unresponsive, causing socket connection timeouts during span buffer flushing.","Redis cluster instability causes connection timeouts, leading to `flush_usage_buffer` task failures and potential billing data loss.","Taskworker concurrency exceeding Redis connection pool limits causes connection timeouts.","Redis connections prematurely close between pipeline creation and execution, causing `TimeoutError` when reading from closed sockets.","Redis cluster instability/overload causes connection timeouts and MOVED errors during ZSCAN operations.","Redis connection timed out during event buffering due to closed socket, indicating network or Redis server unresponsiveness.","Redis buffer server at `redis-buffer-3.de.sentry.internal:6379` is unreachable or unresponsive, causing connection timeouts during task processing.","Redis cluster slot rebalancing causes client redirection to an unresponsive node, leading to connection timeouts.","Redis server `redis-buffer-3.de.sentry.internal:6379` is unresponsive, causing connection timeouts during Sentry post-processing tasks.","Taskworker failed to connect to Redis buffer due to network timeout, preventing event counter updates.","Redis connection timeouts occur because the Redis server or network is unresponsive to TCP connection requests.","Redis cluster instability and connection exhaustion prevent span data loading, causing processing failures.","Redis cluster instability, likely due to resharding/failover, causes TCP connection timeouts to unresponsive nodes, overwhelming client retries.","Redis server `redis-buffer-3.de.sentry.internal:6379` unresponsive, causing connection timeouts during post-processing.","Redis connection pool reuses a stale socket, causing `TimeoutError` during subsequent I/O operations.","Redis cluster instability causes socket connection timeouts during ZSCAN pipeline execution, preventing span segment retrieval.","Redis cluster node 192.168.208.40:11495 is unreachable, causing connection timeouts after cluster redirections.","Redis server at `redis-buffer-3.de.sentry.internal.:6379` failed to establish TCP connection within 3 seconds.","Redis connection timeouts occur due to infrastructure overload from high event volume and new event generation.","Redis buffer server at 10.123.203.92:6379 is unresponsive, causing TCP connection timeouts during event processing.","Redis connection pool returns stale connections, causing `TimeoutError` during `buffer.incr` operations due to closed sockets.","Redis cluster instability causes connection timeouts, preventing error storage.","Redis buffer server at 10.12.146.116:6379 is unreachable or overloaded, causing TCP connection attempts to time out.","Redis buffer server (10.12.146.116) connection timeouts indicate server overload or network issues preventing timely connection establishment.","Redis server instability causes connection closures and new connection timeouts, disrupting buffer operations.","Network latency or Redis unavailability prevents connection to `redis-buffer-3.de.sentry.internal` within 3 seconds.","Redis buffer server overwhelmed, failing to accept connections within 3-second timeout, halting event processing.","Redis cluster network instability or overload causes socket connection timeouts, preventing data retrieval.","Redis server `redis-buffer-3.de.sentry.internal:6379` is unresponsive or overloaded, causing connection timeouts and closures.","Redis server `redis-buffer-1.sentry` overload prevents new connections within timeout, failing `post_process_group`.","Redis buffer server overloaded/unreachable, causing connection timeouts during high-volume event processing, impacting group statistics updates.","Redis buffer instance overloaded, failing to accept connections within 3-second timeout, causing processing failures.","Redis instance `redis-buffer-3` is unstable, causing connection timeouts and internal errors during occurrence processing.","Redis client's 3-second connection timeout to `redis-buffer-3.de.sentry.internal` is exceeded, preventing lock acquisition.","Redis connection pool reuses stale connections, causing socket read timeouts during pipeline execution.","Redis cluster instability causes persistent connection timeouts, overwhelming client-side retries.","Redis server connection timed out due to network latency or server overload, exceeding the 3-second socket connection timeout.","Redis cluster instability causes `MOVED` errors, leading to TCP connection timeouts when reconnecting to new, unresponsive nodes.","Redis server connection timed out after 3 seconds, indicating an unresponsive or unreachable Redis instance.","Taskworker Redis buffer operations fail due to an overly aggressive 3-second socket timeout, causing connection/read failures.","Redis server failed to accept TCP connections, causing `TimeoutError` during `taskworker`'s `post_process_group` task execution.","Redis buffer node 10.12.146.116:6379 unreachable, causing connection timeouts during event processing.","Redis connection to `redis-buffer-3.de.sentry.internal.:6379` timed out after 3 seconds during issue counter increment.","Redis buffer server `redis-buffer-3.de.sentry.internal` is unresponsive, causing connection timeouts and closures."],"transactions":["sentry.spans.buffer in flush_segments","sentry.buffer.redis in _execute_redis_operation","sentry.buffer.redis in get","sentry.spans.buffer in _load_segment_data","issues.occurrence_consumer","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","monitors.monitor_consumer","ingest_consumer.process_event","sentry.tasks.store.save_event","sentry.spans.buffer in done_flush_segments","/api/0/sentry-apps/{sentry_app_id_or_slug}/requests/","sentry.issues.tasks.post_process.post_process_group","sentry.tasks.post_process.post_process_group","sentry.workflow_engine.tasks.process_workflows_event","sentry.buffer.redis in _lock_key","sentry.spans.buffer in _ensure_script"],"title":"Redis timeouts across workers during event processing","description":"Multiple Celery and consumer workflows time out on Redis (socket reads and connect) while executing pipelines and increments, causing failures in event grouping, monitor check-in handling, and post-processing. Indicates Redis latency/outage under load affecting Sentry processing paths.","tags":["Caching","Queueing","Networking","Timeout","Redis","Celery","Pipeline Execute"],"cluster_size":91,"cluster_min_similarity":0.899724403378259,"cluster_avg_similarity":0.9503431034186476},{"project_ids":["1"],"cluster_id":256,"group_ids":[6649146441,6756102295],"issue_titles":["TimeoutError"],"root_cause_summaries":["Seer API call lacks timeout, causing `ThreadPoolExecutor` to exceed its 5-second limit when Seer is slow.","Seer API call lacks timeout, causing `requests.post` to hang, exceeding `fetch_issue_summary`'s 5-second limit."],"transactions":["sentry.tasks.activity.send_activity_notifications","sentry.workflow_engine.tasks.trigger_action"],"title":"Slack issue notifications timeout fetching summary","description":"Slack notification workflows time out while fetching the issue summary used to build message blocks/attachments, causing Celery tasks to fail. The summary retrieval future does not return within the configured timeout.","tags":["API","Queueing","Timeout","Celery","Slack"],"cluster_size":2,"cluster_min_similarity":0.9747403687787553,"cluster_avg_similarity":0.9747403687787553},{"project_ids":["1"],"cluster_id":257,"group_ids":[6649197553,6649405110],"issue_titles":["JSONDecodeError: Expecting value: line 1 column 1 (char 0)"],"root_cause_summaries":["Release registry task failed parsing empty HTTP 200 response body as JSON, due to missing empty body check.","External service returned 200 OK with empty body; `response.json()` failed to parse non-JSON empty string."],"transactions":["sentry.tasks.release_registry.fetch_release_registry_data_control","sentry.tasks.release_registry.fetch_release_registry_data"],"title":"Release registry fetch fails on invalid JSON response","description":"Celery workers calling _fetch_registry_url for /aws-lambda-layers crash with JSONDecodeError when response.json() parses an invalid or empty payload from the release registry endpoint.","tags":["API","Serialization","External System","JSONDecodeError","Celery","AWS Lambda"],"cluster_size":2,"cluster_min_similarity":0.9718948437604619,"cluster_avg_similarity":0.9718948437604619},{"project_ids":["1"],"cluster_id":258,"group_ids":[6649892023,6785408699],"issue_titles":["InvalidZipCodeSyntaxException: 500084","InvalidZipCodeSyntaxException: 424631530395"],"root_cause_summaries":["Stripe provided a US card with a non-US postal code, failing strict US ZIP code validation, causing an expected but logged exception.","Customer's US-flagged payment method has a 6-digit postal code, failing US ZIP validation, causing `InvalidZipCodeSyntaxException`."],"transactions":["getsentry.tasks.create_invoices.process_subscription"],"title":"Invalid ZIP code syntax during tax location lookup","description":"Subscription invoicing fails when computing sales tax because the tax location service rejects a malformed postal code input. Errors originate in location.get_location during next_billing_period invoice creation.","tags":["Input Validation","API","Billing","Sales Tax","Invalid Zip Code"],"cluster_size":2,"cluster_min_similarity":0.9665281223457018,"cluster_avg_similarity":0.9665281223457018},{"project_ids":["1"],"cluster_id":260,"group_ids":[6649910808,6684542990,6740528697,6806746401],"issue_titles":["SeatCreationError: Missing uptime Detector for seat assignment","Detector.DoesNotExist: Detector matching query does not exist."],"root_cause_summaries":["Incorrect object type passed to `get_detector` causes database lookup failure for uptime detector, leading to `SeatCreationError`.","Incorrect `ProjectUptimeSubscription` object passed to `get_detector` expecting `UptimeSubscription`, causing detector lookup failure.","Function `get_detector` received `ProjectUptimeSubscription` instead of `UptimeSubscription`, causing no detector to be found.","Incorrect `ProjectUptimeSubscription` object passed to `get_detector`, causing `Detector` query to fail."],"transactions":["tasks.invoices.charge_invoice","/remote/salesforce/provision/","getsentry.tasks.create_invoices.process_subscription"],"title":"Seat assignment fails due to missing Uptime Detector","description":"Billing seat creation during subscription changes and invoicing raises SeatCreationError when the required Uptime Detector record is absent (Detector.DoesNotExist). This breaks invoice generation and plan changes across worker and webhook flows in BillingSeatAssignment.","tags":["Data Integrity","Configuration","API","BillingSeatAssignment","Detector.DoesNotExist","SeatCreationError"],"cluster_size":4,"cluster_min_similarity":0.956061742596141,"cluster_avg_similarity":0.9696019706672852},{"project_ids":["1"],"cluster_id":261,"group_ids":[6649912675,6657780265,6692090848,6802471689],"issue_titles":["SeatCreationError: Missing ProjectUptimeSubscription for seat assignment"],"root_cause_summaries":["Billing task processed stale seat assignment data due to uncommitted deletion cleanup, causing lookup failure.","ProjectUptimeSubscription deletion cleanup failed, leaving an orphaned BillingSeatAssignment that caused an error during new billing period processing.","Billing seat assignment for Uptime subscription existed, but the underlying `ProjectUptimeSubscription` object was deleted, causing lookup failure during billing period transition.","Concurrent deletion of `ProjectUptimeSubscription` during billing process causes `DoesNotExist` when recreating seats from stale snapshot."],"transactions":["getsentry.tasks.create_invoices.process_subscription"],"title":"Seat assignment fails without ProjectUptimeSubscription","description":"During invoice generation, BillingSeatAssignment attempts to create seats but the related ProjectUptimeSubscription record is missing, causing SeatCreationError and aborting subscription processing.","tags":["Data Integrity","API","Configuration","Django ORM","Does Not Exist","BillingSeatAssignment","ProjectUptimeSubscription"],"cluster_size":4,"cluster_min_similarity":0.9481831330002722,"cluster_avg_similarity":0.9610101970525723},{"project_ids":["1"],"cluster_id":262,"group_ids":[6650111614,6697834713,6700792891,6725517791,6732686696,6775626454,6799373869,6803762773],"issue_titles":["ProjectUptimeSubscription.DoesNotExist: ProjectUptimeSubscription matching query does not exist.","Organization.DoesNotExist: Organization matching query does not exist."],"root_cause_summaries":["Orphaned OrganizationStats record referenced non-existent Organization, causing sync task to fail on lookup.","Detector references a non-existent ProjectUptimeSubscription, causing a `DoesNotExist` error during result processing due to data inconsistency.","Deletion cascade orphans UptimeSubscription; its task fails fetching already-deleted ProjectUptimeSubscription, violating assumed dependency.","Detector's DataSource references a deleted ProjectUptimeSubscription, causing lookup failure during uptime result processing.","Project deletion cascades ProjectUptimeSubscription, but orphaned Detector/DataSource objects remain, causing lookup failure in broken monitor checker.","Concurrent deletion of ProjectUptimeSubscription orphaned its UptimeSubscription and Detector, causing subsequent lookup to fail.","Orphaned `ProductTrial` records, due to missing `on_delete` cascade and incomplete organization deletion, cause `Organization.DoesNotExist`.","Organization deletion task omits ProductTrial cleanup, leading to orphaned records that cause `Organization.DoesNotExist` during email processing."],"transactions":["getsentry.tasks.stats in sync_organization","getsentry.tasks.send_product_trial_emails","sentry.uptime.models in get_project_subscription","sentry.deletions.tasks.run_deletion","sentry.uptime.tasks.broken_monitor_checker","monitors.uptime.result_consumer.ResultProcessor"],"title":"Missing DB rows break org sync and uptime tasks","description":"Background workers fail when expected records are absent: Organization lookups during trial email/sync tasks and ProjectUptimeSubscription lookups during uptime monitor cleanup. Likely stale references or deleted entities cause DoesNotExist exceptions, interrupting Celery task execution.","tags":["Database","Background Jobs","Data Integrity","Django ORM","DoesNotExist","Celery","Stale Reference"],"cluster_size":8,"cluster_min_similarity":0.9170175614430833,"cluster_avg_similarity":0.9434908651307652},{"project_ids":["1"],"cluster_id":265,"group_ids":[6651054841,6793463459],"issue_titles":["UnsupportedResponseType: text/html; charset=utf-8"],"root_cause_summaries":["GitLab API returned HTML with 200 OK due to authentication/authorization failure, causing Sentry's JSON parser to fail.","GitLab API returned HTML with 200 OK, not JSON, due to likely authentication/permission issue, causing JSONDecodeError and UnsupportedResponseType."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow"],"title":"PR diffs API returns HTML causing JSON parse failure","description":"The client fetching pull request diffs receives an HTML response (text/html) instead of JSON, leading to UnsupportedResponseType and JSONDecodeError in the open_pr_comment_workflow Celery task. Likely an upstream API change or error page being returned.","tags":["API","Serialization","External System","JSONDecodeError","Unsupported Response Type","Celery"],"cluster_size":2,"cluster_min_similarity":0.9687012864853572,"cluster_avg_similarity":0.9687012864853572},{"project_ids":["1"],"cluster_id":266,"group_ids":[6651073414,6656692286,6793075224],"issue_titles":["Subscription.DoesNotExist: Subscription matching query does not exist."],"root_cause_summaries":["Subscription creation transaction failed, leaving no record for the organization, causing subsequent task execution to fail.","Organization lacks a subscription because its creation bypassed the `org_setup_complete` signal, preventing automatic provisioning.","Subscription record for organization 4509667383640064 is missing, likely due to creation failure or bypass."],"transactions":["getsentry.tasks.stats.sync_organization","getsentry.tasks.stats.sync_outdated","getsentry.billing.tasks.usagebuffer.flush_usage_buffer"],"title":"Missing Subscription for Organization lookups","description":"Celery workers fail when syncing organization stats and flushing usage because Subscription.get_for_organization_id returns no record, causing Subscription.DoesNotExist errors. Impact: background tasks for organization metrics and usage buffering abort for orgs without a subscription entry.","tags":["Data Integrity","Background Jobs","Django ORM","Record Not Found","Celery"],"cluster_size":3,"cluster_min_similarity":0.9580640433802775,"cluster_avg_similarity":0.9593087740082225},{"project_ids":["1"],"cluster_id":267,"group_ids":[6651075941,6747693223,6772806621,6804142109,6804142127,6804142134],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\n')","OperationalError: QueryCanceled('canceling statement due to user request\\nCONTEXT:  while rechecking updated tuple (50292,69) in relation \"hybridcloud_regioncacheversion\"\\n')","OperationalError: canceling statement due to user request","OperationalError: canceling statement due to lock timeout"],"root_cause_summaries":["Concurrent webhook processing tasks' UPDATE/DELETE operations on `hybridcloud_webhookpayload` table cause `SELECT` queries to block and timeout.","Concurrent `schedule_webhook_delivery` updates and immediate `drain_mailbox` reads on `hybridcloud_webhookpayload` cause database lock contention and timeouts.","Concurrent cache version updates without row-level locking caused database contention, leading to query cancellation due to timeout.","Concurrent `flush_usage_buffer` tasks contend for `BillingHistory` locks, causing database query cancellations due to timeouts.","Concurrent monitor check-in processing causes database lock contention on shared billing metric history records, leading to query cancellation.","Concurrent webhook processing tasks contend for database row locks, causing SELECT queries to time out."],"transactions":["monitors.monitor_consumer","sentry.hybridcloud.tasks.deliver_webhooks in drain_mailbox","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","getsentry.billing.tasks.usagebuffer in flush_usage_buffer"],"title":"PostgreSQL lock timeouts and cancels during billing and webhook updates","description":"Multiple PostgreSQL operations are being canceled or timing out due to row/tuple locks while updating billing metrics and hybridcloud cache/webhook tables. This suggests high contention or long-held locks in billing and hybridcloud workflows, causing task failures and error reporting issues.","tags":["Database","Concurrency","API","PostgreSQL","Lock Timeout","Query Canceled","BillingMetrics","HybridCloud Webhooks"],"cluster_size":6,"cluster_min_similarity":0.9201742102833134,"cluster_avg_similarity":0.9442486870372799},{"project_ids":["1"],"cluster_id":268,"group_ids":[6651314909,6755588583,6802532470,6805899738],"issue_titles":["ObjectErrorUnknown: invalid MachO file"],"root_cause_summaries":["Uploaded MachO file was structurally malformed, causing symbolic library parsing failure despite valid checksum.","Uploaded `Cloaked Dev.debug.dylib` is a malformed MachO file, causing `symbolic` library to fail parsing due to an invalid internal type size.","MachO file's internal structure is corrupted, causing symbolic library parsing to fail despite correct overall file checksum.","Uploaded MachO file contains structurally invalid internal metadata, causing symbolic library parsing failure despite byte-level integrity."],"transactions":["sentry.tasks.assemble.assemble_dif"],"title":"Invalid Mach-O debug file parsed during DIF assembly","description":"The DIF assembly path fails while iterating archive objects because the uploaded Mach-O file is invalid or has oversized type fields, causing ObjectErrorUnknown in debug file detection. This affects symbol processing in the assemble_dif workflow, including Celery and multiprocessing workers.","tags":["Serialization","Input Validation","API","Mach-O","Debug Information Files","Invalid Artifact"],"cluster_size":4,"cluster_min_similarity":0.9477864257947589,"cluster_avg_similarity":0.9626068081027056},{"project_ids":["1"],"cluster_id":269,"group_ids":[6651447929,6719665046,6726050995,6726318603,6726840835,6729688011,6731229766,6731244674,6731780435,6733333883,6738022289,6738082743,6738109437,6743798230,6755356839,6766705201,6776333163],"issue_titles":["UnidiffParseError: Unexpected hunk found: @@ -20,6 +20,7 @@ from comet.cluster import MachineNotFound","UnidiffParseError: Unexpected hunk found: @@ -1,114 +0,0 @@","UnidiffParseError: Unexpected hunk found: @@ -895,6 +895,7 @@ def create_new_serverless_alarms(task_id: int, new_alarms: list['ServerlessAlarm","UnidiffParseError: Unexpected hunk found: @@ -1694,7 +1694,7 @@ def test_worm_legal_hold(precreate_nfs4_cli_env, policy_flavor, remove_write_per","UnidiffParseError: Unexpected hunk found: @@ -163,7 +163,9 @@ export class TilMember {","UnidiffParseError: Unexpected hunk found: @@ -75,15 +75,14 @@ def create_kubernetes_cluster(access_headers: AccessHeaders, body: KubernetesClu","UnidiffParseError: Unexpected hunk found: @@ -19,8 +19,8 @@ export class TriggerService {","UnidiffParseError: Unexpected hunk found: @@ -34,7 +34,8 @@ def delete_function_revision_by_id(access_headers: AccessHeaders, guid: str):","UnidiffParseError: Unexpected hunk found: @@ -2698,6 +2698,9 @@ def test_s3_multiproto_rebucketize(system, s3: use.system_backend): # ORION-2683","UnidiffParseError: Unexpected hunk found: @@ -19,7 +19,7 @@ import tests.test_utils as test_utils","UnidiffParseError: Unexpected hunk found: @@ -0,0 +1,22 @@","UnidiffParseError: Unexpected hunk found: @@ -87,6 +87,7 @@ class PerformanceReportStandardization implements ShouldQueue","UnidiffParseError: Unexpected hunk found: @@ -174,7 +174,7 @@ export default function MissionForm({ mission }: { mission: MissionFormSchema })","UnidiffParseError: Unexpected hunk found: @@ -8132,7 +8132,6 @@ def check_hosts_box_uid(task_id, hosts: 'Iterable[Host]') -> List:","UnidiffParseError: Unexpected hunk found: @@ -1036,7 +1036,7 @@ class NodeMachine(MachineBase):","UnidiffParseError: Unexpected hunk found: @@ -117,7 +117,7 @@ export class MtTenantCreateComponent extends InterconnectedModalDirective implem","UnidiffParseError: Unexpected hunk found: @@ -0,0 +1,248 @@"],"root_cause_summaries":["Older GitLab API returns diffs without file headers; unidiff library expects them, causing parsing failure and workflow interruption.","Older GitLab versions return incomplete unidiff patches, causing `unidiff` library to fail parsing due to missing file headers.","GitLab API returns malformed unified diffs (missing file headers) for older versions, causing unidiff library to fail.","Older GitLab API returns diffs missing file headers; unidiff library fails to parse unexpected hunk.","Older GitLab API versions return malformed diffs, causing unidiff parser to fail due to missing file headers.","GitLab instance older than 16.5 returns malformed diffs, causing unidiff to fail parsing due to missing file headers.","GitLab API returned malformed diff lacking file headers, causing unidiff library to raise parse error.","GitLab API's incomplete diffs from older versions cause `UnidiffParseError`, unhandled by specific `PatchParseError` catch.","Older GitLab versions return malformed diffs missing file headers, causing unidiff library to fail parsing.","Older GitLab versions return incomplete unidiffs, causing `unidiff` library to fail parsing due to missing file headers.","GitLab API returns incomplete diffs, causing unidiff parser to fail on missing file headers.","Older GitLab instance returns incomplete unidiff patches, causing `unidiff` library to fail parsing due to missing file headers.","GitLab API returns incomplete diffs missing file headers, causing `unidiff` to raise `UnidiffParseError` when parsing.","GitLab API returns malformed diffs for file deletions, causing `unidiff` to fail due to missing file headers.","GitLab API for older versions returns incomplete diffs, causing unidiff parser to fail due to missing file headers.","GitLab API returns malformed unified diffs missing file headers, causing `unidiff` to fail parsing and raise `UnidiffParseError`.","Older GitLab API versions return diffs without file headers, causing `unidiff` to raise `UnidiffParseError` when parsing unexpected hunks."],"transactions":["sentry.utils.patch_set in patch_to_file_modifications","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow"],"title":"Unidiff parsing fails on malformed PR patches","description":"PR diff payloads cause unidiff.PatchSet.from_string to raise UnidiffParseError during open_pr_comment_workflow, preventing safe file analysis and comment generation.","tags":["API","Serialization","Queueing","Celery","UnidiffParseError","Patch Parsing","Open PR Comment Workflow"],"cluster_size":17,"cluster_min_similarity":0.9599714349664931,"cluster_avg_similarity":0.9751128539684406},{"project_ids":["1"],"cluster_id":271,"group_ids":[6651512039,6651512051],"issue_titles":["ApiError: {\"type\": \"error\", \"error\": {\"message\": \"Something went wrong\", \"id\": \"4d31abd7015749cba9c3d22820fc9f7a\"}}","IntegrationError: Error Communicating with Bitbucket (HTTP 500): Something went wrong"],"root_cause_summaries":["Bitbucket integration's `zip_commit_data` lacks resilience; single failed diff request halts entire commit fetching process.","Commit fetching fails entirely if any single commit's diff cannot be retrieved, due to missing error handling in `zip_commit_data`."],"transactions":["sentry.tasks.commits.fetch_commits"],"title":"Bitbucket commits API returns 5xx during fetch","description":"Fetching commit data and file changes from Bitbucket fails with server-side 5xx errors, causing IntegrationError in the commits ingestion task. Impact: commit comparison and patch retrieval are blocked for affected repositories.","tags":["External System","API","Upstream Unavailable","HTTP 5xx","Bitbucket"],"cluster_size":2,"cluster_min_similarity":0.9648913631875176,"cluster_avg_similarity":0.9648913631875176},{"project_ids":["1"],"cluster_id":272,"group_ids":[6651778210,6718509196,6760363680],"issue_titles":["Group.DoesNotExist: Group matching query does not exist.","RetryError"],"root_cause_summaries":["Newly created Group not found on database replica due to replication lag, triggering task retry.","Asynchronous task attempts to retrieve Group before its creating database transaction commits, causing a race condition.","Task enqueued before Group creation committed, causing persistent `DoesNotExist` errors despite retries."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.taskworker.retry in retry_task"],"title":"Sentry App resource change retries on missing Group","description":"The Sentry App resource change task fails when fetching a Group by id, raising Group.DoesNotExist and repeatedly retrying, which escalates to RetryError. This likely occurs when the referenced issue was deleted or the instance_id is stale.","tags":["API","Data Integrity","Queueing","Django","Celery","DoesNotExist","Retries Exhausted"],"cluster_size":3,"cluster_min_similarity":0.9443376905637231,"cluster_avg_similarity":0.9508957479598731},{"project_ids":["1"],"cluster_id":273,"group_ids":[6651798515,6681895680],"issue_titles":["ValueError: epm() is not valid search term","ValueError: failure_rate() is not valid search term"],"root_cause_summaries":["The `resolve_aggregate_term` function did not support `Column.BinaryFormula` types for aggregate filters, causing a `ValueError`.","Aggregate term validation failed for `epm()` because `Column.BinaryFormula` was not an allowed type."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events-stats/"],"title":"Invalid aggregate terms in query resolver","description":"Queries using aggregate functions like epm() and failure_rate() are rejected by the search resolver, causing ValueError during organization events timeseries requests. Likely missing or unsupported aggregate definitions in the resolver/having-clause handling for events/spans RPC queries.","tags":["API","Input Validation","Configuration","Query Resolver","Invalid Search Term"],"cluster_size":2,"cluster_min_similarity":0.9631193305114873,"cluster_avg_similarity":0.9631193305114873},{"project_ids":["1"],"cluster_id":393,"group_ids":[6651862458,6678609134,6722328519,6722372896,6722397504,6722419425,6722424709,6725832598,6730822918,6733692058],"issue_titles":["ApiError: {\"$id\":\"1\",\"innerException\":null,\"message\":\"TF401232: Work item 1569 does not exist, or you do not have permissions to read it.\",\"typeName\":\"Microsoft.TeamFoundation.WorkItemTracking.Server.WorkItemUnauthorizedAccessException, Microsoft.TeamFoundation.W...","ApiError: {\"message\":\"404 Commit Not Found\"}","ApiError: {\"message\":\"404 Project Not Found\"}","IntegrationError: Error Communicating with GitHub (HTTP 404): If this repository exists, ensure that your installation has permission to access this repository (https://github.com/settings/installations).","ApiError: {\"message\":\"404 Ref Not Found\"}","ApiError: {\"message\":\"404 File Not Found\"}"],"root_cause_summaries":["Sentry's internal integration proxy returned a 404, failing to route the GitLab API request, leading to a misleading '404 Project Not Found' error.","Internal integration proxy returned 404, preventing external API call; misleading Azure DevOps error propagated.","Sentry's internal integration proxy returned 404 for GitLab commit API request, preventing actual GitLab call.","Internal integration proxy returns 404 for GitLab blame requests, preventing commit context retrieval.","Sentry's internal `sentry-rpc-control` proxy returned 404 for its endpoint, preventing GitLab API requests.","Internal integration proxy service returned 404, preventing external API call to GitHub.","Region silo misconfigures proxy headers, causing control silo's integration proxy to self-reference and return 404.","Region silo's internal proxy request to Control silo failed with 404, misreporting as GitLab 'Ref Not Found' error.","Sentry's internal integration proxy service returned 404, preventing external API calls from reaching their destination.","Outdated GitLab project ID in Sentry's Repository config causes 404 when fetching file blame data."],"transactions":["sentry.tasks.process_commit_context","sentry.tasks.commits.fetch_commits","sentry.shared_integrations.client.base in _request","sentry.integrations.tasks.sync_status_outbound","/api/0/issues|groups/{issue_id}/integrations/{integration_id}/"],"title":"VCS 404s: commit/project/file not found on compare/blame","description":"Background workers calling the VCS provider return 404 Not Found for commits, files, or project IDs during compare_commits and blame retrieval, indicating missing or invalid identifiers in repository configuration or references. This breaks commit fetching and commit context enrichment tasks.","tags":["External System","API","Configuration","HTTP","Not Found (404)","Commit Compare","Blame Lookup"],"cluster_size":10,"cluster_min_similarity":0.9174596999945752,"cluster_avg_similarity":0.9449834288531211},{"project_ids":["1"],"cluster_id":274,"group_ids":[6652011736,6725522988],"issue_titles":["VertexRequestFailed: Response 500: {"],"root_cause_summaries":["Vertex AI API request payload's `contents` field is an object, not an array, causing a 500 internal error.","Vertex AI 500 error caused by Sentry sending malformed request payload with `contents` as object, not array."],"transactions":["sentry.tasks.store.save_event_feedback","sentry.issues.tasks.post_process.post_process_group"],"title":"Vertex AI spam check requests failing in feedback flow","description":"Spam detection for user feedback calls Vertex AI via the complete_prompt backend and raises VertexRequestFailed, impacting create_feedback_issue during event save and post-process pipelines.","tags":["External System","API","Configuration","Google Vertex AI","Request Failed"],"cluster_size":2,"cluster_min_similarity":0.9760764945236393,"cluster_avg_similarity":0.9760764945236393},{"project_ids":["1"],"cluster_id":278,"group_ids":[6652724424,6684542720,6692092636,6739993423,6757703349],"issue_titles":["KeyError: 'trial_usage.end.sent'"],"root_cause_summaries":["Analytics event registered in `getsentry/getsentry` not registered in `getsentry/sentry`'s `EventManager`.","Analytics event 'trial_usage.end.sent' unregistered in Celery worker due to module import order, causing KeyError.","Analytics event 'trial_usage.end.sent' not registered in EventManager, causing KeyError during lookup.","Analytics event 'trial_usage.end.sent' is unregistered in Sentry's EventManager, as it's defined in a separate codebase without cross-repository registration.","EventManager lacks 'trial_usage.end.sent' due to separate analytics registries across repositories."],"transactions":["tasks.invoices.create_invoices","getsentry.tasks.quotas.send_trial_end_notification"],"title":"Missing analytics event 'trial_usage.end.sent' in worker","description":"Celery quota notification tasks fail when recording analytics for trial end emails because the event manager lacks the 'trial_usage.end.sent' event definition. This prevents email notification tracking and may block or retry the send flow.","tags":["Configuration","Queueing","API","Celery","Analytics Event Missing","KeyError"],"cluster_size":5,"cluster_min_similarity":0.9513309660611039,"cluster_avg_similarity":0.9671124198587087},{"project_ids":["1"],"cluster_id":282,"group_ids":[6653437040,6739187267,6752656127,6795107968],"issue_titles":["InvalidSearchQuery: p95(span.self_time): column argument invalid: span.self_time is not a valid column","InvalidSearchQuery: p90(span.self_time): column argument invalid: span.self_time is not a valid column"],"root_cause_summaries":["Percentile function's NumericColumn lacks span-awareness, failing to validate `span.self_time` as a valid column.","Avg function's NumericColumn argument lacks span column support, causing validation failure for 'span.self_time' during notification processing.","p90 function's NumericColumn argument lacks `spans=True`, preventing `span.self_time` validation, causing `InvalidSearchQuery`.","P95 function's NumericColumn argument lacks span-specific column recognition, causing validation failure for 'span.self_time'."],"transactions":["sentry.incidents.tasks.handle_trigger_action"],"title":"Metric alert queries use invalid span.self_time","description":"Alert notifications fail while building charts because p90/p95 aggregates reference span.self_time, which is not a recognized column in the metrics query translation path. This breaks metric alert fire/resolve handlers when constructing notification payloads.","tags":["API","Input Validation","Configuration","Sentry Alerts","Invalid Column","Aggregate Function","Metric Alerts"],"cluster_size":4,"cluster_min_similarity":0.9644072797577843,"cluster_avg_similarity":0.9714690029678916},{"project_ids":["1"],"cluster_id":275,"group_ids":[6653547601,6653547609,6682871523,6683000364,6731284144,6731284155,6731284168,6731284170,6731284171,6731284173,6731284174,6731284178,6731284179,6731284180,6735764102,6735764119],"issue_titles":["IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID BEFC:26796F:3CB614:7A7C6A:686B9CF0 and timestamp 2025-07-07 10:09:52 UTC.","ApiRateLimitedError: Rate limit for this resource has been exceeded","ApiError: {\"message\":\"API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 8138:3C6521:A4ACA:14C9F1:68777291 and timestamp 2025-07-16 09:36:17 UTC.\",\"documentation_url\":\"https://docs.githu...","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 951C:3968EB:21EB118:44D9595:6863ABB8 and timestamp 2025-07-01 09:34:48 UTC.","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 9788:3023C8:3383456:67EB9EE:686E3DCF and timestamp 2025-07-09 10:00:47 UTC.","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 9D88:3F7D45:377B2:70368:686E3DCF and timestamp 2025-07-09 10:00:47 UTC.","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 9D20:266D4F:3E2E8C:7D3A41:6878C709 and timestamp 2025-07-17 09:48:57 UTC.","IntegrationError: Error Communicating with Bitbucket (HTTP 429): unknown error","ApiError: {\"message\":\"API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID DC4C:15CEE7:78EE2:F46B0:6863ABB8 and timestamp 2025-07-01 09:34:48 UTC.\",\"documentation_url\":\"https://docs.github...","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 999E:1766D1:BAB200:178E472:6876235C and timestamp 2025-07-15 09:46:04 UTC.","ApiError: {\"message\":\"API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID A8EA:AB18D:11D1668:240CCD2:687F5BA4 and timestamp 2025-07-22 09:36:36 UTC.\",\"documentation_url\":\"https://docs.git...","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID C00C:1DC747:197C7A:335A6D:6876235C and timestamp 2025-07-15 09:46:04 UTC.","ApiError: {\"message\":\"API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID 8610:234739:1166C87:2335F04:687F5BA4 and timestamp 2025-07-22 09:36:36 UTC.\",\"documentation_url\":\"https://docs.gi...","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID D30A:100D4D:78521A9:F40B457:6874D9FE and timestamp 2025-07-14 10:20:46 UTC.","IntegrationError: Error Communicating with GitHub (HTTP 403): API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID C014:22A0A7:16AB8C:2DBAA4:6876235C and timestamp 2025-07-15 09:46:04 UTC.","ApiError: {\"message\":\"API rate limit exceeded for installation ID 8932293. If you reach out to GitHub Support for help, please include the request ID E402:8CF1:3A9FAA2:76ED61E:6874D9FE and timestamp 2025-07-14 10:20:46 UTC.\",\"documentation_url\":\"https://docs.gith..."],"root_cause_summaries":["GitHub API rate limit exceeded due to N+1 query problem: each commit in a range triggered a separate API call for its patchset.","GitHub integration's commit fetching makes N+1 API calls for patchsets, exhausting rate limits.","Bitbucket integration makes one API call per commit to fetch diffs, causing rate limits.","GitHub App lacks permission for specific commit, despite successful compare API, causing `get_commit` to fail.","N+1 GitHub API calls for commit details exhausted rate limits, triggered by processing a large commit range.","Fetching commit patchsets makes one API call per commit, exceeding GitHub rate limits for large commit ranges.","GitHub API rate limit exceeded due to N+1 individual commit patchset fetches for a large commit range.","GitHub API rate limit exceeded due to N+1 calls fetching each commit's patchset individually.","Fetching 456 commit patchsets individually from GitHub exhausted API rate limit.","GitHub integration's N+1 API calls for commit patchsets exhaust rate limits when processing many commits.","Unbounded GitHub API calls for each commit's patch set within large release ranges exhaust rate limits.","Fetching commit patchsets makes an N+1 API call for each commit, exhausting GitHub's rate limit for large commit ranges.","Fetching commit patchsets individually for large releases causes burst API calls, exhausting GitHub's rate limit.","Bitbucket integration's N+1 API calls for commit diffs exhaust rate limits, causing `ApiRateLimitedError`.","GitHub integration's per-commit patchset fetching, for 453 commits, exhausted API rate limits, causing IntegrationError.","Sequential fetching of 928 individual commit patchsets from GitHub exhausted the API rate limit."],"transactions":["sentry.tasks.commits.fetch_commits"],"title":"VCS commit fetches hit external API rate limits","description":"Background jobs fetching commit data from GitHub and Bitbucket exceed provider rate limits, leading to 403 Forbidden and 429 Too Many Requests errors when calling compare/get commit endpoints. This blocks commit diffs/patch sets from being collected by the integration workers.","tags":["External System","API","Rate Limiting","GitHub","Bitbucket","HTTP 403","HTTP 429"],"cluster_size":16,"cluster_min_similarity":0.9447782452240644,"cluster_avg_similarity":0.9730848537190345},{"project_ids":["11276"],"cluster_id":284,"group_ids":[6653672603,6655337811,6720267282,6734323300],"issue_titles":["Error: Project \"4509809295360000\" not found in useTraceItemDetails","Error: Project \"4509667479060480\" not found in useTraceItemDetails","Error: Project \"4506029437091841\" not found in useTraceItemDetails","Error: Project \"4509821264592976\" not found in useTraceItemDetails"],"root_cause_summaries":["Trace event references project inaccessible to user, causing `useProjectFromId` to fail and `useTraceItemDetails` to error.","User lacks access to a project within a trace, causing `useProjectFromId` to return null, triggering an error.","Log data contains project IDs inaccessible to the user, causing `useProjectFromId` to return null, leading to an explicit error capture.","Trace item's project ID not found in user's accessible projects, causing `useTraceItemDetails` to error."],"transactions":["/explore/traces/","/issues/:groupId/","/explore/traces/trace/:traceSlug/"],"title":"Trace views fail when project ID missing in hook","description":"React trace views (TraceView, EventTraceView, Logs table row) throw 'Project not found' in useTraceItemDetails when an expected projectId is absent or unmapped, breaking trace root event fetching.","tags":["Configuration","UI","Input Validation","React","TraceView","Missing Project Mapping"],"cluster_size":4,"cluster_min_similarity":0.9362304442092019,"cluster_avg_similarity":0.9565462136383901},{"project_ids":["1"],"cluster_id":288,"group_ids":[6654174420,6675798865,6675802701,6675812055,6675815521,6675818430,6675819271,6734979698,6794933340],"issue_titles":["RetryError: Timeout of 5.0s exceeded","RetryError: Timeout of 60.0s exceeded"],"root_cause_summaries":["GCE metadata service 504 timeout prevented Bigtable authentication, causing read operation to exceed its 5s timeout.","Bigtable client failed authentication with Google Cloud, causing read operation to exceed its 5-second timeout.","Bigtable authentication backend unavailable, causing 5-second read timeout.","Bigtable client failed authentication, causing read operation retries to time out.","BigtableKVStorage.get_many lacks explicit timeout, causing 60s retry on Google metadata service 504, unlike get()'s 5s timeout.","Bigtable client authentication failures cause gRPC connection hangs, leading to 60-second read timeouts.","Google Cloud IAM failed to generate access tokens for `getsentry-web` service account, causing Bigtable client retries to timeout.","Bigtable client failed authentication due to unavailable backend, causing read operation to timeout after retries.","Bigtable client failed authentication due to \"Authentication backend unavailable\" error, causing read operation to time out."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/","/api/0/issues|groups/{issue_id}/events/{event_id}/","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/","sentry.tasks.auto_source_code_config","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/json/"],"title":"Bigtable reads time out due to GCP auth/unavailable","description":"Reads from the Nodestore Bigtable backend are failing with gRPC UNAVAILABLE/DEADLINE_EXCEEDED and authentication metadata retrieval errors, causing timeouts in workers and HTTP requests. This indicates transient or regional issues with Google Cloud Bigtable or its metadata/IAM auth path impacting event fetches.","tags":["External System","Networking","Caching","API","Google Cloud Bigtable","gRPC","Timeout","Upstream Unavailable"],"cluster_size":9,"cluster_min_similarity":0.9348537457949273,"cluster_avg_similarity":0.9573853608355604},{"project_ids":["11276"],"cluster_id":291,"group_ids":[6654549368,6667738462],"issue_titles":["Error: useOrganization called but organization is not set.: /issues/","Error: useOrganization called but organization is not set."],"root_cause_summaries":["Route lacks organization slug; middleware clears session, preventing organization context load.","Organization context null because `orgSlug` fallback failed, skipping data fetch, causing `useOrganization` to error."],"transactions":["/issues/:groupId/","/"],"title":"useOrganization hook used without OrganizationContext","description":"React components under Organization routes call useOrganization before the OrganizationContextProvider is set, causing ErrorBoundary crashes on pages like GroupDetails and Overview. Likely a missing or misordered provider in App/LegacyApp layout composition.","tags":["Configuration","API","React","Context Provider Missing","useOrganization"],"cluster_size":2,"cluster_min_similarity":0.960294875663464,"cluster_avg_similarity":0.960294875663464},{"project_ids":["1"],"cluster_id":292,"group_ids":[6654577747,6801575458],"issue_titles":["KeyError: 3629684","KeyError: 3435499"],"root_cause_summaries":["Quota notification code failed to handle `get_email_addresses` returning no email for a user, causing a `KeyError`.","User 3250631 lacks a resolvable email, causing KeyError when notification analytics attempts email lookup."],"transactions":["getsentry.tasks.quotas._send_quota_exceeded_notification"],"title":"KeyError in quota notifications when resolving recipient email","description":"Quota exceeded notifications fail while populating analytics params, raising a KeyError when looking up recipient emails from get_email_addresses(). This breaks both Slack and Email notification flows in the Celery worker.","tags":["API","Configuration","Celery","KeyError","Email Lookup","Slack Notifications","Quota Notifications"],"cluster_size":2,"cluster_min_similarity":0.9728070460902796,"cluster_avg_similarity":0.9728070460902796},{"project_ids":["11276"],"cluster_id":294,"group_ids":[6654690647,6656082102,6738051106],"issue_titles":["HierarchyRequestError: Node.appendChild: Cannot have more than one DocumentType child of a Document","HierarchyRequestError: Failed to execute 'appendChild' on 'Node': Can't insert a doctype after the root element."],"root_cause_summaries":["Synthetic FullSnapshot generation creates multiple DocumentType nodes, violating DOM rules when rrweb rebuilds the snapshot.","ReplayReader injects multiple DOCTYPEs into rrweb event stream, causing HierarchyRequestError during DOM reconstruction.","Mock FullSnapshot events with DOCTYPEs are repeatedly inserted into the rrweb stream, causing DOM reconstruction to fail."],"transactions":["/explore/replays/:replaySlug/","/replays/:replaySlug/"],"title":"rrweb replay appends duplicate DOCTYPE nodes","description":"During replay, rrweb attempts to rebuild a full snapshot and appends a second DocumentType under Document, triggering HierarchyRequestError. This occurs when playing or resuming playback in the replay UI.","tags":["Serialization","DOM","rrweb","HierarchyRequestError","Duplicate DOCTYPE"],"cluster_size":3,"cluster_min_similarity":0.9691531627941635,"cluster_avg_similarity":0.9751112267743176},{"project_ids":["300688","1"],"cluster_id":296,"group_ids":[6654966528,6665075545],"issue_titles":["BadSnubaRPCRequestException: sentry.timestamp can only be compared to TYPE_INT or TYPE_DOUBLE, got TYPE_STRING","SnubaRPCError: code: 400"],"root_cause_summaries":["Sentry's `SearchResolver` incorrectly sets `sentry.timestamp`'s `AttributeKey.type` to `TYPE_STRING` instead of `TYPE_DOUBLE` in Snuba RPC requests.","Snuba query failed because `sentry.timestamp` was sent as `TYPE_STRING` due to `search_type=\"string\"` in its definition, but Snuba expects numeric for comparisons."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events/","EndpointTimeSeries__v1"],"title":"Snuba RPC rejects timestamp string and bad ordering","description":"Queries to Snuba RPC fail because sentry.timestamp is compared as a string instead of a numeric type, and results are ordered by columns not included in the selected fields. This breaks time-series and event queries in the logs/organization events endpoints.","tags":["API","Serialization","Input Validation","Snuba","Type Mismatch","Invalid Order By"],"cluster_size":2,"cluster_min_similarity":0.9673722624537897,"cluster_avg_similarity":0.9673722624537897},{"project_ids":["1"],"cluster_id":297,"group_ids":[6655011317,6658488121],"issue_titles":["IntegrationFormError: {'diff': 'Got not_available value: unknown value for field: diff'}","IntegrationFormError: {'detail': 'Some given field was misconfigured'}"],"root_cause_summaries":["GitHub API failed to generate diff due to complexity, miscategorized as `IntegrationFormError` by Sentry's integration error handling.","GitHub API timed out fetching commit details, causing Sentry's integration to raise a generic configuration error."],"transactions":["sentry.tasks.commits.fetch_commits"],"title":"GitHub compare API returns diff not_available","description":"Background workers fetching commit comparisons hit GitHubs compare/commit endpoints which return Unprocessable Entity with diff marked not_available due to long diff generation, causing IntegrationFormError propagation. Impact: commit fetching and patchset creation fail for large or complex diffs.","tags":["External System","API","Input Validation","GitHub","Upstream Unavailable","Unprocessable Entity","IntegrationFormError"],"cluster_size":2,"cluster_min_similarity":0.9605999875040482,"cluster_avg_similarity":0.9605999875040482},{"project_ids":["1"],"cluster_id":298,"group_ids":[6656050005,6725421805,6725421897,6775280372,6775316673,6775350584,6775384336,6775486526,6793468097],"issue_titles":["IncompatibleMetricsQuery: transaction.duration is not a tag in the metrics dataset","IncompatibleMetricsQuery: url is not a tag in the metrics dataset","SubscriptionError: appPublishedTime is not a tag in the metrics dataset","SubscriptionError: url is not a tag in the metrics dataset","SubscriptionError: transaction.duration is not a tag in the metrics dataset","SubscriptionError: measurements.frames_slow_rate is not a tag in the metrics dataset","SubscriptionError: route is not a tag in the metrics dataset"],"root_cause_summaries":["Metric field `transaction.duration` incorrectly routed to tag validation, failing due to `organizations:mep-use-default-tags` feature flag.","Legacy query deletion fails due to `organizations:mep-use-default-tags` feature flag enforcing tag whitelist on old, unlisted tags.","Subscription deletion fails due to `url` field validation, ignoring `skip_field_validation_for_entity_subscription_deletion` flag.","Old subscription query with 'route' tag fails new validation due to `mep-use-default-tags` feature flag during deletion.","Legacy subscription's custom tag 'appPublishedTime' incompatible with current strict default tag validation feature flag.","Subscription deletion fails because `transaction.duration` (a metric) is strictly validated as a tag due to an enabled feature flag, causing an `IncompatibleMetricsQuery`.","Subscription deletion fails because `appPublishedTime` is not a default metric tag, and a feature flag enforces this restriction.","Enabled feature flag `organizations:mep-use-default-tags` causes validation failure for existing subscriptions with non-default tags during deletion.","Subscription deletion fails because `url` tag, valid during creation, is disallowed by current `mep-use-default-tags` feature flag."],"transactions":["sentry.search.events.builder.metrics in resolve_tag_key","sentry.snuba.tasks.delete_subscription_from_snuba","sentry.snuba.tasks in delete_subscription_from_snuba"],"title":"Metrics queries use non-tag fields (url, transaction.duration)","description":"AlertMetricsQueryBuilder rejects filters referencing url and transaction.duration as tags, causing IncompatibleMetricsQuery and SubscriptionError during subscription deletion and query resolution. Queries must use valid metrics tags or map these fields to supported dimensions.","tags":["API","Configuration","Data Integrity","Sentry Metrics","Incompatible Query","Subscription Error"],"cluster_size":9,"cluster_min_similarity":0.9488166985225271,"cluster_avg_similarity":0.9689441670201906},{"project_ids":["1"],"cluster_id":302,"group_ids":[6656709798,6802477486],"issue_titles":["File.DoesNotExist: File matching query does not exist."],"root_cause_summaries":["Batch deletion of `ArtifactBundle`s referencing a shared `File` causes `File.DoesNotExist` when `post_delete` attempts to delete an already-removed file.","Redundant `instance.file.delete()` in `post_delete` signal handler attempts to delete `File` already removed by `on_delete=CASCADE`."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/files/artifact-bundles/","sentry.tasks.assemble.assemble_artifacts"],"title":"Artifact bundle deletion fails when file missing","description":"Deletion logic for artifact bundles assumes a 'file' exists on the instance, raising KeyError when the underlying file record is absent and File.DoesNotExist occurs during cleanup in both API delete and assembler paths.","tags":["Data Integrity","API","Serialization","KeyError","File.DoesNotExist","Artifact Bundle"],"cluster_size":2,"cluster_min_similarity":0.9607380521803621,"cluster_avg_similarity":0.9607380521803621},{"project_ids":["1"],"cluster_id":303,"group_ids":[6656992606,6675784203],"issue_titles":["VertexRequestFailed: Response 500: {"],"root_cause_summaries":["Vertex AI `gemini-2.0-flash-lite-001` model unavailable or misconfigured for `internal-sentry` project, causing 500 API error.","Vertex AI's `gemini-2.0-flash-lite-001` model returned a 500 internal error during spam detection, causing `VertexRequestFailed`."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.tasks.post_process.post_process_group"],"title":"Vertex AI requests fail during spam detection","description":"Post-processing tasks calling the spam detection prompt fail with VertexRequestFailed when the Vertex AI backend returns an error response, preventing feedback issue creation. The failures occur in the link_event_to_user_report pipeline within Celery workers.","tags":["External System","API","Queueing","Google Cloud Vertex AI","HTTP Error","Celery","Spam Detection"],"cluster_size":2,"cluster_min_similarity":0.9604958930180602,"cluster_avg_similarity":0.9604958930180602},{"project_ids":["1"],"cluster_id":304,"group_ids":[6657168381,6794697352],"issue_titles":["UnknownOption: 'feature.organizations:user-feedback-ai-titles'","UnknownOption: 'feature.organizations:workflow-engine-process-activity'"],"root_cause_summaries":["Celery worker's `sync_options` task runs before feature flag registration, causing `UnknownOption` for valid database entries.","Option exists in database but not in `OptionsManager` registry, causing `UnknownOption` during `sync_options` task."],"transactions":["sentry.tasks.options.sync_options"],"title":"Unknown feature option keys during options sync","description":"Celery workers fail while syncing options due to unregistered feature flags (e.g., feature.organizations:workflow-engine-process-activity, feature.organizations:user-feedback-ai-titles) not present in the option registry.","tags":["Configuration","Deployment","Celery","Feature Flags","Unknown Option","Options Registry"],"cluster_size":2,"cluster_min_similarity":0.9549094107984253,"cluster_avg_similarity":0.9549094107984253},{"project_ids":["1"],"cluster_id":307,"group_ids":[6657937148,6710440710,6725861122,6727233680,6728222372,6737855390,6752394308,6754772488,6792981820,6793448323],"issue_titles":["ValueError: Not a valid response type: <!DOCTYPE html>","UnsupportedResponseType: text/html; charset=utf-8"],"root_cause_summaries":["GitLab API returned HTML with 200 OK for blame request, causing Sentry's response parser to raise a ValueError.","GitLab API returned HTML error page with 200 OK status, causing Sentry's response parser to raise a ValueError.","GitLab API returned HTML instead of expected JSON/XML for diffs, causing Sentry's parser to error.","GitLab API returns HTML with 200 OK instead of JSON, causing Sentry's JSON parsing to fail.","GitLab API returns HTML due to missing `Accept: application/json` header, causing JSON parsing failure.","GitLab API client used incorrect endpoint for PR diffs; GitLab returned HTML with 200 status, causing response parsing to fail.","GitLab API returned HTML with HTTP 200, not expected JSON, causing response parsing failure.","GitLab API returned HTML with 200 OK status instead of JSON, causing Sentry's client to fail JSON decoding and raise UnsupportedResponseType.","GitLab API returned HTML with 200 OK, causing Sentry's JSON parser to fail and raise UnsupportedResponseType.","GitLab instance returned HTML for API request, not JSON, due to misconfiguration, causing Sentry's API client to error."],"transactions":["sentry.tasks.process_commit_context","sentry.shared_integrations.response.base in from_response","sentry.integrations.source_code_management.tasks.open_pr_comment_workflow"],"title":"HTML responses break commit context and PR diff fetch","description":"Git provider API calls for commit blame and pull request diffs return HTML instead of JSON, causing JSON decoding and UnsupportedResponseType errors in the commit context workflow. This likely indicates an upstream HTML error page or auth/redirect response from the integration endpoint.","tags":["API","External System","Serialization","JSON Parsing Error","HTML Response","Commit Context","Pull Request Diffs"],"cluster_size":10,"cluster_min_similarity":0.9352192239749881,"cluster_avg_similarity":0.9577931287480608},{"project_ids":["11276"],"cluster_id":310,"group_ids":[6658944712,6694042021],"issue_titles":["Error: useOrganization called but organization is not set."],"root_cause_summaries":["Search component's `RouteSource` expects organization context, but `OrganizationContainer` renders children without it in specific error states.","Organization-dependent search component used on organization-agnostic page, causing `useOrganization` to fail when context is null."],"transactions":["/settings/account/security/","/settings/"],"title":"useOrganization called without organization context","description":"React components in organization and settings layouts invoke useOrganization when no organization is provided, causing ErrorBoundary crashes across routed pages. Likely missing or misordered OrganizationProvider/OrganizationContainer in the route hierarchy.","tags":["Configuration","Frontend","React","Context Provider Missing","useOrganization","Error Boundary"],"cluster_size":2,"cluster_min_similarity":0.9671046917193569,"cluster_avg_similarity":0.9671046917193569},{"project_ids":["1"],"cluster_id":315,"group_ids":[6660174344,6678082826],"issue_titles":["IndexError: list index out of range"],"root_cause_summaries":["Redis cluster pipeline execution returned an empty list, causing `IndexError` when accessing expected results.","FailoverRedis pipeline.execute() returns empty list during failover, causing IndexError when accessing results."],"transactions":["sentry.tasks.post_process.post_process_group"],"title":"IndexError in post-processing pipeline results","description":"Celery post-processing tasks assume Redis pipeline results exist, but empty pipeline_result lists cause IndexError when accessing the first element. This breaks base URL detection and plugin rate limiting during event post-processing.","tags":["Queueing","Caching","Configuration","Celery","Redis","IndexError","Rate Limiting"],"cluster_size":2,"cluster_min_similarity":0.9641974010013793,"cluster_avg_similarity":0.9641974010013793},{"project_ids":["1"],"cluster_id":316,"group_ids":[6660224810,6760869698],"issue_titles":["QuerySubscription.DoesNotExist: QuerySubscription matching query does not exist."],"root_cause_summaries":["Incident's subscription foreign key was not nullified upon QuerySubscription deletion, causing a dangling reference error.","Incident's foreign key to QuerySubscription points to a non-existent record, causing a `DoesNotExist` error upon access."],"transactions":["sentry.incidents.tasks.handle_trigger_action","/issues/alerts/rules/"],"title":"Alert action fails due to missing incident subscription","description":"Celery alert-resolution tasks crash when building MetricIssueContext because the incident lacks a subscription reference, leading to QuerySubscription.DoesNotExist and a KeyError('subscription'). This breaks email/alert delivery for affected rules.","tags":["API","Configuration","Queueing","Celery","Sentry","KeyError","QuerySubscription.DoesNotExist"],"cluster_size":2,"cluster_min_similarity":0.9723641681693002,"cluster_avg_similarity":0.9723641681693002},{"project_ids":["1"],"cluster_id":319,"group_ids":[6661166301,6741387244],"issue_titles":["ApiError: {\"message\":\"Validation Failed\",\"errors\":[{\"message\":\"The listed users and repositories cannot be searched either because the resources do not exist or you do not have permission to view them.\",\"resource\":\"Search\",\"field\":\"q\",\"code\":\"invalid\"}],\"document..."],"root_cause_summaries":["GitHub integration's repository search uses GitHub Search API with `user:` qualifier, which lacks necessary permissions, causing 422 validation errors.","`handle_search_issues` lacks specific `ApiError` handling for 422 status codes, causing unhandled exceptions when GitHub returns validation failures."],"transactions":["/api/0/organizations/{organization_id_or_slug}/integrations/{integration_id}/repos/","/extensions/github/search/{organization_id_or_slug}/{integration_id}/"],"title":"GitHub search queries rejected as invalid","description":"Queries sent to GitHubs Search API for repositories and issues are failing validation (422 Unprocessable Entity) due to invalid or unauthorized search parameters in the q field. This impacts repository listing and issue search flows in the GitHub integration.","tags":["External System","API","Input Validation","GitHub","HTTP 422","Unprocessable Entity"],"cluster_size":2,"cluster_min_similarity":0.9542597580794702,"cluster_avg_similarity":0.9542597580794702},{"project_ids":["1"],"cluster_id":321,"group_ids":[6661806142,6785437464],"issue_titles":["DataError: StringDataRightTruncation('value too long for type character varying(75)\\n')","DataError: value too long for type character varying(75)"],"root_cause_summaries":["The `GroupEmailThread.email` field's `max_length=75` is too short for incoming email addresses, causing `StringDataRightTruncation`.","Database `email` field (75 chars) too short for long programmatic email addresses."],"transactions":["sentry.tasks.activity.send_activity_notifications","sentry.utils.email.message_builder in build"],"title":"Email thread insert fails due to VARCHAR truncation","description":"Inserting GroupEmailThread records fails because email or msgid exceeds the VARCHAR length in the sentry_groupemailthread table, leading to DataError and subsequent DoesNotExist lookups during email notifications. This impacts digest/alert email sending paths.","tags":["Database","Data Integrity","API","PostgreSQL","String Truncation","Constraint Violation","Email Notifications"],"cluster_size":2,"cluster_min_similarity":0.9714985663935917,"cluster_avg_similarity":0.9714985663935917},{"project_ids":["1"],"cluster_id":326,"group_ids":[6662764500,6744316454],"issue_titles":["IntegrityError: insert or update on table \"sentry_releasecommit\" violates foreign key constraint \"release_id_refs_id_26c8c7a0\"","IntegrityError: ForeignKeyViolation('insert or update on table \"sentry_notificationmessage\" violates foreign key constraint \"sentry_notificationm_parent_notification__f7a9418e_fk_sentry_no\"\\nDETAIL:  Key (parent_notification_message_id)=(534177292) is not present in ta..."],"root_cause_summaries":["Race condition: parent notification deleted by cascading foreign key before child message insertion, causing foreign key violation.","Concurrent release deletion removes record while another process attempts to insert related data, violating foreign key constraint."],"transactions":["sentry.rules.processing.delayed_processing","/api/0/organizations/{organization_id_or_slug}/releases/{version}/"],"title":"Foreign key violations on release and notification inserts","description":"Writes to sentry_releasecommit and sentry_notificationmessage reference nonexistent parent rows, causing PostgreSQL foreign key violations. This likely stems from out-of-order writes or missing transactional guarantees around creating parent records before children in release commit and alert notification flows.","tags":["Database","Data Integrity","PostgreSQL","Foreign Key Violation","Sentry Release","Notifications"],"cluster_size":2,"cluster_min_similarity":0.958117492321526,"cluster_avg_similarity":0.958117492321526},{"project_ids":["11276"],"cluster_id":328,"group_ids":[6663176949,6742471020],"issue_titles":["Error: Unable to join team"],"root_cause_summaries":["User attempted to modify membership of IdP-provisioned teams; Sentry correctly denied the request with 403, but frontend showed generic error.","User attempted to join an IdP-provisioned team, which is forbidden by backend logic, resulting in a 403 error."],"transactions":["/settings/teams/","/settings/:orgId/teams/"],"title":"Client-side error when joining a team","description":"Frontend callbacks raise 'Unable to join team' in API/teams UI flows, indicating client-side handling failure during the join action. No server error details are surfaced, suggesting missing/failed API response handling.","tags":["API","Client-Side","Input Validation","JavaScript","Error Handling"],"cluster_size":2,"cluster_min_similarity":0.9813602314614549,"cluster_avg_similarity":0.9813602314614549},{"project_ids":["1"],"cluster_id":331,"group_ids":[6667014357,6672210447,6707727283,6737480353,6741533525,6749967324,6803879019,6808644708],"issue_titles":["Group.DoesNotExist: Group matching query does not exist."],"root_cause_summaries":["Snuba returned an error event with a null project ID, causing Django's Group.objects.get() to fail.","Trace view fails because an event references a Group ID that was deleted, causing a `Group.DoesNotExist` error during serialization.","Snuba returned event with `issue.id` for non-existent group, causing `Group.objects.get` to fail.","Snuba's event data contains issue IDs for Groups that no longer exist in the Sentry database, causing lookup failures.","Event in Snuba references a Group ID (6674410650) that does not exist in the Django database, causing a `Group.DoesNotExist` error.","Snuba trace data references group IDs absent from the PostgreSQL `sentry_groupedmessage` table, causing `Group.DoesNotExist`.","Snuba event data references a Group ID (6770103221) that does not exist in the main database, causing a `Group.DoesNotExist` error.","Event's group_id exists in Snuba, but corresponding Group object is missing from PostgreSQL, causing post-processing task failure."],"transactions":["/api/0/organizations/{organization_id_or_slug}/events/{project_id_or_slug}:{event_id}/","sentry.models.group in get_group_with_redirect","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/events/{event_id}/","/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/"],"title":"Group lookup fails: Group.DoesNotExist in event/trace views","description":"Multiple request and post-processing paths attempt to fetch Group by ID (and project) for events and traces, but the referenced group no longer exists, causing Group.DoesNotExist errors. Likely stale or redirected group IDs are not being handled, breaking serialization and detail endpoints.","tags":["Data Integrity","API","Django ORM","DoesNotExist","Event Processing","Trace Serialization"],"cluster_size":8,"cluster_min_similarity":0.9247409984447366,"cluster_avg_similarity":0.9554995165104889},{"project_ids":["1"],"cluster_id":333,"group_ids":[6667474706,6667564744,6722390281,6722681579],"issue_titles":["IntegrationFormError: {'customfield_10603': ['Product Name is required.']}","IntegrationFormError: {'issuetype': ['Issue type is required.']}"],"root_cause_summaries":["Jira issue creation failed because the Sentry alert rule configuration omitted a required Jira custom field.","Jira alert rule's action configuration lacks required 'issuetype' field, causing `IntegrationFormError` during ticket creation.","Jira alert rule configuration lacks required 'Product Name' field, causing API rejection due to missing data.","Jira rule lacked 'issuetype' in its configuration, causing `IntegrationFormError` during ticket creation."],"transactions":["sentry.shared_integrations.client.base in _request","sentry.tasks.post_process.post_process_group"],"title":"Jira issue creation fails due to missing required fields","description":"Post-processing rules fail when creating Jira issues because required fields like issuetype and Product Name (customfield_10603) are not provided, leading to 400 Bad Request and IntegrationFormError exceptions.","tags":["API","Input Validation","External System","Jira","Bad Request","Missing Required Field","IntegrationFormError"],"cluster_size":4,"cluster_min_similarity":0.941901342705091,"cluster_avg_similarity":0.9592381770977112},{"project_ids":["1"],"cluster_id":335,"group_ids":[6667666304,6673001592,6678366389,6678376628,6678376671,6692290323,6734206098],"issue_titles":["TimeoutError: Timeout reading from socket"],"root_cause_summaries":["Synchronous Redis-dependent error reporting for missing feature flags blocked user API request due to socket timeout.","Redis server unresponsive, causing socket read timeout during feature flag debounce operation.","Redis timeout during feature flag debouncing due to missing configurations and enabled debounce setting.","Redis `SET` operation timed out due to performance degradation, triggered by missing feature debouncing.","Missing feature configuration triggered Redis debounce, which timed out due to Redis infrastructure issues, failing event processing.","Missing `organizations:workflow-engine-ui-links` Flagpole configuration triggered Redis debounce, which timed out due to connectivity issues.","Missing Flagpole feature configurations trigger Redis debounce key set, which times out due to Redis server unresponsiveness."],"transactions":["/api/0/organizations/{organization_id_or_slug}/projects/","/api/0/organizations/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/","/api/0/organizations/{organization_id_or_slug}/issues/","sentry.tasks.activity.send_activity_notifications","sentry.tasks.post_process.post_process_group"],"title":"Feature-flag Redis timeouts across feature checks","description":"Multiple code paths that call features.has/batch_has are timing out on socket reads when setting a debounce key, indicating Redis latency or unavailability during feature-flag evaluation and notification debounce. This impacts notifications, search, and organization/project serialization flows.","tags":["Caching","External System","API","Redis","Timeout","Feature Flags"],"cluster_size":7,"cluster_min_similarity":0.929658072665388,"cluster_avg_similarity":0.9600982778140984},{"project_ids":["1"],"cluster_id":340,"group_ids":[6671041667,6705562166,6721720817,6760278209,6763136769],"issue_titles":["TimeoutError"],"root_cause_summaries":["Issue summary generation's 5-second timeout is too short, due to 4.5-second lock wait and subsequent external Seer API calls.","Slack integration's 5-second AI summary timeout is exceeded by LLM's ~5-second response time combined with retry mechanism.","Seer AI service exceeded 5-second summary generation timeout, causing Sentry's notification system to log error and omit summary.","Lock contention and Seer API latency cause `get_issue_summary` to exceed `fetch_issue_summary`'s 5-second timeout.","Issue summary generation exceeds 5s timeout due to slow Seer AI Platform responses and cumulative processing time."],"transactions":["sentry.rules.processing.delayed_processing","sentry.tasks.activity.send_activity_notifications","sentry.tasks.digests.deliver_digest","/extensions/slack/action/"],"title":"Issue summary fetch times out in Slack notifications","description":"Fetching issue summaries for alert/digest notifications and Slack action handlers is timing out, causing Celery worker tasks and Slack responses to fail. The delay occurs while awaiting future.result() in issue_summary_for_alerts, impacting alerting, activity notifications, and digests.","tags":["API","Queueing","Timeout","Celery","Slack"],"cluster_size":5,"cluster_min_similarity":0.9281566601567478,"cluster_avg_similarity":0.9458389083432639},{"project_ids":["1"],"cluster_id":343,"group_ids":[6672001636,6744362113],"issue_titles":["UnsupportedFrameInfo: This path is not supported."],"root_cause_summaries":["PathBasedFrameInfo rejected bare filename 'test_sentry.py' because it lacked path separators, ignoring available abs_path.","`PathBasedFrameInfo` discards `abs_path` context, validating only `filename`, causing unsupported path error."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/repo-path-parsing/","/api/0/organizations/{organization_id_or_slug}/derive-code-mappings/"],"title":"UnsupportedFrameInfo raised during frame parsing endpoints","description":"Frame parsing in derived code mappings and repo path parsing endpoints rejects certain paths and raises UnsupportedFrameInfo, indicating unsupported frame formats for the provided platform parameters.","tags":["API","Input Validation","Configuration","Sentry","Unsupported Frame","Derived Code Mappings","Repo Path Parsing"],"cluster_size":2,"cluster_min_similarity":0.9597667602362767,"cluster_avg_similarity":0.9597667602362767},{"project_ids":["11276"],"cluster_id":348,"group_ids":[6672199299,6686123033],"issue_titles":["TypeError: Failed to fetch (api2.amplitude.com)","TypeError: Failed to fetch (singleview.site)"],"root_cause_summaries":["Sentry's frontend SDK captured a network `TypeError: Failed to fetch` from an external analytics service (`singleview.site`), reporting it to Sentry.","Client browser failed to fetch api2.amplitude.com, likely due to ad blocker or network restriction, not Sentry code."],"transactions":["/issues/:groupId/events/:eventId/","/issues/"],"title":"Browser fetch failures sending Amplitude events","description":"Client-side analytics flush attempts to Amplitude fail with TypeError: Failed to fetch, indicating network or CORS issues during fetch() calls from the browser.","tags":["Networking","API","External System","Amplitude","Fetch","CORS","Failed To Fetch"],"cluster_size":2,"cluster_min_similarity":0.9637490127471376,"cluster_avg_similarity":0.9637490127471376},{"project_ids":["1"],"cluster_id":350,"group_ids":[6672345079,6786853811,6797692325],"issue_titles":["AvataxException: GetTaxError: 'Tax calculation cannot be determined. Zip is not valid for the state.'"],"root_cause_summaries":["Inconsistent billing address and credit card location data creates invalid zip/state combination for Avatax API.","Avatax API rejected transaction due to customer's billing address having an invalid ZIP code for the specified state.","Avalara rejected tax calculation due to postal code/state mismatch in customer's billing address."],"transactions":["/api/0/customers/{organization_id_or_slug}/subscription/","getsentry.tasks.create_invoices.process_subscription"],"title":"AvaTax rejects tax calc due to invalid state ZIP","description":"Tax transaction creation fails when billing attempts to compute sales tax via Avalara AvaTax because the provided ZIP code does not match the customer's state, blocking invoice creation in both API and background workers.","tags":["External System","API","Input Validation","Avalara AvaTax","Validation Error"],"cluster_size":3,"cluster_min_similarity":0.9606393170213284,"cluster_avg_similarity":0.9683495535545116},{"project_ids":["6178942"],"cluster_id":352,"group_ids":[6672400992,6672853227,6679641465,6723785712],"issue_titles":["MaxIterationsReachedException: Agent Code reached maximum iterations without finishing.","MaxIterationsReachedException: Agent Drafter reached maximum iterations without finishing.","MaxIterationsReachedException: Agent Solution reached maximum iterations without finishing."],"root_cause_summaries":["Agent's exhaustive bug search for simple change exceeds iteration limit, lacking natural stopping condition.","Agent's prompt emphasizes analysis, not explicit `add_tasks` tool call for solution delivery, causing iteration limit exhaustion.","Agent's open-ended prompt prevents natural termination, causing iteration limit exception.","Coding agent failed to complete tasks within 100 iterations, due to `task_tools.are_tasks_complete()` never returning true."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task","seer.automation.codegen.bug_prediction_step.bug_prediction_task"],"title":"Agent loops hit max-iteration limit across pipeline","description":"Multiple agents (Solution, Drafter, Code) in the RootCause, BugPrediction, and Coding steps are exceeding the configured iteration cap and aborting. This likely indicates missing convergence criteria or overly strict iteration limits causing agents to loop without completion.","tags":["Configuration","Workflow Orchestration","Resource Limits","Iteration Limit","Agent Framework","RootCauseStep","BugPredictionComponent"],"cluster_size":4,"cluster_min_similarity":0.9484577980308838,"cluster_avg_similarity":0.9604281690786728},{"project_ids":["1"],"cluster_id":354,"group_ids":[6672729054,6682733538],"issue_titles":["HTTPError: 502 Server Error: Bad Gateway for url: https://github-deployment-gate.sentry.io/api/sentry/webhook"],"root_cause_summaries":["Deprecated GitHub Deployment Gates integration's webhook URL `github-deployment-gate.sentry.io` returned a 502, indicating its backend service is unavailable.","External webhook service returned 502, causing `HTTPError` re-raised due to Sentry App's published status after retries."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"],"title":"Webhook deliveries return 502 Bad Gateway","description":"Celery workers fail when sending Sentry app resource change webhooks because the upstream endpoint responds with 502, causing response.raise_for_status() to raise HTTPError.","tags":["External System","API","Queueing","Upstream Unavailable","HTTP 502","Celery","Webhooks"],"cluster_size":2,"cluster_min_similarity":0.970784727043355,"cluster_avg_similarity":0.970784727043355},{"project_ids":["1"],"cluster_id":355,"group_ids":[6672740503,6672804708],"issue_titles":["RequestTimeout: Proxied request timed out"],"root_cause_summaries":["Regional RPC service `sentry-rpc-de.psc.control.sentry.internal:8999` connection timed out, preventing proxied request completion.","Internal RPC service connection to `sentry-rpc-de.psc.control.sentry.internal:8999` timed out at the TCP level."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/keys/","/api/embed/error-page/"],"title":"Regional proxy requests to sentry-rpc-de timeout","description":"Requests proxied by the control service to the DE region (sentry-rpc-de) are timing out, leading to max-retry errors when fetching project keys and error-embed pages. Impact: user-facing endpoints via the regional proxy fail due to upstream connection timeouts.","tags":["Networking","External System","API","Timeout","Retries Exhausted","urllib3","Sentry RPC DE"],"cluster_size":2,"cluster_min_similarity":0.975197365097389,"cluster_avg_similarity":0.975197365097389},{"project_ids":["1"],"cluster_id":357,"group_ids":[6672753331,6672888329,6673100184,6673127010,6675959289,6678376554,6690658978,6698196149,6722503477,6726057593,6808135193],"issue_titles":["ReadTimeout: SafeHTTPSConnectionPool(host='api.foam.ai', port=443): Read timed out. (read timeout=2.0)","ReadTimeout: SafeHTTPSConnectionPool(host='api.linear.app', port=443): Read timed out. (read timeout=2.0)","ReadTimeout: SafeHTTPSConnectionPool(host='github-deployment-gate.sentry.io', port=443): Read timed out. (read timeout=2.0)","ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))","ReadTimeout: SafeHTTPSConnectionPool(host='app.komodor.com', port=443): Read timed out. (read timeout=2.0)","ReadTimeout: SafeHTTPSConnectionPool(host='app.incident.io', port=443): Read timed out. (read timeout=2.0)","ReadTimeout: SafeHTTPSConnectionPool(host='applications.zoom.us', port=443): Read timed out. (read timeout=5)","ReadTimeout: SafeHTTPSConnectionPool(host='app.launchdarkly.com', port=443): Read timed out. (read timeout=2.0)","ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))","ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"],"root_cause_summaries":["Webhook's 2-second timeout caused SSL handshake failure to LaunchDarkly, preventing successful delivery.","Webhook write operation timed out due to large payload and insufficient 2-second timeout.","Linear API webhook endpoint failed to respond within the 2-second read timeout, causing the `ReadTimeout` error.","Webhook timeout of 2 seconds is too aggressive for external API calls, causing false failures due to network latency or external service processing.","Zoom webhook endpoint unresponsive within 5s timeout, causing read timeout. Sentry's task retries, but Zoom's latency persists.","External webhook endpoint at `app.incident.io` failed to respond within the aggressive 2-second timeout, causing a `ReadTimeout`.","Webhook timeout (2.0s) is insufficient for external AI service's processing, causing recurring ReadTimeout errors.","Linear API prematurely closes connection during webhook send, causing Sentry's 2-second timeout to be exceeded.","Deprecated external service `github-deployment-gate.sentry.io` times out within 2 seconds, causing webhook delivery failure.","External webhook endpoint unresponsiveness caused repeated connection timeouts, exceeding the task's processing deadline.","Komodor's webhook endpoint failed to respond within Sentry's 2-second read timeout, causing the `ReadTimeout` error."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.workflow_notification","sentry.sentry_apps.tasks.sentry_apps.build_comment_webhook","sentry.sentry_apps.tasks.service_hooks.process_service_hook","sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook","sentry.net.http in request","sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2"],"title":"Outbound webhook requests time out in Celery workers","description":"Celery tasks sending Sentry App webhooks (resource change, alert, comment) via safe_urlopen are timing out on HTTP reads and occasional TLS handshakes, indicating slow or unresponsive third-party webhook endpoints. This causes failed deliveries and retried jobs for outbound integrations.","tags":["Networking","External System","Queueing","API","Timeout","TLS Handshake Failure","Celery","Webhooks"],"cluster_size":11,"cluster_min_similarity":0.9306968809342607,"cluster_avg_similarity":0.9519651911212818},{"project_ids":["1"],"cluster_id":364,"group_ids":[6673346181,6673375724,6723571938],"issue_titles":["ApiError: {\"message\":\"403 Forbidden\"}","IntegrationError: Error Communicating with GitLab (HTTP 403): 403 Forbidden"],"root_cause_summaries":["GitLab user lacks project permissions to create issues, causing 403 Forbidden.","GitLab user account lacks Maintainer role for project 48161171, preventing webhook creation despite valid OAuth scope.","GitLab integration token lacks sufficient permissions to create project webhooks, causing a 403 Forbidden error."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/integrations/{integration_id}/","/api/0/organizations/{organization_id_or_slug}/repos/"],"title":"GitLab integration returns 403 on webhook/issue creation","description":"Calls to GitLab APIs for creating project webhooks and issues are failing with 403 Forbidden, indicating missing permissions or invalid access token scopes in the integration.","tags":["External System","API","Authorization","GitLab","403 Forbidden"],"cluster_size":3,"cluster_min_similarity":0.9535286877857385,"cluster_avg_similarity":0.9651131511951613},{"project_ids":["1"],"cluster_id":368,"group_ids":[6674683667,6686993636,6722209887,6722459118],"issue_titles":["SubscriptionChangeIntegrityError: An unreconciled change already exists - refusing to create invoice"],"root_cause_summaries":["Unapplied SubscriptionChange record persists due to silently handled database error, blocking subsequent attempts via unique constraint.","Prior `apply_subscription_change` failure left an unapplied record, violating unique constraint on subsequent attempts.","Subscription cancellation retries fail due to existing unapplied change, caused by prior partial execution.","Task retry attempts to create duplicate unapplied subscription change, violating unique constraint from previous failed attempt."],"transactions":["sentry.deletions.tasks.run_deletion","getsentry.models.subscriptionchange in capture"],"title":"Duplicate SubscriptionChange blocks invoice creation","description":"Creating an invoice during subscription cancellation attempts to insert a new SubscriptionChange, but a unique constraint on unapplied changes per subscription is violated. This suggests concurrent or unreconciled changes exist for the same subscription, preventing invoice creation.","tags":["Database","Concurrency","Data Integrity","PostgreSQL","Constraint Violation","Unique Constraint","Billing"],"cluster_size":4,"cluster_min_similarity":0.960921717584894,"cluster_avg_similarity":0.972062866923984},{"project_ids":["1"],"cluster_id":369,"group_ids":[6674861587,6696916851,6764552419,6772660126],"issue_titles":["ApiError: HTTP 400 (invalid_request): The authorization grant has expired.","IdentityNotValid: HTTP 400 (invalid_grant): The provided authorization grant failed verification","IdentityNotValid"],"root_cause_summaries":["VSTS integration uses deprecated OAuth flow with expired refresh tokens, causing 400 Bad Request and IdentityNotValid errors.","Legacy Azure DevOps integration's JWT client assertion expired, causing token refresh failure due to unmigrated authentication flow.","VSTS integration's `integration_migration_version` is outdated, causing new Azure AD credentials to be used with the incompatible legacy OAuth refresh flow.","Unmigrated VSTS integration uses deprecated JWT assertion flow with Microsoft's updated OAuth endpoint, causing `invalid_grant`."],"transactions":["/api/0/internal/integration-proxy/","sentry.integrations.vsts.tasks.vsts_subscription_check"],"title":"OAuth refresh fails: expired/invalid grant","description":"OAuth2 token refresh attempts are failing with invalid_grant and bad request responses, causing authorization to expire when calling the integration proxy and VSTS subscription APIs.","tags":["Authentication","API","External System","OAuth2","Invalid Grant","Token Refresh","Bad Request"],"cluster_size":4,"cluster_min_similarity":0.9368156377181965,"cluster_avg_similarity":0.9601095431963049},{"project_ids":["1"],"cluster_id":372,"group_ids":[6675288098,6676778400,6685379014,6710454609],"issue_titles":["DecodeError: Error parsing message"],"root_cause_summaries":["Recursive protobuf conversion failed on deeply nested, inconsistently formatted span data containing Python `repr()` strings.","Protobuf serialization fails due to inconsistent `root_styles` data type, causing schema mismatch.","Recursive protobuf conversion of deeply nested Python dictionaries and lists, containing large strings, exceeded protobuf message size/depth limits.","Recursive protobuf conversion of complex, inconsistently-typed span data failed due to string-serialized JSON, causing deserialization errors."],"transactions":["sentry.spans.consumers.process_segments.convert in _anyvalue"],"title":"DecodeError while serializing span attributes","description":"Span-to-item conversion recurses through nested attributes in convert.py, causing DecodeError during message parsing in the multiprocessing worker. The serialization of mixed dict/list values via _anyvalue appears to loop deeply and fails to produce a valid message.","tags":["Serialization","API","Python","DecodeError","Multiprocessing","Sentry SDK"],"cluster_size":4,"cluster_min_similarity":0.9615498291878637,"cluster_avg_similarity":0.9713887533751211},{"project_ids":["1"],"cluster_id":376,"group_ids":[6675823037,6691590835,6741288045,6769418170,6770714621,6770714623,6772406272,6772406285],"issue_titles":["InternalServerError: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: {","GatewayTimeout: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: {","ServiceUnavailable: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: {"],"root_cause_summaries":["Profile storage fails due to GCS 500, as `GoogleCloudStorage` lacks `InvalidResponse` retry logic.","GCS 503s, Sentry's GCS client retries without backoff, exhausting task deadline, causing `ProcessingDeadlineExceeded`.","Insufficient GCS retry strategy with no backoff or jitter failed to handle transient 503s, leading to 504 timeout.","Aggressive GCS retry policy without backoff amplifies load, causing cascading timeouts during high-volume profile processing.","Google Cloud Storage returned persistent 503 errors during profile upload, exhausting Sentry's retry mechanism.","Google Cloud Storage consistently returns 500 Internal Server Error during profile chunk multipart upload, exhausting Sentry's retries.","GCS 503 errors cause `try_repeated` to exhaust all retries without backoff, leading to task failure.","Google Cloud Storage service degradation caused repeated 503/504 errors, exhausting Sentry's 5 GCS retries, leading to profile upload failure."],"transactions":["sentry.profiles.task.process_profile"],"title":"GCS uploads return non-200 during profile processing","description":"Celery tasks saving VroomRS profiles and chunks to Google Cloud Storage are failing when upload_from_file returns unexpected HTTP statuses (e.g., 502/503/504), causing InvalidResponse errors and task failures.","tags":["External System","API","Queueing","Google Cloud Storage","Invalid Response","Upstream Unavailable"],"cluster_size":8,"cluster_min_similarity":0.9150710225025137,"cluster_avg_similarity":0.9488412082388554},{"project_ids":["11276"],"cluster_id":377,"group_ids":[6675844962,6675865040],"issue_titles":["InternalServerError: GET /organizations/{orgSlug}/issues/{issueId}/ 500"],"root_cause_summaries":["User service RPC unavailability caused issue details endpoint to fail, as it's a critical dependency.","Control silo's user service repeatedly returned 503 errors, exhausting RPC client retries and causing frontend 500s."],"transactions":["/issues/:groupId/"],"title":"500 on issue details API for organization issues","description":"Frontend queries to GET /organizations/{orgSlug}/issues/{issueId}/ consistently return InternalServerError, causing React Query prefetch/refetch to fail. The server-side endpoint likely errors while fetching issue details, breaking issue view updates.","tags":["API","External System","Upstream Unavailable","HTTP 500","React Query","GET /organizations/{orgSlug}/issues/{issueId}/"],"cluster_size":2,"cluster_min_similarity":0.9721048005795453,"cluster_avg_similarity":0.9721048005795453},{"project_ids":["1"],"cluster_id":381,"group_ids":[6677440517,6794413207],"issue_titles":["NotificationClassNotSetException"],"root_cause_summaries":["PlanUpgradeRequestNotification class is undefined/unregistered, causing NotificationClassManager to fail retrieval in task worker.","Notification class 'PlanUpgradeRequestNotification' not found in manager, causing exception due to missing class definition."],"transactions":["/issues/","/disabled-member/"],"title":"Notification class not configured in task worker","description":"Background task _send_notification fails because the requested notification class name cannot be resolved by class_manager.get, causing NotificationClassNotSetException in multiprocessing workers.","tags":["Configuration","Queueing","Multiprocessing","NotificationClassNotSetException"],"cluster_size":2,"cluster_min_similarity":0.9659551750539326,"cluster_avg_similarity":0.9659551750539326},{"project_ids":["1"],"cluster_id":386,"group_ids":[6678376681,6789208105,6789208144,6789208763,6789208834],"issue_titles":["MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /fc/4489/756157429d8046005d95041f32 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7c9af0284950>, 'Connection to 192.168.208.181 timed o...","MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /eventattachments/v1/36/499f/a149714c318dc3366902e3f6fe (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7973942f1490>, 'Connection to 192...","MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /eventattachments/v1/15/269a/4e2b08453c99a6368820aef5fa (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fa9fc516f90>, 'Connection to 192...","MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /16/5f4e/2eb118442da7328319e6a17cfb (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7da9c502f530>, 'Connection to 192.168.208.181 timed o...","MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /86/c154/faa0aa4ba6b82b763521842f76 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7874080fd130>, 'Connection to 192.168.208.181 timed o..."],"root_cause_summaries":["Filestore service at resolved IP 192.168.208.181:8080 is unreachable, causing connection timeouts and MaxRetryError.","Filestore service unreachable; DNS resolved to an unresponsive IP, causing connection timeouts.","Filestore service at 192.168.208.181:8080 is unreachable, causing connection timeouts during chunk upload operations.","Filestore endpoint misconfiguration via `SENTRY_FILESTORE_OPTIONS` caused connection timeouts to an unreachable internal service.","Filestore service at 192.168.208.181:8080 was unreachable, causing connection timeouts and MaxRetryError during attachment upload."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/files/dsyms/","ingest_consumer.process_event","/api/0/organizations/{organization_id_or_slug}/chunk-upload/","/organization-avatar/{avatar_id}/"],"title":"HTTP timeouts to filestore during blob access","description":"Requests to the filestore over HTTP are timing out via urllib3, causing retries to be exhausted when uploading or fetching chunked blobs and event attachments in Sentry services. Impact includes failed attachment saves and debug file/image retrieval due to connect timeouts to the storage backend.","tags":["Networking","Disk/Storage","External System","API","Timeout","Retries Exhausted","urllib3","Filestore"],"cluster_size":5,"cluster_min_similarity":0.9367410144546908,"cluster_avg_similarity":0.9511154407817977},{"project_ids":["1"],"cluster_id":389,"group_ids":[6678537737,6689961415,6689961427,6689961447,6689961448,6722355635,6722355659,6722361289,6722361336,6722394900,6722394949,6746480180,6746480185,6747529022,6748982357,6767665376,6772497218],"issue_titles":["SubscriptionError: transaction.duration is not a tag in the metrics dataset","IncompatibleMetricsQuery: geo.region is not a tag in the metrics dataset","SubscriptionError: se is not a tag in the metrics dataset","SubscriptionError: http.url is not a tag in the metrics dataset","IncompatibleMetricsQuery: SITE_DOMAIN is not a tag in the metrics dataset","SubscriptionError: customerType is not a tag in the metrics dataset"],"root_cause_summaries":["Enabled feature flag forces custom tag validation against static default list, preventing dynamic resolution and causing query failure.","Alert rule with custom tag `SITE_DOMAIN` fails validation when `organizations:mep-use-default-tags` feature flag is enabled, restricting tags to a predefined set.","Query tag 'se' is not a default metric tag, violating `mep-use-default-tags` feature flag validation.","Enabled `organizations:mep-use-default-tags` feature flag restricts metric tag resolution to only default tags, rejecting 'se'.","Legacy alert rule uses 'SITE_DOMAIN' tag, but `organizations:mep-use-default-tags` feature flag restricts it, causing query failure.","Query tag 'se' is not in `DEFAULT_METRIC_TAGS` due to enabled `mep-use-default-tags` feature flag, causing `IncompatibleMetricsQuery`.","Querying with `customerType` tag fails because `organizations:mep-use-default-tags` feature flag restricts tags to a predefined whitelist.","`customerType` not in `DEFAULT_METRIC_TAGS` when `organizations:mep-use-default-tags` feature flag enabled, causing query rejection.","Enabled `organizations:mep-use-default-tags` feature flag incorrectly restricts tag validation to `DEFAULT_METRIC_TAGS`, rejecting valid custom tags like `SITE_DOMAIN`.","Feature flag `organizations:mep-use-default-tags` enabled, blocking custom tags not in `DEFAULT_METRIC_TAGS`.","Existing subscription's custom tag is incompatible with new feature flag restricting metrics queries to default tags.","MetricsQueryBuilder misclassifies metrics as tags, failing strict tag validation when `mep-use-default-tags` is enabled.","Alert query uses high-cardinality tag `http.url`, disallowed by `organizations:mep-use-default-tags` feature flag for metrics dataset.","Alert rule's stored query uses `geo.region` and `url` tags, now incompatible with metrics dataset's stricter schema due to `organizations:mep-use-default-tags`.","Feature flag `mep-use-default-tags` restricts metrics queries to `DEFAULT_METRIC_TAGS`, excluding `http.url` and causing `SubscriptionError`.","Enabled feature flag `organizations:mep-use-default-tags` restricts metric tag validation, causing existing subscriptions with non-default tags to fail.","Metrics query fails because `organizations:mep-use-default-tags` enabled, but `DEFAULT_METRIC_TAGS` whitelist is incomplete."],"transactions":["sentry.snuba.tasks in _create_in_snuba","/extensions/slack/event/","sentry.search.events.builder.metrics in resolve_tag_key","query_subscription_consumer_process_message"],"title":"Snuba metrics subscriptions reject invalid tags","description":"Alert subscription creation fails when queries reference tags not present in the metrics dataset (e.g., se, http.url, transaction.duration). The Snuba metrics query builder raises IncompatibleMetricsQuery, causing SubscriptionError in the alerting pipeline.","tags":["API","Configuration","Data Integrity","Sentry","Snuba","IncompatibleMetricsQuery","Invalid Tag"],"cluster_size":17,"cluster_min_similarity":0.9320357181509313,"cluster_avg_similarity":0.959927398986032},{"project_ids":["1"],"cluster_id":390,"group_ids":[6678537740,6678570727,6722355701,6722361344,6722427546,6722427559,6722458963,6746480184,6793537779,6798177476],"issue_titles":["SubscriptionError: Environment: PRD was not found","SubscriptionError: Environment: Production was not found","SubscriptionError: release value 22.8.2 in filter not found","SubscriptionError: release value com.example.vu.android@2.10.4+43 in filter not found","SubscriptionError: release value application.monitoring.javascript@22.5.5 in filter not found","SubscriptionError: release value application.monitoring.javascript@22.2.1 in filter not found"],"root_cause_summaries":["Environment 'Production' not indexed for 'sessions' use case, causing metrics query failure.","Release value not indexed for sessions use case, causing query builder to fail during alert subscription creation.","Release value not found in metrics indexer, preventing query construction due to missing internal ID.","Alert creation fails because release value is not yet indexed in metrics system.","Metrics indexer lacks `release` tag values, causing query builder to fail lookup and raise `IncompatibleMetricsQuery`.","Metrics string indexer lacks 'Production' environment entry for organization and 'sessions' use case.","Metrics indexer lacks release value for sessions use case, causing query validation failure.","Environment 'PRD' not found in metrics indexer for sessions use case, preventing subscription creation.","Environment 'PRD' not indexed in `sentry_stringindexer` for organization 4509056224198736, preventing resolution for sessions metrics.","Release value '22.8.2' not found in metrics indexer, causing query validation to fail."],"transactions":["sentry.snuba.tasks in _create_in_snuba"],"title":"Snuba alert subscriptions reject invalid filters","description":"Creating alert subscriptions fails when metrics queries reference unknown environments (e.g., Production/PRD) or non-existent release values, causing IncompatibleMetricsQuery errors wrapped as SubscriptionError in the Snuba query builder.","tags":["API","Configuration","Data Integrity","Snuba","Incompatible Metrics Query","Invalid Environment Filter","Invalid Release Filter"],"cluster_size":10,"cluster_min_similarity":0.9455399511827215,"cluster_avg_similarity":0.9624088169556853},{"project_ids":["1"],"cluster_id":391,"group_ids":[6678570733,6678637607,6722355684,6722427590],"issue_titles":["SubscriptionError: Metric: c:custom/checkout.failed@none could not be resolved","SubscriptionError: transaction.duration is not a tag in the metrics dataset"],"root_cause_summaries":["Alert query's 90-day custom metric discovery window misses metric data, causing resolution failure.","Metric `c:custom/checkout.failed@none` cannot be resolved because `is_custom_measurement` excludes `custom` namespace metrics from resolution.","Alert subscriptions use legacy query path, misinterpreting metric 'transaction.duration' as a tag, causing query failure.","Metric resolution fails because 'custom' namespace metrics are not recognized as 'custom measurements'."],"transactions":["sentry.snuba.tasks in _create_in_snuba"],"title":"Snuba alert subscriptions reject unresolved metrics","description":"Creating alert subscriptions fails in Snuba when queries reference unknown metrics (e.g., c:<email>) or invalid tags (transaction.duration not in metrics). This causes SubscriptionError wrapping IncompatibleMetricsQuery during query building in AlertMetricsQueryBuilder.","tags":["API","Configuration","Data Integrity","Sentry","Snuba","Incompatible Metrics Query","Invalid Tag"],"cluster_size":4,"cluster_min_similarity":0.9480956848975538,"cluster_avg_similarity":0.9576516805387844},{"project_ids":["1"],"cluster_id":398,"group_ids":[6679082964,6722334483,6722366782,6723594400,6723611056,6723639579,6725628767],"issue_titles":["ApiInvalidRequestError: {\"error\":\"file_path should be a valid file path\"}","ApiInvalidRequestError","ApiInvalidRequestError: Potential path traversal attempt detected. Feedback issue: https://gitlab.com/gitlab-org/gitlab/-/issues/520714."],"root_cause_summaries":["Code mapping path transformation fails to normalize relative path components, leading to GitLab API rejecting requests due to path traversal detection.","Empty `stack_root` in code mapping causes `replace` to insert `source_root` with leading slash, creating double slash in path, rejected by GitLab API.","Empty code mapping roots cause cached file paths to be sent to GitLab API, resulting in invalid requests.","Code mapping incorrectly translates compiled asset paths to non-existent source file paths in GitLab, causing 400 Bad Request.","GitLab API returned 400 for a non-existent cache file, causing `ApiInvalidRequestError` as 400s are not explicitly handled.","Path sanitization failed when `stack_root` was empty, allowing traversal sequences to reach the GitLab API.","Event frames contain relative paths; insufficient validation allows them to be URL-encoded and rejected by GitLab API."],"transactions":["sentry.shared_integrations.client.base in _request","sentry.tasks.process_commit_context"],"title":"Commit context lookup rejects invalid file paths","description":"Requests to the commit context service fail with 400 Bad Request due to invalid or unsafe file_path values (path traversal detected or not a valid path) when fetching file blame data. This breaks commit blame enrichment during process_commit_context tasks.","tags":["API","Input Validation","Configuration","HTTP 400","Path Traversal","Commit Context"],"cluster_size":7,"cluster_min_similarity":0.9184001921002385,"cluster_avg_similarity":0.9446033022033143},{"project_ids":["1"],"cluster_id":399,"group_ids":[6679809889,6709183081],"issue_titles":["ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"2314a56b02b64b7995a5b8e8f118181c\"}","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"c28ccbe6b9bb49b9b29bd327d1010496\"}"],"root_cause_summaries":["GitHub App installation suspension caused internal proxy 500, leading to `ApiError` during comment reaction fetching.","GitHub App installation suspended, causing proxy to return generic 500, masking true error."],"transactions":["sentry.integrations.github.tasks.github_comment_reactions","sentry.tasks.process_commit_context"],"title":"GitHub integration calls return 500 from upstream API","description":"Celery tasks calling the GitHub client for PR comment reactions and commit blame are receiving 500 Internal Server Error responses from the upstream GitHub API, causing ApiError and failure recording in metrics. Impacted features include PR comment reaction fetches and commit context/blame retrieval.","tags":["External System","API","Queueing","Upstream Unavailable","HTTP 5xx","GitHub","Celery"],"cluster_size":2,"cluster_min_similarity":0.9584317974781988,"cluster_avg_similarity":0.9584317974781988},{"project_ids":["1"],"cluster_id":400,"group_ids":[6680430193,6712113522,6719039882,6722916958],"issue_titles":["RestrictedIPAddress: (localhost/127.0.0.1) matches the URL blocklist","RestrictedIPAddress: (entpkddoi9b5h.x.pipedream.net/198.51.100.1) matches the URL blocklist","RestrictedIPAddress: (172.24.61.151) matches the URL blocklist","RestrictedIPAddress: (172.31.60.38) matches the URL blocklist"],"root_cause_summaries":["Sentry App webhook URL configured with internal IP, triggering SSRF protection and blocking outbound connection.","Webhook URL resolves to `198.51.100.1`, a reserved IP in Sentry's blocklist, triggering `RestrictedIPAddress` exception.","Sentry App's webhook URL configured to localhost, triggering security blocklist and `RestrictedIPAddress` exception.","Sentry App webhook URL points to a private IP, blocked by Sentry's security configuration."],"transactions":["/api/0/internal/rpc/{service_name}/{method_name}/","sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook","sentry.sentry_apps.tasks.sentry_apps.workflow_notification"],"title":"Webhooks blocked by URL/IP allowlist restrictions","description":"Sentry app webhook deliveries are failing because destination hosts match the URL/IP blocklist enforced in safe_urlopen/socket safety checks, preventing outbound connections from Celery workers. This blocks workflow and resource change notifications to restricted endpoints.","tags":["Networking","Configuration","API","Webhooks","URL Blocklist","Celery"],"cluster_size":5,"cluster_min_similarity":0.9516436891610359,"cluster_avg_similarity":0.9635242849851743},{"project_ids":["1"],"cluster_id":765,"group_ids":[6680507283,6760540168,6760540173,6760626939,6792074619,6792680724,6793035306,6794060998,6794061875,6794063158,6794064075,6794066288,6794096477,6794096592,6794097572,6794097578,6794098072,6794099519,6794111951],"issue_titles":["ConnectionError: Error 104 while writing to socket. Connection reset by peer.","ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')"],"root_cause_summaries":["Redis `HGETALL` on large buffer hash keys causes socket read timeouts, leading to connection resets.","Redis server terminates connections during high-volume buffer writes, indicating resource exhaustion or aggressive timeouts.","Redis server at 192.168.208.40:11485 is unstable, causing connection resets and refusals during span buffer operations.","Redis cluster or network instability causes connection resets during `DECRBY` command execution.","Redis server abruptly closed connection during active read, likely due to resource exhaustion, aggressive timeout, or network instability.","Network instability or Redis server issues caused connections to be reset, leading to Redis operation failures and broader API connectivity problems.","Redis infrastructure instability causes connection resets; long-lived worker processes attempt to reuse stale connections, leading to errors.","Long-running taskworker's persistent Redis connection was reset by peer, causing `RedisBuffer.incr` to fail without retry.","Redis client library buffer corruption during response parsing causes connection reset by peer.","Redis server forcibly closed connection during socket read, indicating server instability or network issues.","Redis server instability or overload causes abrupt connection resets during pipelined cleanup operations, leading to client-side ConnectionErrors.","No summary found","Idle Redis connections terminated by network/server due to mismatched timeouts, causing client read failures.","Redis server OOM prevention causes connection resets; application lacks resilience, failing tasks without retries.","Redis server instability or network issues cause connections to reset, leading to task failures across Sentry's event processing and notification systems.","Redis server at 192.168.208.40:11499 is unavailable or unstable, causing connection refusals and resets during span buffer cleanup.","Redis connection pool reuses stale connections, causing socket read failures due to server-side idle timeouts.","Taskworker's Redis `hgetall` on oversized buffer hashes exceeds socket timeout or causes connection resets, failing data retrieval.","Redis memory exhaustion caused connection resets during socket reads, leading to `ConnectionError`."],"transactions":["sentry.spans.buffer in done_flush_segments","sentry.buffer.redis in _execute_redis_operation","sentry.buffer.redis in get","sentry.tasks.store.save_event","ingest_consumer.process_event","sentry.buffer.redis in _process_single_incr","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/"],"title":"Redis connections reset during event buffering","description":"Workers saving events and flushers executing pipelines hit 'Connection reset by peer' when issuing Redis pipeline execute calls, causing grouping counters to fail. Likely a Redis-side drop or network interruption affecting Celery/Arroyo processes.","tags":["Networking","Caching","Queueing / Messaging","Redis","Connection Reset","Pipeline Execute","Celery/Arroyo Workers"],"cluster_size":20,"cluster_min_similarity":0.908657967823947,"cluster_avg_similarity":0.9440239154935962},{"project_ids":["1"],"cluster_id":401,"group_ids":[6680516253,6722443056,6723530557,6724009526,6725391922],"issue_titles":["ApiError: {\"message\":\"404 File Not Found\"}"],"root_cause_summaries":["Misconfigured code mapping's `source_root` prepends `src/` to already `src/`-prefixed stack paths, creating non-existent GitLab file paths.","GitLab API requests fail due to unnormalized file paths containing `./` components, causing 404 \"File Not Found\" errors.","GitLab blame API request used internal `app://` file path, causing 404 due to invalid external format.","Path conversion logic fails to strip `app:///` prefix from event frame `abs_path`, causing GitLab API 404.","Misconfigured code mapping's `source_root` containing a full URL causes GitLab API 404 for blame requests."],"transactions":["sentry.tasks.process_commit_context","sentry.shared_integrations.client.base in _request"],"title":"Commit context fetch returns 404 for file blame","description":"Workers calling the commit-context service fail when requesting file blame data, receiving HTTP 404 Not Found. Likely due to missing or invalid file paths/SHAs in the blame API request within the commit context integration.","tags":["API","External System","Input Validation","HTTP 404","Commit Context","Blame Lookup"],"cluster_size":5,"cluster_min_similarity":0.933652184324199,"cluster_avg_similarity":0.9548301192300881},{"project_ids":["1"],"cluster_id":409,"group_ids":[6683878991,6686531755,6776701837],"issue_titles":["SnubaRPCError: code: 500"],"root_cause_summaries":["Nested Snuba queries within `time_spent_percentage()` for each calculation overwhelm ClickHouse's concurrency limits, causing 500 errors.","Complex span query with expensive `time_spent_percentage()` calculation and sorting exceeds ClickHouse's 25s timeout.","ClickHouse query for trace logs exceeded 25s timeout due to large data volume."],"transactions":["/api/0/organizations/{organization_id_or_slug}/trace-logs/","/api/0/organizations/{organization_id_or_slug}/events/"],"title":"Snuba RPC queries timing out on table_rpc","description":"Multiple endpoints calling Snuba via table_rpc are exceeding execution time limits, causing SnubaRPCError timeouts from the analytics platform nodes. Impacted areas include organization events and trace logs queries.","tags":["External System","API","Timeout","Snuba","ClickHouse","table_rpc"],"cluster_size":3,"cluster_min_similarity":0.9399469175100923,"cluster_avg_similarity":0.9450785500323916},{"project_ids":["1"],"cluster_id":411,"group_ids":[6684310791,6710958153,6755570545],"issue_titles":["DecodeError: Error parsing message"],"root_cause_summaries":["Snuba returned a JSON error response for a failed RPC query, but Sentry expected a protobuf `ErrorProto`, causing a `DecodeError`.","Snuba RPC client expects protobuf error responses, but Snuba returns JSON errors, causing a DecodeError.","Snuba RPC client attempts parsing non-Protobuf 504 Gateway Timeout response as Protobuf error, causing DecodeError."],"transactions":["/api/0/organizations/{organization_id_or_slug}/trace-meta/{trace_id}/","/api/0/organizations/{organization_id_or_slug}/trace-logs/","/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/"],"title":"Protobuf decode errors in Snuba RPC responses","description":"Multiple trace, meta, and logs endpoints fail when parsing Snuba RPC responses, raising DecodeError during ParseFromString. Likely mismatch between protobuf schema and payload or corrupted/invalid response data impacting trace retrieval.","tags":["API","Serialization","External System","Protobuf","Decode Error","Snuba RPC"],"cluster_size":3,"cluster_min_similarity":0.9409686118818813,"cluster_avg_similarity":0.9512345153716791},{"project_ids":["6178942"],"cluster_id":412,"group_ids":[6684547171,6727762541,6763405664,6771053956,6805321764],"issue_titles":["GithubException: 403 {\"message\": \"Resource not accessible by integration\", \"documentation_url\": \"https://docs.github.com/rest/git/trees#create-a-tree\", \"status\": \"403\"}","GithubException: 403 {\"message\": \"Resource not accessible by integration\", \"documentation_url\": \"https://docs.github.com/rest/git/trees#get-a-tree\", \"status\": \"403\"}","GithubException: 403 {\"message\": \"Resource not accessible by integration\", \"documentation_url\": \"https://docs.github.com/rest/git/refs#create-a-reference\", \"status\": \"403\"}","GithubException: 403 {\"message\": \"Resource not accessible by integration\", \"documentation_url\": \"https://docs.github.com/rest/repos/contents#get-repository-content\", \"status\": \"403\"}","Exception: GraphQL errors: [{'type': 'FORBIDDEN', 'path': ['resolveReviewThread'], 'extensions': {'saml_failure': False}, 'locations': [{'line': 3, 'column': 13}], 'message': 'Resource not accessible by integration'}]"],"root_cause_summaries":["GitHub App lacks \"Repository Administration: Read/Write\" permission for `resolveReviewThread` GraphQL mutation.","GitHub App lacks \"Contents: write\" permission for `instruct-ai/backend` repository, preventing branch creation.","GitHub App lacks `contents:read` permission for `doordash/android` repository, preventing git tree data access for root cause analysis.","GitHub App lacks \"contents:read\" permission for the repository, causing 403 Forbidden when accessing file content.","GitHub App lacks 'Contents: Write' permission, preventing Git tree creation for autofix changes."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task","seer.automation.codegen.pr_review_step.pr_review_task","seer.automation.autofix.steps.change_describer_step.autofix_change_describer_task"],"title":"GitHub integration lacks permissions for repo operations","description":"Multiple steps fail with 'Resource not accessible by integration' when the app attempts to read trees, create branches, fetch contents, and resolve review threads via REST and GraphQL. The GitHub App likely lacks required repository permissions or installation access for these operations.","tags":["External System","Authorization","API","GitHub","Permission Denied","GraphQL","REST"],"cluster_size":5,"cluster_min_similarity":0.9371213407592797,"cluster_avg_similarity":0.9546153754629417},{"project_ids":["1"],"cluster_id":414,"group_ids":[6686163685,6690646009],"issue_titles":["ValueError: not enough values to unpack (expected 2, got 0)"],"root_cause_summaries":["Redis key `pc:<project_id>` was not a list, causing `pipe.execute()` to return an empty list, leading to unpacking error.","Redis failover client returns empty pipeline results, causing `ValueError` during short ID unpacking."],"transactions":["sentry.tasks.store.save_event"],"title":"Redis counter unpack error in event grouping","description":"Event ingestion workers fail while generating project short IDs due to unpacking the Redis pipeline result, causing ValueError during group creation.","tags":["Caching","Queueing","API","Redis","Value Unpack Error","Celery","Event Ingestion"],"cluster_size":2,"cluster_min_similarity":0.960309521865663,"cluster_avg_similarity":0.960309521865663},{"project_ids":["1"],"cluster_id":421,"group_ids":[6688322664,6688322778,6688322788,6688322808,6688322810,6688322861,6688322895,6688322943,6688323025,6688323380,6688323546,6688324811,6688326289,6689845945,6689847586,6689847877,6689848067,6689848120,6689848225,6689856331,6691438657,6694791217,6694791220,6694791235,6694791350,6694791372,6694791425,6713349055,6717441961,6717442020,6722351929,6724785390,6725366299,6727271527,6727272077,6727272217,6728187517,6728187518,6731822907,6731822925,6731822948,6731822949,6731822952,6731822981,6731822982,6731822987,6731822989,6731823002,6731823006,6731823010,6731823011,6731823013,6731823016,6731823017,6731823019,6731823022,6731823060,6731823087,6731823194,6733713933,6733713941,6733714085,6739200313,6739200336,6739200351,6739200363,6739200381,6739200382,6739200384,6739200385,6739200391,6739200399,6739200414,6739200415,6739200418,6739200428,6739200430,6739200448,6739200449,6739200453,6739200456,6739200458,6739200461,6739200463,6739200464,6739200466,6739200469,6739200471,6739200475,6739200477,6739200479,6739200481,6739200483,6739200486,6739200489,6739200490,6739200495,6739200507,6739200511,6739200513,6739200514,6739200518,6739200522,6739200532,6739200533,6739200537,6739200538,6739200540,6739200543,6739200544,6739200549,6739200551,6739200555,6739200561,6739200585,6739200588,6739200589,6739200593,6739200599,6739200600,6739200601,6739200608,6739200609,6739200614,6739200618,6739200623,6739200629,6739200630,6739200651,6739200658,6739200660,6739200662,6739200663,6739200665,6739200671,6739200673,6739200674,6739200678,6739200679,6739200688,6739200693,6739200694,6739200698,6739200705,6739200712,6739200733,6739200737,6739200742,6739200749,6739200754,6739200781,6739200792,6741992393,6741992395,6741992401,6741992408,6741992409,6741992412,6741992416,6741992417,6741992422,6741992438,6741992456,6741992475,6741992479,6741992485,6741992516,6746466931,6746467188,6746468889,6746469780,6746469787,6746469794,6746471161,6746472017,6746472306,6746480906,6746492348],"issue_titles":["OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"],"root_cause_summaries":["Sentry worker failed connecting to PostgreSQL; database connection string pointed to container's localhost, not external database service.","Sentry's `DATABASE_URL` environment variable misconfigures PostgreSQL port to 6432, causing connection refusal.","pgbouncer, expected on localhost:6432 for attachments database, was not running or accessible, causing connection refusal.","Taskworker's database connection to PgBouncer on 127.0.0.1:6432 refused, indicating PgBouncer unavailability or misconfiguration.","PostgreSQL connection pooler (PgBouncer) at 127.0.0.1:6432 refused connection, preventing database access for Sentry taskworkers.","Taskworker attempts PostgreSQL connection on port 6432 due to `SENTRY_POSTGRES_PORT` environment variable, but no server listens there.","PostgreSQL connection refused at 127.0.0.1:6432; database service or connection pooler is unavailable.","Sentry worker's PostgreSQL connection failed; database not listening on configured 127.0.0.1:6432.","Taskworker's database connection failed because no PostgreSQL or pooler listens on configured 127.0.0.1:6432.","Taskworker's database connection failed because PgBouncer, configured on port 6432, was not running or accessible.","PgBouncer is not running or listening on port 6432, causing database connection refusal for taskworker.","Taskworker processes attempt PostgreSQL connection on port 6432, but the database listens on 5432, causing connection refused.","Taskworker's database connection failed because PgBouncer was not listening on the configured `127.0.0.1:6432` endpoint.","Sentry worker failed to connect to PgBouncer at 127.0.0.1:6432, due to PgBouncer being unavailable.","PgBouncer, expected on port 6432, was not running or accessible, causing Sentry's database connection attempts to be refused.","Task worker's database connection failed because PgBouncer/PostgreSQL was not listening on the configured 127.0.0.1:6432.","Celery worker's DATABASE_URL environment variable incorrectly points to a non-existent PostgreSQL instance.","PgBouncer service is unavailable, causing database connection refusal on its standard port 6432.","Sentry worker's `DATABASE_URL` misconfiguration points to an unavailable PostgreSQL server at `127.0.0.1:6432`, causing connection refusal.","Pgbouncer for 'attachments' database was unreachable at 127.0.0.1:6432, causing connection refusal.","PostgreSQL server for 'attachments' database not running on 127.0.0.1:6432, preventing UserReport updates.","Attachments database at 127.0.0.1:6432 refused connection, halting `link_event_to_user_report` task execution.","Taskworker's `DATABASE_URL` misconfiguration points to an inaccessible PostgreSQL instance at `127.0.0.1:6432`.","Taskworker's `flush_usage_buffer` failed to connect to the `usage` PostgreSQL database at `127.0.0.1:6432` because the database server was unreachable.","PostgreSQL service for attachments at 127.0.0.1:6432 is unavailable, causing connection refused error during `EventAttachment` update.","PgBouncer, configured on port 6432, was unreachable, preventing database connections for issue processing.","Attachments database connection refused; the PostgreSQL server or PgBouncer on 127.0.0.1:6432 was unreachable.","PostgreSQL connection refused for 'attachments' database on 127.0.0.1:6432; the dedicated database service is not running.","Billing task failed: `usage_replica` database connection refused at `127.0.0.1:6432` because no server was listening.","Taskworker fails to connect to PostgreSQL because PgBouncer is not running or accessible on the configured 127.0.0.1:6432.","PgBouncer for the `attachments` database is not running or accessible on `127.0.0.1:6432`, causing connection refusal for post-processing tasks.","Taskworker attempts database connection to `127.0.0.1:6432` due to hardcoded settings, but no database service is listening locally within its container.","PgBouncer service on 127.0.0.1:6432 is unavailable, preventing worker database connections.","PgBouncer connection pooler was not running or accessible on port 6432, causing database connection refusal.","Attachments PostgreSQL database service not running or inaccessible at 127.0.0.1:6432.","Database server not listening on 127.0.0.1:6432, causing connection refusal during task processing.","Taskworker database connection fails due to misconfigured PostgreSQL host/port (127.0.0.1:6432), preventing event post-processing.","PgBouncer for 'attachments' database is down, causing connection refused errors for post-processing tasks.","Celery worker's database connection to `localhost:6432` (PgBouncer) failed because the service was unreachable.","PostgreSQL server for 'usage' database not running or accessible at `127.0.0.1:6432`, causing connection refusal for taskworker.","Worker attempts database connection to localhost:6432; no service listens, causing connection refusal.","PgBouncer service is not listening on 127.0.0.1:6432, causing database connection refusal.","PgBouncer was unavailable on port 6432, causing Sentry's task worker to fail database connection attempts.","Taskworker's billing query routed to non-existent PostgreSQL service on port 6432, causing connection refusal.","PgBouncer, expected at 127.0.0.1:6432, is unavailable, causing database connection refusal for Sentry workers.","Taskworker's database connection to `127.0.0.1:6432` is refused because the PostgreSQL connection pooler is not running or accessible.","Worker's database connection parameters are misconfigured, pointing to a non-existent PostgreSQL service at `127.0.0.1:6432`.","Celery worker failed connecting to `control_replica` database at `127.0.0.1:6432`; no PostgreSQL listener was present.","Taskworker's `SENTRY_POSTGRES_PORT` environment variable is misconfigured to 6432, causing connection refusal.","PostgreSQL connection refused due to server unavailability or misconfigured `DATABASE_URL` environment variable.","Task worker's PostgreSQL connection failed due to `SENTRY_POSTGRES_PORT` environment variable being incorrectly set to `6432`.","Taskworker attempts PostgreSQL connection on port 6432, but server listens on 5432, causing connection refusal.","Celery worker's `DATABASE_URL` environment variable incorrectly points to `127.0.0.1:6432`, causing PostgreSQL connection refusal.","Task worker's database connection to 127.0.0.1:6432 refused; PgBouncer/PostgreSQL not listening or accessible.","PostgreSQL server not listening on port 6432, causing connection refusal for Sentry's taskworker.","PgBouncer not listening on 127.0.0.1:6432, causing database connection refusal for Sentry task workers.","PostgreSQL connection refused; application configured for port 6432, but database listens on 5432, likely missing connection pooler.","Sentry's PostgreSQL connection failed because Kubernetes deployment configured database host as localhost, where no service listened.","Read replica database server at 127.0.0.1:6432 is unreachable, causing connection refusal for usage buffer task.","Monitors consumer failed to connect to its dedicated PostgreSQL database on port 6432; service unavailable.","Sentry worker's configured PostgreSQL connection pooler (PgBouncer) at 127.0.0.1:6432 is unreachable, causing connection refusal.","PostgreSQL server unreachable at `127.0.0.1:6432`, causing connection refusal during Sentry task's database query.","Taskworker's PostgreSQL connection failed due to misconfigured host '127.0.0.1' and non-standard port '6432'.","PostgreSQL server at 127.0.0.1:6432 refused connection, preventing Sentry worker from processing rules due to database inaccessibility.","Taskworker's `DATABASE_URL` environment variable misconfigures PostgreSQL connection, leading to connection refused error during event processing.","PgBouncer not running or accessible on 127.0.0.1:6432, causing database connection refusal for Sentry task worker.","Pgbouncer connection pooler is not running or accessible on port 6432, causing database connection refusal.","Pgbouncer service unavailable at 127.0.0.1:6432, causing database connection refusal for taskworker.","PostgreSQL/PgBouncer not listening on 127.0.0.1:6432, causing connection refused during taskworker's database operation.","PostgreSQL server not listening on 127.0.0.1:6432, causing connection refusal during event processing.","PostgreSQL server at 127.0.0.1:6432 refused connection, preventing project lookup during occurrence processing.","Worker's PostgreSQL connection parameters are misconfigured, pointing to an inaccessible localhost address and port, causing connection refusal.","PostgreSQL service at 127.0.0.1:6432 refused connection, halting Celery worker's database operations.","PostgreSQL connection refused at 127.0.0.1:6432, preventing `process_rules` from querying the database.","Sentry worker failed to connect to `usage_replica` database at `127.0.0.1:6432` because the PostgreSQL service or connection pooler was not running or accessible.","Sentry worker's database configuration points to a non-existent local pgbouncer, causing connection refusal during database operations.","The attachments PostgreSQL database, configured externally on port 6432, was unreachable, causing connection refusal during event post-processing.","Taskworker processes fail to connect to PgBouncer on port 6432; the connection pooler is unavailable or misconfigured.","PostgreSQL connection refused on port 6432 because no database service or connection pooler was listening.","Taskworker's `DATABASE_URL` environment variable specified incorrect PostgreSQL port 6432, causing connection refusal.","PostgreSQL service for 'attachments' database not running/accessible at `127.0.0.1:6432`, causing connection refusal during post-processing.","Task worker's `DATABASE_URL` environment variable incorrectly points to an inaccessible PostgreSQL instance at `127.0.0.1:6432`.","Sentry worker failed to connect to PostgreSQL due to misconfigured database host `127.0.0.1:6432`, expecting a local PgBouncer not present in the container.","Celery worker's PgBouncer connection failed; PgBouncer not running or accessible at 127.0.0.1:6432.","Celery worker's PgBouncer connection failed; port 6432 refused connection, indicating PgBouncer was unavailable.","PostgreSQL connection refused due to `SENTRY_POSTGRES_PORT` misconfigured to 6432 instead of 5432.","PostgreSQL connection pooler on 127.0.0.1:6432 refused connections, preventing database access for event post-processing tasks.","Celery worker's `DATABASE_URL` environment variable points to non-existent PgBouncer on port 6432, causing connection refusal.","Sentry worker failed to connect to the externally configured 'attachments' PostgreSQL database on port 6432 because the server was unavailable.","Sentry's taskworker failed to connect to PgBouncer at 127.0.0.1:6432 because no service was listening there.","PostgreSQL connection refused due to SENTRY_POSTGRES_PORT environment variable incorrectly set to 6432 instead of 5432.","UserReport model incorrectly routed to non-existent 'attachments' database, causing connection refusal.","Taskworker failed to connect to PostgreSQL at 127.0.0.1:6432; no server listening there.","Task worker's database connection fails; PostgreSQL server unreachable at configured `127.0.0.1:6432` address/port.","PostgreSQL connection pooler (PgBouncer) is unreachable on port 6432, preventing database access for Sentry tasks.","Taskworker's database connection failed because PostgreSQL was not listening on the configured port 6432.","Sentry taskworker's PostgreSQL connection to 127.0.0.1:6432 failed due to 'Connection refused', indicating database unavailability or misconfiguration.","PgBouncer/PostgreSQL not listening on 127.0.0.1:6432, causing connection refusal for usage database.","Usage database server at 127.0.0.1:6432 is not running or accessible, causing connection refusal for billing tasks.","PostgreSQL server at 127.0.0.1:6432 refused connection for 'getsentry-workers' user, preventing issue occurrence processing.","Taskworker failed to connect to pgbouncer on port 6432; pgbouncer was not running or misconfigured.","Task worker's `DATABASE_URL` environment variable points to an inaccessible PostgreSQL instance at `127.0.0.1:6432`, causing connection refusal.","PgBouncer not running/accessible at 127.0.0.1:6432, preventing database connections configured for local proxy.","Sentry task failed due to PostgreSQL connection refused at 127.0.0.1:6432, likely from `DATABASE_URL` misconfiguration.","Task worker's `DATABASE_URL` environment variable specifies PostgreSQL port 6432, causing connection refusal.","PostgreSQL connection pooler on 127.0.0.1:6432 refused connection, preventing database operations.","Taskworker's environment variables incorrectly point to an inaccessible PostgreSQL instance at 127.0.0.1:6432.","Taskworker's `DATABASE_URL` environment variable incorrectly specifies PostgreSQL port 6432, causing connection refusal as the database listens on 5432.","PostgreSQL server not running or accessible at 127.0.0.1:6432 for taskworker database connection.","PgBouncer missing/inaccessible at 127.0.0.1:6432, preventing Sentry's occurrence consumer from connecting to PostgreSQL.","PgBouncer service is not running or accessible at 127.0.0.1:6432, causing database connection refusal for Sentry task workers.","PgBouncer connection refused on port 6432, preventing database access for event processing.","Taskworker's PgBouncer connection pooler is unavailable at 127.0.0.1:6432, causing database connection refusal.","Task worker's DATABASE_URL environment variable misconfigured, pointing to a non-existent or inaccessible PostgreSQL/PgBouncer service at 127.0.0.1:6432.","Taskworker failed to connect to PgBouncer on 127.0.0.1:6432, causing database query refusal during post-processing.","PgBouncer service not running on 127.0.0.1:6432, causing database connection refusal for uptime result processing.","PostgreSQL connection pooler unavailable at 127.0.0.1:6432, causing database connection refusal.","PgBouncer not listening on 127.0.0.1:6432, preventing taskworker database connections.","Application's database connection string points to localhost:6432, but pgbouncer is external.","Database connection pooler on 127.0.0.1:6432 is unavailable, causing connection refused errors for replica database access.","PgBouncer, configured on port 6432, was not running or accessible, causing database connection attempts to be refused.","Taskworker attempts PostgreSQL connection on port 6432, but service listens on 5432, causing connection refused due to misconfigured SENTRY_POSTGRES_PORT.","Taskworker's replica database connection misconfigured to unreachable 127.0.0.1:6432, causing connection refused error.","Taskworker failed connecting to a non-existent PostgreSQL replica at 127.0.0.1:6432, configured externally.","Task worker failed to connect to PgBouncer at `127.0.0.1:6432` because the connection pooler was unavailable.","PgBouncer sidecar within the Sentry pod failed to listen on localhost:6432, causing connection refusal.","PostgreSQL connection pooler for 'attachments' database unavailable at 127.0.0.1:6432, causing connection refused.","Worker process failed to connect to its dedicated PostgreSQL instance at 127.0.0.1:6432, causing connection refused error.","Database connection proxy (PgBouncer) on port 6432 was unreachable, causing connection refused errors for taskworkers.","Taskworker child process fails to connect to PgBouncer at 127.0.0.1:6432, causing database query failure.","Attachments database connection refused; external configuration incorrectly set port 6432, not 5432, for PostgreSQL.","Celery worker's PostgreSQL connection failed due to misconfigured `SENTRY_POSTGRES_HOST`/`PORT` environment variables pointing to an inaccessible database.","Commercial `getsentry` database router misdirected `UserReport` query to unavailable `attachments` database on port 6432.","Celery worker's `flush_usage_buffer` task fails to connect to PgBouncer on 127.0.0.1:6432, as PgBouncer is not running or accessible.","PostgreSQL server not listening on 127.0.0.1:6432, causing connection refused error during database query.","PostgreSQL connection pooler (PgBouncer) is not running on `127.0.0.1:6432`, causing Sentry worker database connection failures.","Celery worker's database connection failed; PostgreSQL server not listening on configured port 6432.","PgBouncer service is unavailable at 127.0.0.1:6432, causing taskworker database connection refusal.","Sentry worker attempts connecting to PGBouncer on localhost:6432, but PGBouncer is a separate Kubernetes service, causing connection refusal.","Taskworker fails to connect to PostgreSQL because no database server or connection pooler is listening on the configured 127.0.0.1:6432.","PostgreSQL connection refused at 127.0.0.1:6432; database service or connection pooler is unavailable or inaccessible.","PostgreSQL service or connection pooler at 127.0.0.1:6432 is down, causing connection refused errors for Sentry taskworkers.","Task worker failed to connect to `usage_replica` database at `127.0.0.1:6432` because no service was listening.","Taskworker's database connection failed due to an incorrect PostgreSQL port (6432) specified in its configuration.","Ingest consumer failed to connect to its configured PostgreSQL replica on port 6432, due to the database server being unavailable.","PgBouncer not running or accessible at 127.0.0.1:6432, causing database connection refusal for task worker.","Attachments database service not running or accessible at 127.0.0.1:6432, causing connection refusal during user report processing.","PostgreSQL server unavailable at configured 127.0.0.1:6432, preventing QuerySubscription data retrieval.","Application configured for PgBouncer on port 6432; connection refused as no service listens there.","Taskworker failed to connect to PostgreSQL because PgBouncer, configured via DATABASE_URL to use port 6432, was unavailable.","Sentry taskworker failed to connect to PostgreSQL at `127.0.0.1:6432` because no database server was listening on that address and port.","Celery worker's database connection to `127.0.0.1:6432` failed; `pgbouncer` service is not running or accessible.","Taskworker failed to connect to PostgreSQL on port 6432 because the PgBouncer/connection pooler service was unavailable.","Taskworker failed to connect to PostgreSQL at 127.0.0.1:6432; database not listening on configured port.","PgBouncer on 127.0.0.1:6432 unresponsive, causing database connection refusal and subsequent data access failures.","PostgreSQL connection refused for 'attachments' database at 127.0.0.1:6432; no service listening on that port.","Sentry worker failed connecting to PostgreSQL at 127.0.0.1:6432; no service listening.","PgBouncer service not running or inaccessible, causing database connection refusal for task workers.","Task worker's database connection to `127.0.0.1:6432` failed because PgBouncer was inaccessible at that address.","Taskworker failed to connect to the 'usage' PostgreSQL database at 127.0.0.1:6432; the database was not running or accessible.","Task worker's database connection failed because `127.0.0.1:6432` (PgBouncer) was unreachable from its isolated localhost environment.","PostgreSQL replica server not running or accessible at 127.0.0.1:6432, causing connection refusal.","PgBouncer not listening on localhost:6432, preventing taskworker database connections.","Sentry worker's PgBouncer connection pooler is not running or refusing connections on 127.0.0.1:6432, preventing database access.","PostgreSQL connection refused because `SENTRY_POSTGRES_PORT` was misconfigured to `6432` instead of `5432`.","Dedicated 'crons' PostgreSQL server at 127.0.0.1:6432 is not running or configured to accept connections.","Sentry workers fail to connect to PostgreSQL because PgBouncer, configured on port 6432, is unavailable.","Taskworker's PostgreSQL connection failed due to server refusing connection at configured `127.0.0.1:6432`.","PostgreSQL connection refused; configured PgBouncer proxy at 127.0.0.1:6432 is unreachable or not running.","Sentry worker's configured database connection to `127.0.0.1:6432` was refused; no process was listening.","PostgreSQL connection refused: database server or connection pooler not listening on 127.0.0.1:6432.","PostgreSQL connection pooler not running on `127.0.0.1:6432`, causing connection refused error.","Taskworker fails database connection to 127.0.0.1:6432; PostgreSQL/connection pooler not listening or accessible.","PgBouncer not running at 127.0.0.1:6432, causing database connection refusal for taskworker.","Taskworker failed to connect to local PgBouncer on port 6432, as the PgBouncer service was not running or accessible.","PostgreSQL connection pooler on port 6432 is unavailable, causing connection refusal for the consumer process.","Task worker's database connection to PgBouncer on port 6432 failed; PgBouncer was unavailable.","PgBouncer not listening on 127.0.0.1:6432, preventing worker processes from connecting to the `default_replica` database."],"transactions":["sentry.tasks.autofix.start_seer_automation","sentry.buffer.base in process","sentry.db.models.manager.base_query_set in update","ingest_consumer.process_attachment_chunk","sentry.rules.processing.delayed_processing","sentry.tasks.store.save_event","/api/0/relays/projectconfigs/","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","sentry.monitors.clock_tasks.check_timeout in mark_checkin_timeout","sentry.tasks.post_process.post_process_group","issues.occurrence_consumer","sentry.hybridcloud.tasks.deliver_webhooks.schedule_webhook_delivery","sentry.tasks.store.save_event_transaction","query_subscription_consumer_process_message","getsentry.models.billinghistory in get_current","monitors.monitor_consumer","monitors.uptime.result_consumer","ingest_consumer.process_event","getsentry.billing.tasks.usagebuffer in flush_usage_buffer"],"title":"Celery post-process jobs fail: DB connection refused","description":"Celery workers running post_process tasks cannot connect to the database, causing OperationalError (connection refused) and subsequent InterfaceError (connection already closed) during rule processing and service hook queries.","tags":["Database","Queueing","Networking","Celery","Connection Refused","InterfaceError","Post-Process Pipeline"],"cluster_size":179,"cluster_min_similarity":0.9139547141731486,"cluster_avg_similarity":0.9557026901985709},{"project_ids":["1"],"cluster_id":702,"group_ids":[6688323492,6728188991,6741992400,6746468909],"issue_titles":["Retriable: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused","OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"],"root_cause_summaries":["Database replica misconfiguration prevents connection to PostgreSQL at 127.0.0.1:6432.","Ingest consumer failed to connect to a non-existent or inaccessible PostgreSQL read replica at 127.0.0.1:6432, configured externally.","Celery worker attempts connecting to a non-existent PostgreSQL read replica on 127.0.0.1:6432, causing connection refused.","Replica database connection refused due to no service listening on `127.0.0.1:6432`."],"transactions":["ingest_consumer.process_event","sentry.tasks.post_process.post_process_group","getsentry.billing.tasks.usagebuffer in flush_usage_buffer","sentry.tasks.store.save_event"],"title":"Database connections to Postgres are refused across workers","description":"Multiple Celery and Kafka processor paths fail with OperationalError/InterfaceError when fetching cached Django ORM objects, indicating the PostgreSQL server is unreachable (TCP connection refused). This disrupts event processing and post-processing tasks across services.","tags":["Database","Networking","Queueing","PostgreSQL","Connection Refused","Celery","Kafka"],"cluster_size":4,"cluster_min_similarity":0.9578732944802084,"cluster_avg_similarity":0.9642255517539587},{"project_ids":["1"],"cluster_id":522,"group_ids":[6689854760,6713349226,6713349237,6722351545,6722351578,6722351772,6722351790,6722351903,6722352074,6722352268,6722353680,6728188695,6728188697,6728188703,6730689458,6746468610,6746468617],"issue_titles":["OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"],"root_cause_summaries":["PostgreSQL/pgBouncer for `usage_replica` database is inaccessible at `127.0.0.1:6432`, preventing subscription data retrieval for feature evaluation.","Pgbouncer connection refused, blocking feature flag evaluation, causing event processing failure.","PostgreSQL 'usage' database connection refused at 127.0.0.1:6432; server unavailable or misconfigured for consumer process.","Post-processing task failed because `kick_off_seer_automation` could not connect to an unavailable PostgreSQL replica at 127.0.0.1:6432.","Worker pods attempt to connect to a non-existent PostgreSQL server on localhost:6432, due to misconfigured database host settings.","Event post-processing fails because feature flag evaluation requires subscription data, which queries an unavailable `usage` database.","PgBouncer service on localhost:6432 is unreachable, causing database connection refusal for `usage_replica`.","PgBouncer service is unavailable at 127.0.0.1:6432, preventing database connections for post-processing tasks.","PostgreSQL server at 127.0.0.1:6432 refused connection, preventing subscription data retrieval for feature flag evaluation.","PgBouncer not listening on 127.0.0.1:6432, preventing database connections for feature flag evaluation.","PostgreSQL server at `127.0.0.1:6432` refused connection for `usage_replica` database, halting feature flag evaluation.","Database connection pooler at 127.0.0.1:6432 is unreachable, preventing subscription data retrieval for feature checks.","Billing/usage database at 127.0.0.1:6432 refused connection, preventing feature flag evaluation and halting event post-processing.","The 'usage' database, critical for feature checks, was unreachable, causing connection refusal during event post-processing.","PostgreSQL server for 'usage' database is unreachable at 127.0.0.1:6432, preventing feature flag checks.","Usage replica database at 127.0.0.1:6432 refused connection, preventing subscription data retrieval.","PgBouncer connection pooler for `usage_replica` database was unreachable on `127.0.0.1:6432`, causing connection refused errors."],"transactions":["issues.occurrence_consumer","sentry.tasks.post_process.post_process_group"],"title":"Database connections refused during feature-flag checks","description":"Celery post-process tasks fail when querying subscription/plan models to evaluate feature flags, as database connections are refused. This prevents workflow engine alerts, service hooks, and automation from running.","tags":["Database","Queueing","Configuration","Connection Refused","Celery","Feature Flags"],"cluster_size":17,"cluster_min_similarity":0.9407201153353595,"cluster_avg_similarity":0.9633895037045581},{"project_ids":["1"],"cluster_id":428,"group_ids":[6692053876,6750317918,6794819975],"issue_titles":["IntegrationError: Error Communicating with GitHub (HTTP 500): unknown error","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"5820bd2170894d039596fd73fae813d3\"}"],"root_cause_summaries":["GitHub App lacks issue creation permissions for `freshdesk` organization, causing 500 error from GitHub API.","GitHub App lacks permissions for Freshworks organization, causing Sentry's integration proxy to return 500 errors.","GitHub App installation token invalid, causing 403 during refresh, masked as 500 by generic proxy error handling."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.rules.processing.delayed_processing","sentry.tasks.post_process.post_process_group"],"title":"GitHub issue creation returns 5xx during post-process","description":"Post-process rules fail when creating GitHub issues due to upstream 5xx responses, causing IntegrationError/ApiError in the GitHub integration client. Impact: automatic issue creation in GitHub is failing for Sentry post-processing and delayed processing flows.","tags":["External System","API","Upstream Unavailable","GitHub","HTTP 5xx","IntegrationError"],"cluster_size":3,"cluster_min_similarity":0.9383029361095223,"cluster_avg_similarity":0.9513892580836188},{"project_ids":["1"],"cluster_id":430,"group_ids":[6692100082,6692907449,6715347427],"issue_titles":["OperationalError: timed out"],"root_cause_summaries":["Message broker connection timed out during task queuing due to network or broker unresponsiveness.","Celery's AMQP connection to RabbitMQ timed out, preventing task scheduling for project config generation.","Celery task timed out communicating with AMQP broker while scheduling a new task, causing the task to crash."],"transactions":["getsentry.billing.tasks.usagebuffer.flush_usage_buffer","/api/0/relays/projectconfigs/","sentry.tasks.post_process.post_process_group"],"title":"Celery task dispatch timing out and crashing workers","description":"Celery apply_async/delay calls are timing out during post-processing and usage tallying, leading to AttributeError on ChannelPromise and occasional worker crashes when the broker connection closes. This suggests broker or AMQP channel issues causing timeouts and unstable connections across web and worker paths.","tags":["Queueing","Networking","Configuration","Celery","Timeout","Server Closed Connection","ChannelPromise AttributeError"],"cluster_size":3,"cluster_min_similarity":0.9418075842008128,"cluster_avg_similarity":0.9505243381865646},{"project_ids":["1"],"cluster_id":437,"group_ids":[6694403659,6783293009,6793509594,6808353360,6808353436,6813542827,6813542886],"issue_titles":["ConnectionError: Error -3 connecting to redis-buffer-0.sentry.:6379. Temporary failure in name resolution.","ConnectionError: Error -3 connecting to redis-buffer-2.sentry.:6379. Temporary failure in name resolution.","ConnectionError: Error -3 connecting to redis-buffer-4.sentry.:6379. Temporary failure in name resolution.","ConnectionError: Error -5 connecting to rc-memorystore-processing.sentry.:6379. No address associated with hostname.","ConnectionError: Error -3 connecting to rc-memorystore-processing-transactions.sentry.:6379. Temporary failure in name resolution.","ConnectionError: Error -3 connecting to redis-buffer-1.sentry.:6379. Temporary failure in name resolution."],"root_cause_summaries":["Task worker cannot resolve Redis service hostname `rc-memorystore-processing-transactions.sentry.` due to DNS configuration.","Sentry worker's configured Redis hostname `redis-buffer-7.sentry.` fails DNS resolution, preventing connection.","DNS resolution for `redis-buffer-1.sentry.` temporarily failed, preventing Redis connection due to network infrastructure issues.","DNS resolution for `redis-buffer-1.sentry.` failed due to a temporary name resolution issue, preventing Redis connection.","Task worker's environment failed to resolve `redis-buffer-4.sentry.` hostname to an IP address, preventing Redis connection.","Redis connection failed: hostname `rc-memorystore-processing.sentry.` unresolvable by DNS.","Redis connection fails due to `redis-buffer-2.sentry.` hostname's DNS resolution failure, indicating infrastructure misconfiguration."],"transactions":["ingest_consumer.process_event","sentry.tasks.post_process.post_process_group","sentry.buffer.redis in _process_single_incr","sentry.tasks.store.process_event","sentry.tasks.store.save_event_transaction"],"title":"Redis buffer DNS resolution failures in workers","description":"Celery/Sentry workers fail to access the redis-buffer instance due to DNS resolution errors, causing increments and reads in the buffer backend to fail. Impact: event grouping and buffered counters (times_seen, release counts) are not updated.","tags":["Networking","External System","Caching","DNS Resolution Failure","Redis","Celery","Sentry"],"cluster_size":7,"cluster_min_similarity":0.9312220967141072,"cluster_avg_similarity":0.9523422096450163},{"project_ids":["1"],"cluster_id":438,"group_ids":[6696355803,6793416499],"issue_titles":["Release.DoesNotExist: Release matching query does not exist."],"root_cause_summaries":["Snuba's session data contains stale organization IDs, causing `Release.DoesNotExist` when Sentry attempts database operations for non-existent organizations.","Session data exists for an organization absent from the database, causing foreign key violations during release creation."],"transactions":["sentry.tasks.process_projects_with_sessions"],"title":"Release adoption fails on missing Organization FK","description":"The adopt_releases task in process_projects_with_sessions attempts to fetch or create Release rows with an organization_id that does not exist in sentry_organization, causing ForeignKeyViolation/IntegrityError and subsequent Release.DoesNotExist errors.","tags":["Database","Data Integrity","API","PostgreSQL","Foreign Key Violation","IntegrityError","Sentry Release Adoption"],"cluster_size":2,"cluster_min_similarity":0.9719092566978667,"cluster_avg_similarity":0.9719092566978667},{"project_ids":["1"],"cluster_id":447,"group_ids":[6698211440,6698214718],"issue_titles":["Exception: Same primary email address for multiple users"],"root_cause_summaries":["Multiple active users share the same verified primary email due to missing database uniqueness constraint on `User.email`.","Multiple `User` records share the same `User.email` due to missing unique database constraint, causing identity resolution failure."],"transactions":["/extensions/{provider_id}/setup/"],"title":"Duplicate primary email maps to multiple users","description":"Authentication pipeline fails when a primary email address is associated with more than one user, causing the identity step to raise an exception. This blocks login/identity linking in the SSO/onboarding flow.","tags":["Authentication","Data Integrity","Input Validation","Identity Pipeline","Duplicate Email"],"cluster_size":2,"cluster_min_similarity":0.9731142090532466,"cluster_avg_similarity":0.9731142090532466},{"project_ids":["6178942"],"cluster_id":452,"group_ids":[6703194335,6705674845],"issue_titles":["UnpickleableExceptionWrapper: ClientError(\"429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details.', 'status': 'RESOURCE_EXHAUSTED'}}\")","ClientError: Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details."],"root_cause_summaries":["Vertex AI rate limits exhausted all LLM regions/models due to high request volume from document expansion.","LLM client failed due to single model's regions exhausting, lacking alternative models for fallback."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task"],"title":"Gemini API requests hit Resource Exhausted","description":"Calls to the Gemini generate_content endpoints are failing with RESOURCE_EXHAUSTED, likely due to quota or rate limits during structured and text generation steps within the Autofix pipeline.","tags":["External System","API","Rate Limiting","Retries Exhausted","Google Gemini","RESOURCE_EXHAUSTED"],"cluster_size":2,"cluster_min_similarity":0.9557329480717034,"cluster_avg_similarity":0.9557329480717034},{"project_ids":["1"],"cluster_id":455,"group_ids":[6703953976,6712149791,6810382376],"issue_titles":["MarketoError: Max rate limit '100' exceeded with in '20' secs","MarketoError: Concurrent access limit '10' reached"],"root_cause_summaries":["MarketoClient's lack of token caching and per-task instantiation doubles API calls, exceeding rate limits during high lead volume.","MarketoClient's per-request token retrieval, combined with high task concurrency, exhausts Marketo's API limit.","MarketoClient's per-request token fetching, combined with taskworker concurrency, exceeds Marketo's concurrent access limit."],"transactions":["/extensions/{provider_id}/setup/"],"title":"Marketo REST writes hit rate and concurrency limits","description":"Lead submission requests to Marketo are exceeding both the max rate limit and the concurrent access limit, causing MarketoError failures during POSTs to the lead endpoint.","tags":["External System","API","Rate Limiting","Concurrency","Marketo"],"cluster_size":3,"cluster_min_similarity":0.9614881935658862,"cluster_avg_similarity":0.9666961705433997},{"project_ids":["1"],"cluster_id":459,"group_ids":[6704590594,6768244431],"issue_titles":["BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles-de/o?uploadType=multipart: <!DOCTYPE html>","BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: <!DOCTYPE html>"],"root_cause_summaries":["GCS 502 error, unlisted in Sentry's retryable exceptions, caused profile upload failure without retries.","Google Cloud Storage frontend infrastructure returned 502 Bad Gateway with HTML, preventing profile upload after retries."],"transactions":["sentry.profiles.task.process_profile"],"title":"GCS uploads return 502/InvalidResponse in profiling task","description":"VroomRS transaction profiling tasks fail when uploading compressed profiles to Google Cloud Storage, receiving Bad Gateway responses and InvalidResponse errors from the GCS client. This prevents profiles from being saved downstream.","tags":["External System","API","Networking","Google Cloud Storage","Upstream Unavailable","InvalidResponse"],"cluster_size":2,"cluster_min_similarity":0.9649493455100249,"cluster_avg_similarity":0.9649493455100249},{"project_ids":["1"],"cluster_id":463,"group_ids":[6705342006,6795623703],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification"],"root_cause_summaries":["Webhook SSL connection failure, compounded by slow error reporting, caused task deadline to be exceeded.","Slow webhook response combined with costly error handling operations exceeds task's 10-second deadline."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.workflow_notification"],"title":"Workflow webhook notifications exceed task deadline","description":"Sentry Apps workflow_notification tasks time out while sending webhooks via safe_urlopen, causing ProcessingDeadlineExceeded and failure recording in SentryAppInteractionEvent.","tags":["External System","Queueing / Messaging","API","Timeout","Webhook","Sentry Apps"],"cluster_size":2,"cluster_min_similarity":0.9683168423551282,"cluster_avg_similarity":0.9683168423551282},{"project_ids":["1"],"cluster_id":467,"group_ids":[6706122053,6716314480],"issue_titles":["Project.DoesNotExist: Project matching query does not exist."],"root_cause_summaries":["SDK crash detection failed because the configured internal project ID `4505469596663808` does not exist in this Sentry instance.","SDK crash detection attempted to save an event to a configured project ID (4505469596663808) that does not exist."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"SDK crash post-process fails: missing Project","description":"Post-processing for SDK crash monitoring attempts to load a Project by ID but the Project is not found in the cache/DB, causing Project.DoesNotExist exceptions in the event manager.","tags":["Data Integrity","Configuration","API","Django ORM","Cache Miss","Does Not Exist"],"cluster_size":2,"cluster_min_similarity":0.9705098645995689,"cluster_avg_similarity":0.9705098645995689},{"project_ids":["1"],"cluster_id":468,"group_ids":[6706206696,6724526054,6728054891,6751864031,6792798563,6792915748,6800342104,6800384336,6813320324],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 300 seconds exceeded by sentry.tasks.auto_source_code_config"],"root_cause_summaries":["Recursive GitHub tree API call for large repository exceeded 300-second task deadline, causing `ProcessingDeadlineExceeded`.","Task's sequential, uncached, recursive GitHub tree fetches for 779+ repositories cumulatively exceed its 300-second deadline.","Sequential GitHub API calls for large repository trees exceeded the 300-second task deadline.","Sequential fetching of thousands of large GitHub repository trees exceeds the task's 300-second processing deadline.","Task times out because 'Not Found' API errors for inaccessible repositories are not counted by the circuit breaker, leading to excessive network calls.","Task timed out due to thousands of uncached GitHub API calls fetching large repository file trees.","Task exceeds deadline processing many inaccessible demo repositories sequentially, due to 404s not triggering circuit breaker.","Sequential processing of thousands of GitHub repositories' full recursive trees exceeds task's 300-second deadline.","Sequential, uncached GitHub recursive tree fetches for multiple repositories exceeded the task's 300-second deadline."],"transactions":["sentry.tasks.auto_source_code_config"],"title":"Auto source code config tasks exceed processing deadline","description":"The sentry.tasks.auto_source_code_config job times out while fetching repository trees from the SCM client, occasionally encountering 404 Not Found responses. Long-running external API calls and intermittent upstream errors cause the worker to hit its ProcessingDeadlineExceeded alarm.","tags":["External System","Networking","Queueing","API","Timeout","Upstream Unavailable","HTTP 404"],"cluster_size":9,"cluster_min_similarity":0.9404860947196524,"cluster_avg_similarity":0.967266242820813},{"project_ids":["1"],"cluster_id":469,"group_ids":[6706531586,6785481558],"issue_titles":["OperationalError: canceling statement due to statement timeout","OperationalError: QueryCanceled('canceling statement due to statement timeout\\nCONTEXT:  while inserting index tuple (5853,119) in relation \"sentry_projectcounter\"\\n')"],"root_cause_summaries":["Database `INSERT ON CONFLICT` query on `sentry_projectcounter` times out due to index contention, exceeding 1-second `statement_timeout`.","High ingestion rate causes multiple counter refill tasks, leading to PostgreSQL index lock contention exceeding the 1-second statement timeout."],"transactions":["ingest_consumer.process_event","sentry.models.counter.refill_cached_short_ids"],"title":"PostgreSQL timeouts updating sentry_projectcounter","description":"INSERT ... ON CONFLICT updates to the sentry_projectcounter table are timing out during index tuple insertion, causing QueryCanceled errors in worker tasks. Likely due to database contention or slow index writes leading to statement timeout.","tags":["Database","Concurrency","PostgreSQL","Timeout","ON CONFLICT Upsert","Index Write Contention"],"cluster_size":2,"cluster_min_similarity":0.9799977973387801,"cluster_avg_similarity":0.9799977973387801},{"project_ids":["1"],"cluster_id":470,"group_ids":[6706758580,6778861085],"issue_titles":["Environment.DoesNotExist: Environment matching query does not exist."],"root_cause_summaries":["Event's environment tag (or default empty string) lacks a matching Sentry environment record.","Event lacks environment tag; system fails to find non-existent default environment, causing `DoesNotExist`."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"Missing Environment record in post-process pipeline","description":"Post-processing tasks fail when linking events to user reports because the referenced Environment record is not found in the database. This likely stems from missing or deleted Environment entries or a race condition during environment resolution.","tags":["Data Integrity","API","Configuration","Django ORM","Environment.DoesNotExist"],"cluster_size":2,"cluster_min_similarity":0.9588425616499506,"cluster_avg_similarity":0.9588425616499506},{"project_ids":["6178942"],"cluster_id":472,"group_ids":[6707410828,6721352074,6792360748],"issue_titles":["ClientError: The input token count (32826) exceeds the maximum number of tokens allowed (32767).","UnpickleableExceptionWrapper: ClientError(\"400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'The input token count (2530477) exceeds the maximum number of tokens allowed (1048576).', 'status': 'INVALID_ARGUMENT'}}\")","UnpickleableExceptionWrapper: ClientError(\"400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'The input token count (6048721) exceeds the maximum number of tokens allowed (1048576).', 'status': 'INVALID_ARGUMENT'}}\")"],"root_cause_summaries":["LLM prompt size exceeded token limit due to verbose stack traces, code snippets, and trace data.","Replay summarization fails because verbose log generation, without token limits, creates LLM input exceeding Gemini's token cap.","Unbounded log data concatenated into LLM prompt exceeds Gemini API token limit, causing `INVALID_ARGUMENT` error."],"transactions":["seer.automation.autofix.steps.root_cause_step.root_cause_task","seer.automation.summarize.replays.start_replay_breadcrumbs_summary"],"title":"LLM requests exceed model token limit in Gemini client","description":"Calls to the Gemini generate_content/generate_structured APIs fail with INVALID_ARGUMENT when input token counts exceed the models maximum. This impacts root-cause and replay summary steps that stream or structure responses via the LLM client.","tags":["External System","API","Input Validation","Google Gemini","Invalid Argument","Token Limit Exceeded"],"cluster_size":3,"cluster_min_similarity":0.9506147654778289,"cluster_avg_similarity":0.9542784499820319},{"project_ids":["1"],"cluster_id":476,"group_ids":[6707981946,6754386746,6792265174],"issue_titles":["ApiError: {","ApiError: <html>","ApiError: {\"message\":\"403 Forbidden\"}"],"root_cause_summaries":["GitLab client's missing `org_integration_id` causes proxy signature mismatch, leading to 403 Forbidden.","Control Silo's integration proxy rejected Region Silo's request due to `SENTRY_SUBNET_SECRET` mismatch, failing HMAC signature verification.","Control Silo proxy rejects Region Silo requests due to missing/mismatched `SENTRY_SUBNET_SECRET`, causing 403s before external API calls."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","sentry.tasks.process_commit_context"],"title":"Forbidden (403) from code host API breaks commit context and PR workflows","description":"Calls to the external code hosting API are returning 403 Forbidden during blame retrieval, PR diff fetch, and merge-commit lookup, causing commit context processing and PR comment workflows to fail. Likely due to missing permissions or invalid/expired installation tokens on the integration client.","tags":["API","External System","Authorization","Forbidden","HTTP 403","Commit Context","Pull Request Comments"],"cluster_size":3,"cluster_min_similarity":0.951556387213743,"cluster_avg_similarity":0.9627790865057917},{"project_ids":["1"],"cluster_id":477,"group_ids":[6708210396,6774914346],"issue_titles":["SnubaRPCError: code: 500"],"root_cause_summaries":["Trace data retrieval for large organizations exceeds ClickHouse memory due to querying all projects.","ClickHouse memory limit exceeded due to `explore.trace-items.keys.max` being set too high, causing large attribute queries to fail."],"transactions":["/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/","/api/0/organizations/{organization_id_or_slug}/trace-items/attributes/"],"title":"ClickHouse query exceeds memory limit in Snuba RPC","description":"Snuba RPC calls for trace and attribute queries against ClickHouse fail due to query memory limit exceeded while reading attributes columns from MergeTree parts. This impacts organization trace views by causing SnubaRPCError responses when fetching traces and attribute names.","tags":["Database","Resource Limits","API","ClickHouse","Snuba","Memory Limit Exceeded","MergeTree"],"cluster_size":2,"cluster_min_similarity":0.9609937200346018,"cluster_avg_similarity":0.9609937200346018},{"project_ids":["1"],"cluster_id":485,"group_ids":[6708605342,6739860069,6745615404,6761282427,6805905105],"issue_titles":["IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"accounts_paymentmethod_stripe_id_key\"\\nDETAIL:  Key (stripe_id)=(pm_0RodopKaD3zFyOgNebPcqWZq) already exists.\\n')","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_externalissue_organization_id_integrat_71094268_uniq\"\\nDETAIL:  Key (organization_id, integration_id, key)=(4507628037865472, 247713, CelestialyXYZ/Website#2) already exists.\\n')","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_externalactor_organization_id_provider_f7ab874d_uniq\"\\nDETAIL:  Key (organization_id, provider, external_name, team_id)=(131127, 200, @Appboy/cdit, 4509708434538496) already exists....","IntegrityError: duplicate key value violates unique constraint \"sentry_grouplink_group_id_73ee52490ebedd34_uniq\"","IntegrityError: UniqueViolation('duplicate key value violates unique constraint \"sentry_grouplink_group_id_73ee52490ebedd34_uniq\"\\nDETAIL:  Key (group_id, linked_type, linked_id)=(6746352884, 1, 3425162884) already exists.\\n')"],"root_cause_summaries":["Duplicate `GroupLink` creation without idempotency check causes `IntegrityError` due to unique database constraint.","Non-idempotent `GroupLink` creation attempts duplicate database insert, violating unique constraint.","Customer's multiple PaymentMethod records cause `get_or_none_for_customer` to retrieve wrong record, leading to delete-then-duplicate-insert error.","Incomplete `get_or_create` lookup parameters cause duplicate `INSERT` attempts, violating database unique constraint.","External issue creation's duplicate check only verifies current group linkage, not global uniqueness enforced by database constraint."],"transactions":["/api/0/teams/{organization_id_or_slug}/{team_id_or_slug}/external-teams/","/api/0/organizations/{organization_id_or_slug}/issues/","/api/0/customers/{organization_id_or_slug}/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.tasks.post_process.post_process_group"],"title":"PostgreSQL unique constraint violations across linking tables","description":"Concurrent or duplicate create paths are inserting existing records into Sentry linking tables (GroupLink, ExternalIssue, ExternalActor) and billing PaymentMethod, triggering PostgreSQL unique constraint violations. Likely missing upsert/get-or-create semantics or race protection causes duplicate inserts during issue resolution, integration callbacks, and customer updates.","tags":["Database","Data Integrity","Concurrency","PostgreSQL","Constraint Violation","Duplicate Insert","Get-Or-Create Missing"],"cluster_size":6,"cluster_min_similarity":0.9363038654466418,"cluster_avg_similarity":0.9562374570035264},{"project_ids":["300688"],"cluster_id":486,"group_ids":[6708652994,6746743252,6750909227],"issue_titles":["ClickhouseError: DB::Exception: Received from snuba-events-analytics-platform-14-3:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 6, 'UInt64' :: 10) -> CAST(round(mult...","QueryException: Code: 70. DB::Exception: Received from snuba-events-analytics-platform-10-2:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 2, 'UInt64' :: 45) -> CAST(...","QueryException: Code: 70. DB::Exception: Received from snuba-events-analytics-platform-10-1:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 5, 'UInt64' :: 11) -> CAST(..."],"root_cause_summaries":["Rust processor's sampling factor rounding to zero causes ClickHouse division by zero, leading to 'inf' to integer conversion error.","Aggressive rounding of small sampling factors to zero causes division by zero in ClickHouse, leading to `inf` conversion errors.","Rust processor's sampling_factor rounding converts small values to zero, causing ClickHouse division-by-zero error."],"transactions":["EndpointTimeSeries__v1","EndpointTraceItemTable__v1"],"title":"ClickHouse NaN/Inf cast in sampling factor math","description":"Queries in Snuba against the events analytics storage fail when rounding and casting a value derived from divide(..., sampling_factor) to UInt64, due to NaN/Inf results. This impacts trace and time-series endpoints that compute sampled counts in ClickHouse.","tags":["Database","Data Integrity","ClickHouse","Type Casting Error","NaN/Inf Value"],"cluster_size":3,"cluster_min_similarity":0.9685382090307085,"cluster_avg_similarity":0.9729202837494509},{"project_ids":["300688"],"cluster_id":487,"group_ids":[6708653003,6724803245,6724835360,6750909224,6750916048],"issue_titles":["ClickhouseError: DB::Exception: Received from snuba-events-analytics-platform-15-1:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 37, 'UInt64' :: 39) -> CAST(round(mul...","ClickhouseError: DB::Exception: Received from snuba-events-analytics-platform-14-3:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 4, 'UInt64' :: 8) -> CAST(round(multi...","ClickhouseError: DB::Exception: Received from snuba-events-analytics-platform-10-1:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 4, 'UInt64' :: 8) -> CAST(round(multi...","QueryException: DB::Exception: Received from snuba-events-analytics-platform-14-3:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 6, 'UInt64' :: 10) -> CAST(round(mult...","QueryException: Code: 70. DB::Exception: Received from snuba-events-analytics-platform-14-2:9000. DB::Exception: Unexpected inf or nan to integer conversion: while executing 'FUNCTION CAST(round(multiply(divide(1, sampling_factor), 1000)) :: 3, 'UInt64' :: 11) -> CAST(..."],"root_cause_summaries":["Redis config override enabled premature `1/sampling_factor` calculation, causing ClickHouse `inf`/`nan` to integer conversion error.","ClickHouse query failed due to `sampling_factor` being zero, causing division by zero and `inf` to `UInt64` cast error.","ClickHouse query fails due to `sampling_factor` being zero/near-zero, causing `inf`/`nan` conversion to `UInt64`.","Floating-point rounding in Rust processor yields zero `sampling_factor`, causing ClickHouse division by zero and `inf` to `UInt64` conversion error.","ClickHouse query fails casting infinity to UInt64 due to `sampling_factor` being zero, causing division by zero."],"transactions":["EndpointTraceItemTable__v1","EndpointTimeSeries__v1"],"title":"ClickHouse cast fails on NaN/Inf in sampling math","description":"Queries against the snuba events analytics platform are casting rounded sampling calculations to UInt64, but the arithmetic can produce NaN/Inf, causing ClickHouse to fail with 'Unexpected inf or nan to integer conversion.' This breaks time-series query execution paths that read through the cache and ClickHouse native driver.","tags":["Database","Data Integrity","ClickHouse","Type Casting","NaN/Infinity"],"cluster_size":5,"cluster_min_similarity":0.9572559491577916,"cluster_avg_similarity":0.9676398183613447},{"project_ids":["1"],"cluster_id":488,"group_ids":[6709178234,6709445557],"issue_titles":["ApiError: {"],"root_cause_summaries":["GitHub secondary rate limit errors, returned as generic `ApiError`, bypass specific rate limit handling, causing task failures.","GitHub integration lacks secondary rate limit handling, causing requests to be forbidden despite primary quota availability, leading to task retries."],"transactions":["sentry.tasks.process_commit_context"],"title":"Commit context fetch forbidden from integration API","description":"Workers requesting blame data for files via the commit-context integration receive 403 Forbidden responses from the upstream API, causing failures while processing commit context. Likely due to missing or insufficient permissions or token scope on the integration installation.","tags":["API","Authorization","External System","Forbidden","Commit Context","HTTP 403"],"cluster_size":2,"cluster_min_similarity":0.9683749123582125,"cluster_avg_similarity":0.9683749123582125},{"project_ids":["1"],"cluster_id":489,"group_ids":[6709187004,6731974214,6737903303,6771660379,6789342363,6805920447,6807580661],"issue_titles":["SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"],"root_cause_summaries":["Slack message block text field empty because AI summary failed and fallback title extraction yielded an empty string for the specific error message.","Event data lacked title-generating metadata, causing Slack message link text to be empty, violating Slack API requirements.","Slack API rejected message; event data lacked title, resulting in empty link text.","Issue's title/metadata empty; `build_attachment_title` returns empty string, causing Slack API `invalid_blocks` error.","Malformed event data, an HTML error page, caused empty title extraction, leading to Slack API rejecting notification due to empty link text.","Slack message link text was empty because title generation failed to produce content, violating Slack API's non-empty text requirement.","Event metadata's 'type' field is empty, overwriting the valid issue title, causing Slack API 'invalid_blocks' error."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.rules.processing.delayed_processing"],"title":"Slack message rejected due to invalid Block Kit text","description":"Slack chat_postMessage fails with invalid_blocks because a Block Kit text element is shorter than the minimum allowed length, breaking issue alert notifications in the SlackNotifyServiceAction path.","tags":["External System","API","Input Validation","Slack","Slack API","Invalid Payload"],"cluster_size":7,"cluster_min_similarity":0.9589631736710167,"cluster_avg_similarity":0.9691204846113273},{"project_ids":["1"],"cluster_id":491,"group_ids":[6709191469,6803477343],"issue_titles":["OutboxFlushError: Could not flush shard category=23 (SUBSCRIPTION_UPDATE)"],"root_cause_summaries":["QuerySubscription deletion task lacks DataSource/Detector cleanup, causing dangling references and subsequent `DoesNotExist` errors during processing.","QuerySubscription deletion lacks DataSource cleanup, causing stale references and OutboxFlushError during workflow engine processing."],"transactions":["sentry.tasks.drain_outbox_shards","tasks.invoices.charge_invoice"],"title":"Outbox flush fails: missing QuerySubscription on SUBSCRIPTION_UPDATE","description":"Outbox shard processing for SUBSCRIPTION_UPDATE raises QuerySubscription.DoesNotExist when workflow engine looks up a subscription by ID, causing OutboxFlushError during invoice/region tasks. Likely a stale or deleted subscription referenced in the outbox payload.","tags":["API","Data Integrity","Queueing","Django","Outbox","DoesNotExist","Subscription Update"],"cluster_size":2,"cluster_min_similarity":0.9729490535434356,"cluster_avg_similarity":0.9729490535434356},{"project_ids":["1"],"cluster_id":493,"group_ids":[6709247172,6709590029,6712751886,6717112532,6750326916,6762410540,6791586536,6792421202],"issue_titles":["ApiError: <!DOCTYPE html>"],"root_cause_summaries":["Atlassian Jira Cloud API returned 'Page Unavailable' HTML with 404, preventing Sentry from creating issues.","Jira API returned 404 with HTML 'Page unavailable' content, causing Sentry's client to raise an `ApiError`.","Atlassian Cloud instance returned 404 HTML 'Page unavailable' for Jira API request, causing Sentry's client to raise ApiError.","Jira API unavailable, returning HTML 404, causing Sentry's integration to fail and report its own failure.","Redis cache failure forced Jira API call during Atlassian outage, causing 404 error in ticket creation.","Jira Cloud instance returned HTML 'Page Unavailable' with 404 status, causing Sentry's API client to raise an ApiError.","Jira API returned 404 for `createmeta` endpoint because the user-configured Jira project ID `10018` is invalid or inaccessible.","Jira Cloud subscription deactivated, causing 503 on metadata request, preventing issue creation."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.rules.processing.delayed_processing","sentry.shared_integrations.client.base in _request"],"title":"Jira issue creation returns 404 in post-processing","description":"During Sentry post-processing callbacks, the Jira integration fails when fetching create metadata for the target project, raising HTTP 404 Not Found and preventing issue creation. Likely due to an invalid/missing Jira project or misconfigured integration/permissions.","tags":["External System","API","Configuration","Jira","HTTP 404","Post-Processing"],"cluster_size":8,"cluster_min_similarity":0.9286559318530196,"cluster_avg_similarity":0.9522214917658455},{"project_ids":["1"],"cluster_id":495,"group_ids":[6709596455,6711717462,6740511846,6784479206],"issue_titles":["ApiInvalidRequestError: {\"message\": \"Invalid Form Body\", \"code\": 50035, \"errors\": {\"embeds\": {\"0\": {\"description\": {\"_errors\": [{\"code\": \"BASE_TYPE_MAX_LENGTH\", \"message\": \"Must be 4096 or fewer in length.\"}]}}}}}","ApiInvalidRequestError: {\"message\": \"Invalid Form Body\", \"code\": 50035, \"errors\": {\"embeds\": {\"_errors\": [{\"code\": \"MAX_EMBED_SIZE_EXCEEDED\", \"message\": \"Embed size exceeds maximum size of 6000\"}]}}}"],"root_cause_summaries":["N+1 query's full SQL statement, used as important evidence, exceeds Discord embed size limit.","Discord embed size limit exceeded due to long SQL query in notification, as Sentry lacked content validation/truncation.","Discord embed description exceeded 6000 characters due to untruncated SQL query from performance issue evidence.","Discord embed description, populated by a long SQL query, exceeded Discord's 4096 character limit due to missing truncation."],"transactions":["sentry.tasks.post_process.post_process_group","sentry.rules.processing.delayed_processing","sentry.issues.tasks.post_process.post_process_group"],"title":"Discord embed payload exceeds API limits","description":"Notifications fail when sending embeds that exceed Discord's maximum size or field length, resulting in 400 Bad Request and Invalid Form Body errors from the Discord API. The notification worker attempts to post messages with oversized embed descriptions or total embed size.","tags":["API","External System","Input Validation","Discord","Bad Request 400","Max Embed Size Exceeded","Field Length Violation"],"cluster_size":4,"cluster_min_similarity":0.9636384209705858,"cluster_avg_similarity":0.9673068536641559},{"project_ids":["1"],"cluster_id":496,"group_ids":[6709682183,6710318713,6782769108,6784842757,6797113037,6803429068,6804191092],"issue_titles":["IntegrationError: Error Communicating with Jira (HTTP 400): Issue 'PM-1101' must be of type 'Epic'.","IntegrationError: Error Communicating with Jira (HTTP 400): Given parent work item does not belong to appropriate hierarchy.","IntegrationError: Error Communicating with Jira Server (HTTP 400): Assignee: The default assignee does NOT have ASSIGNABLE permission OR Unassigned issues are turned off."],"root_cause_summaries":["Jira's createmeta omitted sprint field, causing Sentry to send invalid data, leading to 400 Bad Request.","Jira alert rule misconfigured: non-Epic issue type sent with Epic Link field populated, violating Jira's validation.","Jira's project hierarchy rules rejected issue creation due to incompatible issue type and parent link.","Jira API rejected issue creation due to an invalid parent work item reference in the Sentry alert rule configuration.","Jira project configuration requires assignee, but Sentry's request lacked one, causing API rejection.","Jira rule misconfigured: non-Epic issue type linked to Epic, violating Jira project's workflow constraint.","Jira project configuration requires Epic Link for issue type '10004', but 'PM-1101' is not an Epic."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.shared_integrations.client.base in _request","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/integrations/{integration_id}/","sentry.tasks.post_process.post_process_group"],"title":"Jira issue creation fails due to invalid hierarchy/type","description":"Jira integration returns 400 Bad Request when creating issues: the specified parent requires an Epic issue type or the parent item is not in the correct hierarchy. This affects post-process rule callbacks attempting to auto-create Jira issues.","tags":["External System","API","Configuration","Jira","HTTP 400","Validation Error"],"cluster_size":7,"cluster_min_similarity":0.9185579895656711,"cluster_avg_similarity":0.9502310733905297},{"project_ids":["1"],"cluster_id":497,"group_ids":[6709696802,6799783270],"issue_titles":["IntegrationError: Error Communicating with Azure DevOps (HTTP 404): VS402323: Work item type Microsoft.VSTS.WorkItemTypes.Bug does not exist in project a1b727ac-c1d6-4c10-bbfb-b10fa5d7c6b0 or you do not have permission to access it."],"root_cause_summaries":["Azure DevOps work item type became unavailable between form load and submission, causing a 404 during creation.","Azure DevOps integration failed; configured work item type 'Microsoft.VSTS.WorkItemTypes.Bug' was invalid, causing a 404 API response."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/integrations/{integration_id}/","sentry.issues.tasks.post_process.post_process_group"],"title":"Azure DevOps bug work item type missing or unauthorized","description":"Creating Azure DevOps work items fails because the 'Bug' work item type is not available in the target project or the integration lacks permission, leading to 404/VS402323 errors during create_work_item calls.","tags":["External System","Authorization","API","Azure DevOps","Work Item Type Not Found","HTTP 404"],"cluster_size":2,"cluster_min_similarity":0.9705518853473085,"cluster_avg_similarity":0.9705518853473085},{"project_ids":["1"],"cluster_id":503,"group_ids":[6709881663,6746817432],"issue_titles":["NotificationSettingProvider.MultipleObjectsReturned: get() returned more than one NotificationSettingProvider -- it returned 2!","DetectorState.MultipleObjectsReturned: get() returned more than one DetectorState -- it returned 2!"],"root_cause_summaries":["Duplicate `DetectorState` objects exist for the same detector due to incomplete transactions during creation, violating unique constraint.","Custom `create_or_update`'s flawed `IntegrityError` retry logic creates duplicate records, causing subsequent `get()` calls to fail."],"transactions":["/api/0/users/{user_id}/notification-providers/","/api/0/organizations/{organization_id_or_slug}/alert-rules/{alert_rule_id}/"],"title":"Duplicate rows returned for unique model lookups","description":"GET lookups for NotificationSettingProvider and DetectorState return multiple rows, indicating missing or broken uniqueness constraints on their key fields and causing update flows to fail.","tags":["Data Integrity","Database","Django ORM","Multiple Objects Returned","Uniqueness Violation"],"cluster_size":2,"cluster_min_similarity":0.9537931458267257,"cluster_avg_similarity":0.9537931458267257},{"project_ids":["1"],"cluster_id":505,"group_ids":[6710005939,6731341912],"issue_titles":["ConnectTimeout: SafeHTTPSConnectionPool(host='git.tashkent.uz', port=443): Max retries exceeded with url: /oauth/token (Caused by ConnectTimeoutError(<sentry.net.http.SafeHTTPSConnection object at 0x7876da167c50>, 'Connection to git.tashkent.uz timed out. (connect time...","ConnectTimeout: SafeHTTPSConnectionPool(host='git.veritasprime.com', port=443): Max retries exceeded with url: /oauth/token (Caused by ConnectTimeoutError(<sentry.net.http.SafeHTTPSConnection object at 0x7874dc4e8b90>, 'Connection to git.veritasprime.com timed out. (co..."],"root_cause_summaries":["Sentry's network connection to user-configured GitLab instance `code.qburst.com` timed out, preventing OAuth token exchange.","GitLab instance at git.tashkent.uz unreachable from Sentry, causing TCP connection timeout during OAuth token exchange."],"transactions":["/extensions/{provider_id}/setup/"],"title":"OAuth token exchange to upstream times out","description":"During the OAuth2 access token exchange, HTTP requests to the upstream /oauth/token endpoint are timing out and exhausting retries, causing the pipeline to fail. The failures originate from network connection timeouts to the OAuth provider (e.g., git.tashkent.uz).","tags":["Networking","External System","API","Timeout","Retries Exhausted","OAuth2","HTTPS"],"cluster_size":2,"cluster_min_similarity":0.9774743041496399,"cluster_avg_similarity":0.9774743041496399},{"project_ids":["1"],"cluster_id":507,"group_ids":[6710340351,6726771278,6741288972,6746198160,6747500716,6754106154],"issue_titles":["IntegrationError: There was an error assigning the issue.","ApiInvalidRequestError: {\"errorMessages\":[\"Sentry, vous n'avez pas l'autorisation de commenter ce ticket.\"],\"errors\":{}}","IntegrationError: Error Communicating with Jira (HTTP 403): You do not have permission to create issues in this project.","ApiError: {\"errorMessages\":[\"You do not have permission to assign issues.\"],\"errors\":{}}"],"root_cause_summaries":["Jira API rejected comment creation due to Sentry integration lacking 'Add Comments' permission for FRS-916.","Jira integration lacked 'Assign Issues' permission for project HSS-29749, causing 403 Forbidden on assignee update API call.","Jira integration lacks \"Assign Issues\" permission, causing 403 Forbidden on assignment API call.","Jira integration user lacks \"Assign Issues\" permission for target project, causing API 403 Forbidden.","Jira integration's configured reporter lacks 'Create Issue' permission in target project, causing 403 Forbidden.","Jira integration lacked 'Assign Issues' permission, causing 403 Forbidden on assignment API call."],"transactions":["sentry.integrations.tasks.sync_assignee_outbound","sentry.integrations.tasks.create_comment","sentry.tasks.post_process.post_process_group"],"title":"Jira integration lacks permissions for issue ops","description":"Sentrys Jira integration is receiving 403/400 responses from Jira when creating issues, assigning assignees, and posting comments due to insufficient project permissions on the Jira side. This blocks automated issue creation and synchronization workflows.","tags":["External System","Authorization","API","Jira","Forbidden","Bad Request","Permission Denied"],"cluster_size":6,"cluster_min_similarity":0.9332501269397377,"cluster_avg_similarity":0.9607229970159931},{"project_ids":["1"],"cluster_id":509,"group_ids":[6710772038,6711391614,6718026324,6810188157],"issue_titles":["ApiError: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/issues/comments#update-an-issue-comment\",\"status\":\"404\"}"],"root_cause_summaries":["Sentry attempted to update a GitHub PR comment that no longer existed, causing a 404 API error.","Sentry's stale database record for a GitHub comment led to attempts to update a non-existent comment, causing a 404 API error.","Sentry's database held a stale GitHub comment ID, causing a 404 when attempting to update a non-existent comment.","Sentry's stale GitHub PR comment ID caused 404 API error, unhandled by integration's specific error checks, leading to task failure."],"transactions":["sentry.integrations.source_code_management.tasks.pr_comment_workflow"],"title":"PR comment update returns 404 from upstream API","description":"Celery task pr_comment_workflow fails while calling update_pr_comment, as the upstream API responds Not Found when PATCHing an existing PR comment. Likely the external comment ID is stale or the resource was deleted/never created, causing repeated 404s during comment updates.","tags":["API","External System","Queueing/Messaging","HTTP 404","Celery","Pull Request Comments"],"cluster_size":4,"cluster_min_similarity":0.9678744268471328,"cluster_avg_similarity":0.9733893807044484},{"project_ids":["1"],"cluster_id":510,"group_ids":[6711129367,6725597341,6793866834,6802974693],"issue_titles":["ApiError: {\"errorMessages\":[\"Issue Does Not Exist\"],\"errors\":{}}","ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}"],"root_cause_summaries":["Sentry's stale Jira issue references cause 404s when syncing, as external issues were deleted or permissions changed.","Sentry's `ExternalIssue` references a Jira issue that is deleted or inaccessible, causing 404 `ApiError` during status sync.","Jira issue deleted externally, leaving orphaned Sentry `ExternalIssue` record, causing sync task to fail on 404.","Sentry's Jira sync task fails due to stale `ExternalIssue` data, lacking specific handling for Jira's 404 \"Issue Does Not Exist\" response."],"transactions":["sentry.integrations.tasks.sync_status_outbound"],"title":"Jira issue lookup fails during status sync","description":"Outbound status sync attempts to fetch Jira issues that either no longer exist or are inaccessible, leading to 404 errors from the Jira API and task failures in sync_status_outbound.","tags":["External System","API","Authorization","Jira","Not Found","SyncStatusOutbound"],"cluster_size":4,"cluster_min_similarity":0.9542515928850637,"cluster_avg_similarity":0.9642851325875713},{"project_ids":["1"],"cluster_id":513,"group_ids":[6712091471,6793117883],"issue_titles":["RuleDataError: failed to find rule action dda0213d-2ecd-4863-a8ec-db9b45fc4848 for rule 15027951","RuleDataError: failed to find rule action 2d8b997e-e49e-4fe7-964e-fa8486f5f7a9 for rule 15835493"],"root_cause_summaries":["Rule action UUID in notification message references a non-existent action in the rule's updated configuration, causing lookup failure.","NotificationMessage rule_action_uuid references non-existent rule action due to rule data modification."],"transactions":["sentry.integrations.slack.tasks.send_activity_notifications_to_slack_threads"],"title":"Missing rule action data breaks Slack thread notifications","description":"Slack notification service fails to resolve a rule's action ID when deriving the parent channel/thread, raising RuleDataError and halting thread notifications. The issue appears during send_activity_notifications_to_slack_threads and is reported via Sentry.","tags":["Configuration","API","External System","Slack","Sentry","Missing Resource","RuleDataError"],"cluster_size":2,"cluster_min_similarity":0.9773263966262199,"cluster_avg_similarity":0.9773263966262199},{"project_ids":["1"],"cluster_id":515,"group_ids":[6712307599,6729439110,6735647515,6794551146],"issue_titles":["OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 2491316 waits for ShareLock on transaction 1674286961; blocked by process 2884909.\\nProcess 2884909 waits for ShareLock on transaction 1674286959; blocked by process 2491316.\\nHINT:  See server log f...","IntegrityError: ExclusionViolation('conflicting key value violates exclusion constraint \"exclude_overlapping_date_start_end\"\\nDETAIL:  Key (group_id, tstzrange(date_started, date_ended, \\'[]\\'::text))=(9566833, [\"2025-07-03 00:08:35.930104+00\",)) conflicts with existin...","OperationalError: deadlock detected","OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 486754 waits for ShareLock on transaction 1737913657; blocked by process 1107854.\\nProcess 1107854 waits for ShareLock on transaction 1737913656; blocked by process 486754.\\nHINT:  See server log for..."],"root_cause_summaries":["Concurrent non-atomic `GroupOpenPeriod` creation for same group causes database exclusion constraint deadlock.","Concurrent `create_open_period` calls for same group cause deadlock due to non-atomic check and `ExclusionConstraint` violation.","Concurrent event processing creates duplicate open periods for the same group, violating a database exclusion constraint.","Concurrent `GroupOpenPeriod` creation for same group, due to non-atomic check-then-act, violates exclusion constraint, causing database deadlock."],"transactions":["ingest_consumer.process_event","sentry.tasks.store.save_event"],"title":"PostgreSQL deadlocks on GroupOpenPeriod exclusion constraint","description":"Concurrent inserts into sentry_groupopenperiod for the same group create overlapping tstzrange intervals, causing exclusion constraint checks to deadlock and sometimes raise exclusion violations. This occurs during regression handling when creating open periods.","tags":["Database","Concurrency","Data Integrity","PostgreSQL","Deadlock","Exclusion Constraint","sentry_groupopenperiod"],"cluster_size":4,"cluster_min_similarity":0.9523322558118774,"cluster_avg_similarity":0.9693195988629594},{"project_ids":["11276"],"cluster_id":516,"group_ids":[6712517231,6763180599],"issue_titles":["Error: Cannot find module './native/switch'","Error: Cannot find module './console/nintendo-switch'"],"root_cause_summaries":["Project's platform ID 'switch' is invalid, causing dynamic module import for 'console/switch' to fail.","Nintendo Switch platform misconfiguration prevents `getPlatformPath` from generating the correct documentation module path."],"transactions":["/explore/replays/","/getting-started/:projectId/:platform/"],"title":"Dynamic import fails: missing Nintendo Switch modules","description":"Client-side dynamic imports for Nintendo Switch-related modules ('./native/switch' and './console/nintendo-switch') are failing at runtime because the modules are not present in the bundle or resolution path.","tags":["Configuration","Build/Packaging","Module Resolution","JavaScript","Dynamic Import","Cannot Find Module"],"cluster_size":2,"cluster_min_similarity":0.9612675296010884,"cluster_avg_similarity":0.9612675296010884},{"project_ids":["1"],"cluster_id":520,"group_ids":[6712838624,6773426711,6803811392],"issue_titles":["IntegrationError: Unable to retrieve repositories. Please try again later.","ApiError: {\"message\":\"404 Group Not Found\"}"],"root_cause_summaries":["GitLab group (ID 69072888) became inaccessible after integration setup, causing 404 on API calls for repositories.","GitLab integration failed; configured group ID 69072888 not found via API, causing repository retrieval to error.","GitLab group ID in Sentry integration metadata is invalid, causing GitLab API to return 404 when fetching repositories."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/integrations/{integration_id}/"],"title":"Repository lookup fails due to missing group","description":"Create-issue flow calls the SCM integrations project search with an invalid or missing Sentry group, leading the upstream client to return 404 and propagate as IntegrationError/ApiError. This breaks repository selection in Group Integration Details.","tags":["API","External System","Configuration","Upstream Unavailable","404 Not Found","IntegrationError"],"cluster_size":3,"cluster_min_similarity":0.9666483884292755,"cluster_avg_similarity":0.9724809291300601},{"project_ids":["1"],"cluster_id":521,"group_ids":[6712854024,6713042870],"issue_titles":["RuntimeError: dictionary changed size during iteration"],"root_cause_summaries":["Buffer processing overwhelmed by backlog, exceeding deadline, causing concurrent dictionary modification during cleanup.","Undersized `incr_batch_size` (2) for `RedisBuffer` caused excessive task dispatches, leading to `process_pending` timeout and subsequent cleanup `RuntimeError`."],"transactions":["sentry.buffer.redis in process_pending"],"title":"Process buffer task exceeds deadline and mutates tags dict","description":"The sentry.tasks.process_buffer.process_pending job in the Redis-backed process buffer exceeds its execution deadline, and concurrent metrics emission attempts to build a tags dictionary during iteration, triggering 'dictionary changed size during iteration'. This likely stems from mutating metric tags while iterating under deadline pressure in the worker process.","tags":["Concurrency","Queueing","Configuration","Redis","Processing Deadline Exceeded","Dictionary Mutated During Iteration","Metrics Tags"],"cluster_size":2,"cluster_min_similarity":0.9738714484673993,"cluster_avg_similarity":0.9738714484673993},{"project_ids":["1"],"cluster_id":525,"group_ids":[6713625091,6803156842],"issue_titles":["ExportError: Internal error. Please try again."],"root_cause_summaries":["EventUser.for_projects generates oversized Snuba queries for large user ID lists, exceeding ClickHouse's query size limit.","EventUser query's large IN clauses from numerous user tags exceed ClickHouse's query size limit."],"transactions":["sentry.data_export.tasks.assemble_download"],"title":"Snuba query exceeds ClickHouse size limit in issue export","description":"Issues-by-tag export builds an oversized Snuba query, triggering ClickHouse max-query-size enforcement and bubbling up as ExportError. This prevents assembling download fragments for data exports.","tags":["Database","API","Serialization","ClickHouse","Snuba","Query Size Limit","Export"],"cluster_size":2,"cluster_min_similarity":0.9747299460793613,"cluster_avg_similarity":0.9747299460793613},{"project_ids":["1"],"cluster_id":526,"group_ids":[6713727526,6713727616,6771657191,6782675336,6784043738,6789207121,6789207122,6789207170,6789207180,6789207181,6789207184,6789207190,6789207242,6789207243,6789207348,6789207375,6789207422,6789207424,6791193961,6803836799],"issue_titles":["ReadTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Read timed out. (read timeout=6)","ReadTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Read timed out. (read timeout=6)","ReadTimeoutError: HTTPConnectionPool(host='seer-web-breakpoint', port=80): Read timed out. (read timeout=5)","ReadTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Read timed out. (read timeout=6)","ReadTimeoutError: HTTPConnectionPool(host='seer-web-severity', port=80): Read timed out. (read timeout=0.6)"],"root_cause_summaries":["Symbolicator service unresponsive, causing all JavaScript symbolication requests to time out after 6 seconds, exhausting retries.","Sentry's 0.6s HTTP read timeout is too short for Seer's variable, computationally intensive ML model inference.","Seer's autofixability model loads on-demand, exceeding Sentry's short 0.6s read timeout.","JavaScript symbolicator service consistently times out (6s) due to unresponsiveness, exhausting all retries for symbolication requests.","Seer's ML model processing exceeds Sentry's 600ms timeout, causing ReadTimeoutError.","Sentry's 6-second polling timeout is too short for symbolicator tasks requiring 30 seconds, causing premature ReadTimeouts.","Symbolicator service times out fetching external sourcemaps, causing Sentry's symbolication task to fail after retries.","JavaScript Symbolicator service timed out due to unresponsiveness, likely from large payloads or slow external source map fetching.","GPU-enabled Seer service times out due to aggressive 0.6s read timeout, preventing autofix.","6-second symbolicator timeout is insufficient for large iOS app payloads requiring external symbol lookups, causing repeated ReadTimeout errors.","Symbolicator's 6-second timeout for external resource fetching is insufficient, causing repeated read timeouts and symbolication failure.","Symbolicator-JVM service times out processing complex ANR event with ProGuard deobfuscation.","Seer's CUSUM algorithm, processing complex time series data, exceeds Sentry's 5-second HTTP read timeout, causing task failure.","Seer service's occasional slowness exceeds the aggressive 0.6s read timeout, causing `ReadTimeoutError`.","Symbolicator's external symbol fetching from Microsoft/NuGet exceeds 6-second timeout, causing Sentry's request to fail.","Symbolicator service exceeds 6-second processing timeout for complex JavaScript payloads, causing all retries to fail.","HTTP client's 6-second read timeout is too short for Symbolicator's 5-second processing, causing premature connection closure.","Symbolicator's processing exceeds Sentry's tight 6-second HTTP read timeout, causing premature connection closure.","Symbolicator-js service unresponsiveness causes read timeouts, leading to profile processing task deadline overruns.","Symbolicator service times out processing large React Native stack traces, exceeding 6-second read limit, causing task failure."],"transactions":["sentry.tasks.store.symbolicate_event","sentry.tasks.symbolicate_js_event","sentry.tasks.activity.send_activity_notifications","sentry.tasks.statistical_detectors.detect_transaction_change_points","sentry.issues.tasks.post_process.post_process_group","sentry.tasks.autofix.start_seer_automation","sentry.tasks.symbolicate_jvm_event","sentry.profiles.task.process_profile"],"title":"Seer API requests time out from automation and detectors","description":"Celery workers calling make_signed_seer_api_request to the Seer service are hitting HTTP read timeouts, impacting issue summaries, fixability scoring, and change-point detection workflows.","tags":["Networking","API","External System","Timeout","HTTP","Celery","Seer"],"cluster_size":20,"cluster_min_similarity":0.9092749129899638,"cluster_avg_similarity":0.9493329881242115},{"project_ids":["1"],"cluster_id":532,"group_ids":[6717787527,6725769707,6732801993,6810741013],"issue_titles":["ApiInvalidRequestError: Invalid routing key"],"root_cause_summaries":["PagerDuty provided a routing key with trailing whitespace; Sentry stored and used it unsanitized, causing API rejection.","PagerDuty's API returns integration keys with trailing whitespace, which Sentry stores and uses unsanitized, causing API request failures.","PagerDuty integration keys with trailing whitespace, provided by PagerDuty, are stored unsanitized, causing API request failures.","PagerDuty integration key stored with trailing space, causing API rejection when used as routing key."],"transactions":["sentry.incidents.tasks.handle_trigger_action","getsentry.tasks.quotas.deactivate_db_spike"],"title":"PagerDuty alerts fail due to invalid routing key","description":"Requests to the PagerDuty API to trigger on-call alerts return 400 Bad Request because the routing key in the payload is invalid, causing incident and spike protection notifications to fail.","tags":["External System","API","Configuration","PagerDuty","Bad Request","Invalid Routing Key"],"cluster_size":4,"cluster_min_similarity":0.9644285591642795,"cluster_avg_similarity":0.9712714610487755},{"project_ids":["1"],"cluster_id":537,"group_ids":[6718559205,6718762188,6719158348,6725638899,6726633450,6726980228,6729257809,6729973974,6740674548,6745290214,6748138308,6795857245],"issue_titles":["IntegrationError: Error Communicating with Jira (HTTP 400): An Assignee is required for all finished work.","IntegrationError: Error Communicating with Jira (HTTP 400): Please specify the branch where you are submitting a fix. If you are resolving this as CNR/WNF, please use \"--No Branch--\"","IntegrationError: Error Communicating with Jira (HTTP 400): You should define estimation (Story Points).","IntegrationError: Error Communicating with Jira (HTTP 400): You must select the current or a future sprint before reopen this issue","ApiInvalidRequestError: {\"errorMessages\":[\"An unknown exception occured executing Validator com.atlassian.jira.workflow.SkippableValidator@122f72f6: root cause: java.lang.NullPointerException: Cannot invoke \\\"com.atlassian.jira.issue.fields.ConfigurableField.getId()\\\" because ...","IntegrationError: Error Communicating with Jira (HTTP 400): Please provide a justification for dropping the issue.","IntegrationError: Error Communicating with Jira (HTTP 400): Please update story points (enter 0 if 0) Kindly enter the Dev Owner"],"root_cause_summaries":["Jira workflow requires work log for status transition, but Sentry's integration does not provide it.","Jira workflow requires justification for status transition, but Sentry's integration sends only transition ID, causing API rejection.","Jira workflow validator failed due to Sentry's incomplete transition request, omitting a required field.","Jira integration fails because Sentry's transition request lacks required workflow fields, like 'Original estimate', mandated by Jira's configuration.","Jira transition requires 'Original estimate' field; Sentry's integration only sends transition ID, causing 400 Bad Request.","Jira workflow requires mandatory fields for status transitions; Sentry's integration only sends transition ID, causing 400 Bad Request.","Jira workflow requires assignee for status transition, but Sentry's sync only sends transition ID.","Jira workflow requires assignee for 'done' status; Sentry's status sync omits assignee in transition request.","Jira workflow validation requires 'Story Points' for status transition; Sentry's request omits this field.","Jira integration fails to transition issue because it omits workflow-required fields, only sending the transition ID.","Jira workflow requires justification for status transition, but Sentry's request lacks this field.","Jira workflow requires 'Story Points' for status transition, but Sentry's fixed payload omits it, causing HTTP 400."],"transactions":["sentry.integrations.tasks.sync_status_outbound","sentry.shared_integrations.client.base in _request"],"title":"Jira transition blocked by required custom fields","description":"Outbound status sync to Jira fails with HTTP 400 when transitioning issues because required fields (Story Points, Dev Owner, Branch, Drop Justification) are missing. The Jira API returns ApiInvalidRequestError, causing IntegrationError in sync_status_outbound.","tags":["External System","API","Configuration","Jira","HTTP 400","Validation Error","IntegrationError"],"cluster_size":12,"cluster_min_similarity":0.9428786245815314,"cluster_avg_similarity":0.9654308410595949},{"project_ids":["1"],"cluster_id":539,"group_ids":[6719007322,6755304100],"issue_titles":["Exception: Seer API error: 503"],"root_cause_summaries":["Non-GPU Seer API 503 errors cause hard failures due to inconsistent error handling, preventing autofix, unlike graceful GPU path.","Seer severity service unavailability caused autofix automation failure due to lack of 5xx error handling."],"transactions":["sentry.tasks.autofix.start_seer_automation"],"title":"Seer API returns non-200 causing automation failure","description":"Calls to the Seer API during fixability score generation return an error status, causing issue summary automation to raise and fail in worker processes.","tags":["API","External System","Configuration","Seer","Non-200 Response"],"cluster_size":2,"cluster_min_similarity":0.9727721324315575,"cluster_avg_similarity":0.9727721324315575},{"project_ids":["11276"],"cluster_id":545,"group_ids":[6719887792,6781200689],"issue_titles":["<unknown>","EvalError: Refused to evaluate a string as JavaScript because 'unsafe-eval' is not an allowed source of script in the following Content Security Policy directive: \"script-src 'self' 'unsafe-inline' 'report-sample' s1.sentry-cdn.com js.sentry-cdn.com browser.sentry..."],"root_cause_summaries":["Pendo's dynamic script evaluation violates production CSP's `script-src` directive, which disallows `unsafe-eval` for security.","Kaspersky antivirus injects `eval()`-using JavaScript, violating Sentry's strict `script-src` Content Security Policy."],"transactions":["/issues/:groupId/","/settings/account/details/"],"title":"CSP blocks unsafe-eval in browser JavaScript","description":"Browser execution fails with EvalError because the current Content Security Policy disallows unsafe-eval in script-src, preventing code that relies on eval/new Function. Update CSP or remove eval usage in the affected frontend code.","tags":["Configuration","Security","API","Content Security Policy","Unsafe-Eval","Browser JavaScript"],"cluster_size":2,"cluster_min_similarity":0.9502161116378999,"cluster_avg_similarity":0.9502161116378999},{"project_ids":["1"],"cluster_id":551,"group_ids":[6720444326,6720539456,6791455908],"issue_titles":["SentryAppSentryError: event_not_in_servicehook"],"root_cause_summaries":["ServiceHook events list out of sync with SentryApp events due to failed asynchronous updates, exacerbated by rate limiting.","ServiceHook cache not invalidated after event subscription updates, causing `event_not_in_servicehook` error from stale data.","ServiceHook's events list is inconsistent with Sentry App's configured events, likely due to stale cache or incomplete updates."],"transactions":["sentry.sentry_apps.tasks.sentry_apps in send_webhooks","sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"],"title":"Sentry app webhook event not in service hook","description":"Webhook dispatch for Sentry Apps fails with SentryAppSentryError 'event_not_in_servicehook' when sending resource change events, indicating the event type isnt configured on the apps service hook.","tags":["API","Configuration","Sentry","Webhooks","Invalid Event Type"],"cluster_size":3,"cluster_min_similarity":0.9407495652585571,"cluster_avg_similarity":0.9589094371631536},{"project_ids":["1"],"cluster_id":553,"group_ids":[6721080411,6727918432],"issue_titles":["ApiError: {\"code\":40301,\"message\":\"To perform this action, use an API key from an API integration.\",\"took\":0.001,\"requestId\":\"6c307443-1c1b-436b-ae0d-6e9cc6e1261b\"}","ApiError: {\"code\":40301,\"message\":\"To perform this action, use an API key from an API integration.\",\"took\":0.0,\"requestId\":\"63220428-ead0-42f0-86aa-3acdebe56a11\"}"],"root_cause_summaries":["Opsgenie API key lacks required 'API integration' type for alert creation, causing 403 Forbidden.","Opsgenie API key lacks 'API integration' permissions for alert creation, causing 403 Forbidden on POST /alerts."],"transactions":["sentry.incidents.tasks.handle_trigger_action","query_subscription_consumer_process_message"],"title":"Metric alert notifications forbidden due to missing API integration key","description":"Metric alert actions (fire/resolve) fail when posting to /alerts because the request uses a non-integration API key, resulting in 403 Forbidden. Notifications are not sent until an API Integration key is configured and used by the client.","tags":["API","Authorization","Configuration","HTTP 403","Metric Alerts"],"cluster_size":2,"cluster_min_similarity":0.9666491303126427,"cluster_avg_similarity":0.9666491303126427},{"project_ids":["1"],"cluster_id":559,"group_ids":[6721678267,6738388353,6793930647],"issue_titles":["ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}"],"root_cause_summaries":["Sentry's stale ExternalIssue reference to a deleted/inaccessible Jira ticket causes repeated 404 API errors during comment creation.","Sentry's `ExternalIssue` for Jira issue `CDP-9790` is stale; Jira returned 404, indicating the issue is inaccessible or deleted.","Sentry's `ExternalIssue` record for 'SP-8427' is stale; Jira issue inaccessible, causing 404 during comment creation."],"transactions":["sentry.integrations.tasks.create_comment"],"title":"Jira comment creation fails with 404/permission error","description":"Creating a comment via the Jira integration returns 404 with message 'Issue does not exist or you do not have permission to see it,' indicating the target issue key is invalid or the app/user lacks access. This blocks automated comment posting from the worker task.","tags":["External System","API","Authorization","Jira","Not Found","Permission Denied"],"cluster_size":3,"cluster_min_similarity":0.9664107826453717,"cluster_avg_similarity":0.9709490740094577},{"project_ids":["1"],"cluster_id":838,"group_ids":[6722475317,6722475326,6780390008],"issue_titles":["TimeoutError: Timeout reading from socket","ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')"],"root_cause_summaries":["Redis socket closed prematurely by external system; subsequent cleanup attempt times out on already-closed connection.","Redis connection pool reuses stale connections, causing `ConnectionResetError` during cleanup when peer already closed socket.","Redis client attempts cleanup on stale connection, causing TimeoutError due to closed socket, indicating insufficient connection health checks."],"transactions":["sentry.tasks.store.save_event_transaction"],"title":"Redis socket timeouts and connection resets during key deletes","description":"Worker processes deleting processing-store keys in Redis are hitting socket read timeouts and occasional connection resets, causing task failures during save_event_transaction. This indicates Redis latency/instability or network issues between workers and the Redis cluster.","tags":["Networking","Caching","API","Redis","Timeout","Connection Reset","Key Deletion"],"cluster_size":3,"cluster_min_similarity":0.9539800719143368,"cluster_avg_similarity":0.9622292847278128},{"project_ids":["6178942"],"cluster_id":574,"group_ids":[6723171772,6723171802],"issue_titles":["MaxIterationsReachedException: Agent Agent reached maximum iterations without finishing."],"root_cause_summaries":["Empty repository directory causes ripgrep timeouts, preventing agent progress, leading to max iterations reached.","Agent failed due to GitHub App installation missing for repository, preventing code access and causing iteration exhaustion."],"transactions":["seer.automation.codegen.pr_review_step.pr_review_task"],"title":"PR review agent loops until max iterations reached","description":"The PR review agent in pr_review_coding_component repeatedly iterates without converging, causing MaxIterationsReachedException in agent.run and failing the review step. This indicates an unbounded loop or missing termination condition in the agent workflow.","tags":["Configuration","Concurrency","Agent Framework","Max Iterations Reached"],"cluster_size":2,"cluster_min_similarity":0.9523458320855825,"cluster_avg_similarity":0.9523458320855825},{"project_ids":["1"],"cluster_id":575,"group_ids":[6723347604,6794698261,6794699743,6794705106],"issue_titles":["UnknownOption: 'feature.projects:ourlogs-breadcrumb-extraction'","UnknownOption: 'feature.organizations:user-feedback-ai-titles'"],"root_cause_summaries":["Database option `feature.organizations:user-feedback-ai-titles` lacks application registry definition, causing `UnknownOption` during sync.","Database contains 'feature.projects:ourlogs-breadcrumb-extraction' option not registered in application code, causing `UnknownOption` during sync.","Feature flag stored in options table, processed by option sync task, causing lookup failure in option registry.","Feature flag incorrectly stored in options database, causing sync task to fail when option not found in registry."],"transactions":["sentry.tasks.options.sync_options"],"title":"Unknown option keys during options sync","description":"Workers syncing feature flags/options fail when looking up missing keys like 'feature.projects:ourlogs-breadcrumb-extraction' and 'feature.organizations:user-feedback-ai-titles', causing UnknownOption/KeyError in the options manager. Likely a configuration drift where code references flags not registered in the options registry.","tags":["Configuration","API","Feature Flags","Unknown Option","KeyError"],"cluster_size":4,"cluster_min_similarity":0.9474941006329821,"cluster_avg_similarity":0.9663146275674349},{"project_ids":["300688"],"cluster_id":576,"group_ids":[6723442126,6723464230,6723464264,6723464276,6723480609,6723500449,6723516109,6724388292,6724389622,6724389642,6776943181,6776943247,6792369919],"issue_titles":["ClickhouseError: DB::Exception: Attempt to read after eof: while receiving packet from snuba-errors-tiger-mz-3-5:9000: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Attempt to read after eof: while receiving packet from snuba-errors-tiger-mz-4-5:9000: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Attempt to read after eof: while receiving packet from snuba-errors-tiger-mz-2-6:9000: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Attempt to read after eof: while receiving packet from snuba-events-analytics-platform-7-3:9000: While executing Remote. Stack trace:","QueryException: Code: 32. DB::Exception: Attempt to read after eof: while receiving packet from snuba-events-analytics-platform-10-2:9000: While executing Remote.\"","QueryException: DB::Exception: Attempt to read after eof: while receiving packet from snuba-events-analytics-platform-7-3:9000: While executing Remote. Stack trace:","ClickhouseError: DB::Exception: Attempt to read after eof: while receiving packet from snuba-errors-tiger-mz-5-5:9000: While executing Remote. Stack trace:","QueryException: Code: 32. DB::Exception: Attempt to read after eof: while receiving packet from snuba-events-analytics-platform-1-3:9000: While executing Remote.\""],"root_cause_summaries":["ClickHouse distributed query failed: shard connection terminated prematurely, causing coordinator to return incomplete data.","ClickHouse server exhausted memory processing complex trace query, causing connection termination and client-side 'read after eof' error.","ClickHouse server prematurely terminated connection during complex query execution, exhausting Snuba's retries due to likely resource exhaustion.","ClickHouse cluster overload causes connection termination during query execution, leading to client-side EOF errors.","ClickHouse cluster overload causes inter-node connection termination during distributed query execution, leading to client-side EOF errors.","ClickHouse distributed query failed: remote shard unexpectedly closed connection to coordinator, causing read-after-eof error.","Network instability causes premature ClickHouse connection termination during data transfer, leading to 'Attempt to read after eof' errors.","A ClickHouse shard failed processing a resource-intensive distributed query, prematurely closing its connection, causing the coordinating node to report EOF.","ClickHouse remote node prematurely closed connection during complex distributed query execution, exhausting Snuba's retries.","ClickHouse server `snuba-errors-tiger-mz-3-4:9000` prematurely closed connection during query result transmission, exhausting Snuba's retries.","Remote ClickHouse node exhausted resources processing complex query, causing connection termination and client-side EOF error.","ClickHouse server's internal distributed query failure, not retried by Snuba's client due to specific error code.","ClickHouse server `snuba-errors-tiger-mz-3-5:9000` prematurely terminated connection during query execution, exhausting Snuba's retries."],"transactions":["EndpointTraceItemTable__v1","snql_dataset_query_view__events__Group.get_latest","snql_dataset_query_view__events__serializers.GroupSerializerSnuba._execute_error_seen_stats_query","EndpointGetTrace__v1","EndpointTraceItemAttributeNames__v1","snql_dataset_query_view__events__api.group-hashes","snql_dataset_query_view__events__tagstore._get_tag_keys_and_top_values","snql_dataset_query_view__discover__eventstore.get_next_or_prev_event_id","[cli init] subscriptions-executor","snql_dataset_query_view__events__tagstore.__get_tag_key_and_top_values","EndpointTimeSeries__v1"],"title":"ClickHouse EOF while receiving remote packets","description":"Queries to ClickHouse fail with 'Attempt to read after EOF' when executing Remote reads from the snuba-errors cluster, indicating the remote shard/connection closes unexpectedly. This disrupts readthrough-cached query execution in the db_query/native client path.","tags":["Database","Networking","ClickHouse","Snuba","EOF","Remote Query","Connection Reset"],"cluster_size":13,"cluster_min_similarity":0.9222751367135011,"cluster_avg_similarity":0.9555838661782582},{"project_ids":["1"],"cluster_id":578,"group_ids":[6723602444,6799839824],"issue_titles":["OutboxDatabaseError: Failed to process Outbox, TEAM_UPDATE due to database error"],"root_cause_summaries":["Team deletion task exceeds its 900-second deadline, causing the worker to cancel the ongoing database query, leading to an OutboxDatabaseError.","Synchronous outbox processing within deletion tasks exceeds task deadline, causing query cancellation and database errors."],"transactions":["sentry.deletions.tasks.run_deletion"],"title":"PostgreSQL cancels FOR UPDATE on Outbox shard","description":"Outbox draining for TEAM_UPDATE is failing because SELECT ... FOR UPDATE on sentry_regionoutbox is being canceled by the database, interrupting deletion tasks that enqueue and process outbox rows.","tags":["Database","Concurrency","PostgreSQL","Query Canceled","SELECT FOR UPDATE","Outbox"],"cluster_size":2,"cluster_min_similarity":0.965318962064555,"cluster_avg_similarity":0.965318962064555},{"project_ids":["300688"],"cluster_id":581,"group_ids":[6724388259,6724388284],"issue_titles":["ClickhouseError: DB::NetException. DB::NetException: Connection reset by peer, while reading from socket (192.168.209.152:9000): while receiving packet from snuba-events-analytics-platform-7-3:9000: While executing Remote. Stack trace:","QueryException: Code: 32. DB::Exception: Attempt to read after eof: while receiving packet from snuba-events-analytics-platform-13-3:9000: While executing Remote.\""],"root_cause_summaries":["ClickHouse query exceeded 25-second execution limit, causing server to reset connection, leading to client-side network error.","ClickHouse query exceeded 25-second execution limit, causing server to close connection, leading to client's 'read after eof' error."],"transactions":["EndpointTraceItemTable__v1","EndpointGetTrace__v1"],"title":"ClickHouse remote queries reset/EOF from Snuba nodes","description":"Remote ClickHouse queries to snuba-events analytics shards are failing with connection resets and EOF while reading packets, causing query pipeline errors in db_query/native execution paths.","tags":["Networking","Database","External System","Connection Reset","EOF","ClickHouse","Snuba"],"cluster_size":2,"cluster_min_similarity":0.9567506243429068,"cluster_avg_similarity":0.9567506243429068},{"project_ids":["1"],"cluster_id":585,"group_ids":[6724996720,6724996740,6730822913,6750534466,6796235882],"issue_titles":["IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401232: Work item 1424 does not exist, or you do not have permissions to read it.","ApiError: {\"$id\":\"1\",\"innerException\":null,\"message\":\"TF401232: Work item 1224 does not exist, or you do not have permissions to read it.\",\"typeName\":\"Microsoft.TeamFoundation.WorkItemTracking.Server.WorkItemUnauthorizedAccessException, Microsoft.TeamFoundation.W...","ApiError: {\"$id\":\"1\",\"innerException\":null,\"message\":\"TF401232: Work item 19419 does not exist, or you do not have permissions to read it.\",\"typeName\":\"Microsoft.TeamFoundation.WorkItemTracking.Server.WorkItemUnauthorizedAccessException, Microsoft.TeamFoundation....","IntegrationError: Error Communicating with Azure DevOps (HTTP 404): TF401232: Work item 6363 does not exist, or you do not have permissions to read it."],"root_cause_summaries":["Azure DevOps work item 154689 does not exist or is inaccessible, causing Sentry's sync task to fail.","Sentry's `ExternalIssue` references a non-existent Azure DevOps work item, causing repeated 404 errors during status synchronization attempts.","Sentry attempts to update a non-existent Azure DevOps work item due to missing pre-validation, causing API errors.","Sentry's database holds a stale reference to an Azure DevOps work item that no longer exists or is inaccessible, causing sync failures.","Sentry's external issue reference is stale; VSTS work item 1224 does not exist, causing sync task to fail after retries."],"transactions":["sentry.integrations.tasks.sync_status_outbound","sentry.integrations.tasks.create_comment"],"title":"Azure DevOps work item missing or unauthorized","description":"Outbound status sync and comment updates to Azure DevOps fail when fetching or updating a work item returns TF401232, indicating the item does not exist or the integration lacks permissions. Impact: status and comments cannot be synchronized for affected items.","tags":["External System","API","Authorization","Azure DevOps","HTTP 404","WorkItemUnauthorizedAccessException","TF401232"],"cluster_size":5,"cluster_min_similarity":0.948815670781813,"cluster_avg_similarity":0.9630562821177483},{"project_ids":["6178942"],"cluster_id":588,"group_ids":[6725292018,6732249561,6759740627,6759756370],"issue_titles":["ConnectTimeout: HTTPSConnectionPool(host='overwatch.codecov.io', port=443): Max retries exceeded with url: /api/github/installation/check (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7a0024141350>, 'Connection to overwatch.codecov.io t...","ConnectTimeout: HTTPSConnectionPool(host='api.github.com', port=443): Max retries exceeded with url: /repos/woodyhayday/Strew/installation (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7b1ea4955e90>, 'Connection to api.github.com timed ...","ConnectTimeout: HTTPSConnectionPool(host='api.github.com', port=443): Max retries exceeded with url: /repos/Geeky-Medics/geeky_quiz/installation (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7804014f4690>, 'Connection to api.github.com ...","ConnectTimeout: HTTPSConnectionPool(host='api.github.com', port=443): Max retries exceeded with url: /repos/rockhallweb/snl-video-browser/installation (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7e294dacce50>, 'Connection to api.githu..."],"root_cause_summaries":["TCP connection to api.github.com failed due to network timeout, preventing GitHub App authentication.","GitHub API connection timed out after 15s during repo access check, indicating network connectivity issues.","Network connection to GitHub API timed out, likely due to lingering infrastructure issues from a prior GitHub incident.","HTTP request to external service lacked timeout, causing indefinite hang and subsequent connection timeout error."],"transactions":["app.overwatch_request_endpoint","seer.automation.codegen.pr_closed_step.pr_closed_task","app.autofix_start_endpoint"],"title":"GitHub App installation checks timing out","description":"Requests to GitHub App installation endpoints are timing out during repo access/auth flows (e.g., get_repo_installation and installation check), causing retries to be exhausted in multiple code paths. Likely a networking issue or upstream slowness impacting GitHub API connectivity.","tags":["Networking","External System","API","Timeout","Retries Exhausted","GitHub API","urllib3"],"cluster_size":4,"cluster_min_similarity":0.9449725578887548,"cluster_avg_similarity":0.9613650927421293},{"project_ids":["1"],"cluster_id":589,"group_ids":[6725353740,6793730332],"issue_titles":["ApiError: <!DOCTYPE html>","ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}"],"root_cause_summaries":["Jira integration repeatedly fails due to suspended subscription; Sentry lacks specific error handling and circuit breaking for permanent external service failures.","Sentry's Jira integration lacks external service deactivation detection, causing failed syncs against permanently unavailable Jira instances."],"transactions":["sentry.integrations.tasks.sync_status_outbound"],"title":"Jira issue fetch fails with 404/403 in outbound sync","description":"During sync_status_outbound, calls to Jiras get_issue return Not Found or Forbidden, indicating missing issues or insufficient permissions for the integration user. This breaks status synchronization from our service to Jira.","tags":["External System","API","Authorization","Jira","404 Not Found","403 Forbidden"],"cluster_size":2,"cluster_min_similarity":0.9606800161494914,"cluster_avg_similarity":0.9606800161494914},{"project_ids":["1"],"cluster_id":590,"group_ids":[6725487265,6725487266],"issue_titles":["InvalidSearchQuery: Choose a single environment to filter by release stage."],"root_cause_summaries":["Release stage filter requires single environment, but subscription query builder provides none, causing validation error.","Subscription query with `release.stage` filter lacks environment context, causing `filter_by_stage` to fail."],"transactions":["sentry.models.releases.util in filter_by_stage"],"title":"Release stage filter requires single environment","description":"Queries building subscriptions in Snuba fail when filtering release stage across multiple environments, triggering InvalidSearchQuery. Limit the query to one environment when using the release stage filter.","tags":["API","Input Validation","Configuration","Sentry","InvalidSearchQuery","Release Stage Filter"],"cluster_size":2,"cluster_min_similarity":0.9797656713191761,"cluster_avg_similarity":0.9797656713191761},{"project_ids":["1"],"cluster_id":591,"group_ids":[6725490091,6760338992,6760360334,6777474105],"issue_titles":["SentryAppSentryError: missing_event"],"root_cause_summaries":["Webhook task attempts event data retrieval from nodestore before full persistence or after cleanup, causing missing_event error.","Webhook task attempts to retrieve event from nodestore before data is consistently available, causing missing_event error.","Task fails to retrieve event data from nodestore due to race condition; explicit configuration prevents retry.","Asynchronous event deletion task removes data from nodestore before dependent processing task can retrieve it, causing missing event error."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound"],"title":"Sentry App interaction events missing during resource change","description":"SentryAppSentryError 'missing_event' is raised while processing Sentry App resource change hooks, as SentryAppInteractionEvent fails to record and capture the exception via sentry_sdk.","tags":["API","Configuration","Sentry","Missing Event"],"cluster_size":4,"cluster_min_similarity":0.9425221061713239,"cluster_avg_similarity":0.9599572064283411},{"project_ids":["1"],"cluster_id":595,"group_ids":[6725580646,6739129577],"issue_titles":["Exception: The operation p95 supports one or more parameters"],"root_cause_summaries":["On-demand metric spec validation fails for `p95()` because it strictly requires explicit arguments for distribution metrics, ignoring optional arguments.","On-demand metric spec validation fails for `p95()` because it doesn't recognize default arguments, expecting explicit parameters."],"transactions":["sentry.tasks.on_demand_metrics.process_widget_specs"],"title":"On-demand metrics fail: p95 requires parameters","description":"Widget query metric extraction throws when parsing the p95 aggregate because required parameters are missing. This breaks on-demand metric spec creation in the extraction pipeline for widget queries.","tags":["API","Input Validation","Configuration","On-Demand Metrics","Aggregate Parsing","Missing Parameters"],"cluster_size":2,"cluster_min_similarity":0.9678121978437046,"cluster_avg_similarity":0.9678121978437046},{"project_ids":["1"],"cluster_id":596,"group_ids":[6725594321,6725660245,6725969830,6731196370,6738807145,6749980433,6782831025,6792293637,6796271507,6802766740],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.process_commit_context","ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.tasks.process_commit_context","ProcessingDeadlineExceeded: execution deadline of 905 seconds exceeded by sentry.tasks.commits.fetch_commits","ProcessingDeadlineExceeded: execution deadline of 120 seconds exceeded by sentry.issues.tasks.post_process.post_process_group"],"root_cause_summaries":["GitLab API response latency exceeds Sentry's 10-second proxy timeout, causing `process_commit_context` task to exceed its 60-second deadline.","Task times out because a single GitHub GraphQL query for blame data includes too many files.","GitLab blame API's per-line calls for numerous in-app frames from same file exceed task deadline.","Task's 90-second deadline is overridden to 60 seconds by a system-level configuration, causing timeout during external API calls.","GitLab API call for file blame exceeded 60-second deadline, causing task worker to raise `ProcessingDeadlineExceeded`.","Microsoft Teams API rate limits cause immediate failures; proxy lacks 429 retry, leading to task timeout.","GitLab integration's per-commit diff fetching, due to GitLab API limitations, caused `fetch_commits` task timeout on large commit ranges.","Microsoft Teams API rate limits cause `ApiError`s; lack of retry/backoff in Sentry's notification logic exhausts `post_process_group` task deadline.","GitLab API call exceeds task's enforced 60-second deadline, despite task's 90-second configuration, causing `ProcessingDeadlineExceeded`.","Microsoft Teams API rate limits requests; Sentry's client lacks retry logic, causing task deadline exceedance."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.tasks.process_commit_context","sentry.tasks.commits.fetch_commits","sentry.taskworker.workerchild in handle_alarm"],"title":"Commit context task exceeds deadline on GitHub calls","description":"sentry.tasks.process_commit_context hits its processing deadline while fetching commit/blame data from GitHub, with upstream requests timing out. Read timeouts and rate-limit checks cause the task to overrun and be killed.","tags":["External System","API","Networking","Timeout","GitHub","Read Timeout","process_commit_context"],"cluster_size":10,"cluster_min_similarity":0.920835965124215,"cluster_avg_similarity":0.9507492655509185},{"project_ids":["1"],"cluster_id":600,"group_ids":[6725840726,6794683496,6794693939,6794695253],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook","ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"],"root_cause_summaries":["Hybrid cloud caching's `RegionCacheVersion` database lookups, exacerbated by `SELECT FOR UPDATE` contention during cache invalidation, exceed task deadlines.","Task's configured 30s deadline not applied; defaults to 10s, which slow database query exceeds.","Slow `hybridcloud_regioncacheversion` database query for cache versioning causes `send_resource_change_webhook` task deadline exceedance.","Database query for cache version experienced severe delay, causing task deadline to be exceeded."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"],"title":"Webhook task times out in Sentry Apps service","description":"send_resource_change_webhook consistently exceeds the processing deadline while fetching installation data from the Sentry Apps service cache layer, causing the task to time out. This blocks delivery of resource change webhooks.","tags":["API","Queueing","Timeout","Sentry Apps","Background Task"],"cluster_size":4,"cluster_min_similarity":0.943759553120963,"cluster_avg_similarity":0.960487197333586},{"project_ids":["1"],"cluster_id":601,"group_ids":[6725840749,6736162416,6742154796,6751541557,6756102602,6756102612,6760205253,6760311099,6760559141,6791678508,6799141727],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification","ProcessingDeadlineExceeded: execution deadline of 20 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2","ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.workflow_engine.tasks.process_workflows_event","ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.tasks.process_commit_context","ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.workflow_engine.processors.delayed_workflow"],"root_cause_summaries":["Inefficient database query for linked commits during Group serialization in a time-sensitive task caused timeout.","Slow database queries for pull requests and repositories cause task to exceed 60-second Celery deadline.","Task's 10-second deadline exceeded due to expensive `serialize(issue)` operation, fetching extensive related data.","Inefficient data access for related objects causes repeated database queries, exceeding task execution deadline.","Bulk workflow processing times out due to inefficient `GroupAssignee` lookups via `HybridCloudForeignKey` on cache misses.","Cache TTL equals task deadline, causing repeated slow database queries for assignees, exceeding the task's processing limit.","Snuba query for issue's 'unhandled' status, using `argMax` on `exception_stacks.mechanism_handled` array, exceeds 10-second deadline.","Workflow condition evaluation on large, malformed event data exceeds 60-second processing deadline.","Issue serialization's `id__in` query on `ExternalIssue` with many `GroupLink`s exceeds task's 10-second deadline.","Task's default 10-second deadline is exceeded by slow `flagpole.allowed_features` option retrieval during issue serialization.","Workflow notification task exceeded its deadline due to serializing an issue with an excessive number of linked external issues."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2","sentry.tasks.process_commit_context","sentry.workflow_engine.processors.delayed_workflow","sentry.workflow_engine.tasks.process_workflows_event","sentry.sentry_apps.tasks.sentry_apps.workflow_notification","sentry.taskworker.workerchild in handle_alarm"],"title":"Sentry app webhooks timing out in workflow tasks","description":"Workflow notification and alert webhook tasks exceed processing deadlines while serializing issue data and sending outbound requests, sometimes surfacing a KeyError('organization') in alert webhook payload assembly. This causes Sentry App integrations to miss or delay webhook deliveries.","tags":["External System","API","Queueing","Timeout","Webhook","Sentry Apps","ProcessingDeadlineExceeded"],"cluster_size":11,"cluster_min_similarity":0.9112643182281103,"cluster_avg_similarity":0.9384520489819076},{"project_ids":["1"],"cluster_id":611,"group_ids":[6726661433,6732038487],"issue_titles":["IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Done' that is not in the list of supported values","IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Resolved' that is not in the list of supported values"],"root_cause_summaries":["Sentry's stored 'Resolved' status for Azure DevOps project is invalid, causing API rejection.","Azure DevOps rejected Sentry's 'Resolved' state update; it's not a valid state for the work item type."],"transactions":["sentry.integrations.tasks.sync_status_outbound"],"title":"Azure DevOps work item state not allowed","description":"Outbound status sync attempts to set Azure DevOps Work Item 'State' to values like 'Done' or 'Resolved', which are not valid for the project's workflow, causing 400 Bad Request responses. Update the integration mapping or ADO process configuration to use supported state values.","tags":["External System","API","Configuration","Azure DevOps","HTTP 400","Invalid Field Value"],"cluster_size":2,"cluster_min_similarity":0.9693132780681541,"cluster_avg_similarity":0.9693132780681541},{"project_ids":["1"],"cluster_id":613,"group_ids":[6727663731,6760235161],"issue_titles":["InvalidSchema: No connection adapters were found for \"b''://b''/\"","MissingSchema: Invalid URL '': No scheme supplied. Perhaps you meant https://?"],"root_cause_summaries":["SentryApp's webhook_url was an empty string, causing `requests` to fail due to missing URL scheme.","Sentry App's `webhook_url` is `None`, causing malformed URL construction and `InvalidSchema` error during external request."],"transactions":["/api/0/sentry-app-installations/{uuid}/external-issue-actions/","/api/0/internal/rpc/{service_name}/{method_name}/"],"title":"Webhooks use empty/invalid URL in Sentry App requests","description":"Sentry App webhook and external-issue actions attempt HTTP requests with missing or malformed URLs, causing MissingSchema/InvalidSchema errors and failed interactions. Likely due to misconfigured or empty integration endpoint settings.","tags":["API","Configuration","Input Validation","HTTP","Sentry Apps","MissingSchema","InvalidSchema"],"cluster_size":2,"cluster_min_similarity":0.950904821687826,"cluster_avg_similarity":0.950904821687826},{"project_ids":["1"],"cluster_id":621,"group_ids":[6729222788,6794396340,6794693399],"issue_titles":["ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}"],"root_cause_summaries":["Jira's eventual consistency causes 404 on immediate retrieval of newly created issues, despite successful creation.","Jira's eventual consistency causes 404 when Sentry immediately retrieves newly created issue.","Jira's eventual consistency delays new sub-task availability, causing Sentry's immediate retrieval attempt to fail."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.rules.processing.delayed_processing"],"title":"Jira issue lookup fails with 404 in rule callbacks","description":"During rule-driven create_issue callbacks, the integration attempts to fetch a Jira issue key that no longer exists or is inaccessible, resulting in a 404 and ApiError. This breaks post-process and delayed processing paths when recording ProjectManagementEvent metrics.","tags":["External System","API","Authorization","Jira","Not Found","HTTP 404"],"cluster_size":3,"cluster_min_similarity":0.9657243147663431,"cluster_avg_similarity":0.9715544593197425},{"project_ids":["1"],"cluster_id":622,"group_ids":[6729389206,6796629268],"issue_titles":["ApiError: upstream connect error or disconnect/reset before headers. reset reason: connection termination"],"root_cause_summaries":["Discord API returned 503 Service Unavailable, causing Sentry's client to re-raise as a generic API connection error.","Discord API returned 503 Service Unavailable, causing connection termination during alert delivery."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.tasks.post_process.post_process_group"],"title":"Notification post-processing fails due to upstream 503","description":"Post-process notification callbacks encounter upstream connection termination and HTTP 503 Service Unavailable when sending messages via the external client, causing error reporting in Sentry. Likely an upstream service outage or Envoy/ingress reset before headers.","tags":["Networking","External System","API","Upstream Unavailable","Connection Reset","HTTP","Notifications"],"cluster_size":2,"cluster_min_similarity":0.9775517702459567,"cluster_avg_similarity":0.9775517702459567},{"project_ids":["1"],"cluster_id":623,"group_ids":[6729626909,6779186130],"issue_titles":["TransportError: KafkaError{code=MSG_SIZE_TOO_LARGE,val=10,str=\"Broker: Message size too large\"}"],"root_cause_summaries":["Individual `TraceItem` protobufs, especially from memory events, exceed Kafka's message size limit when encoded.","Kafka broker's message size limit is smaller than Sentry's compression threshold, causing uncompressed messages to be rejected."],"transactions":["replays.consumer.recording_buffered.commit_message","sentry.utils.arroyo_producer in _track_futures"],"title":"Kafka produce fails due to oversized messages","description":"Producers in cleanup and replay recording paths attempt to publish messages that exceed Kafkas max message size, causing MSG_SIZE_TOO_LARGE TransportErrors and failing async task dispatch and trace item emission.","tags":["Queueing","Configuration","Kafka","Message Too Large","Arroyo"],"cluster_size":2,"cluster_min_similarity":0.9563937597316063,"cluster_avg_similarity":0.9563937597316063},{"project_ids":["1"],"cluster_id":626,"group_ids":[6731059211,6731059212,6791824168],"issue_titles":["ApiRetryError: SafeHTTPConnectionPool(host='sentry-rpc-prod-control.us.sentry.internal', port=8999): Max retries exceeded with url: /api/0/internal/integration-proxy/ (Caused by ResponseError('too many 503 error responses'))"],"root_cause_summaries":["Integration proxy overloaded by MsTeams rate limits, exhausting retries and failing notifications.","Control Silo's integration proxy became overwhelmed, returning 429s then 503s, causing Region Silo's retries to exhaust.","Control silo's integration proxy service repeatedly returned 503s and 429s, exhausting client retries and causing task timeouts."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.tasks.post_process.post_process_group"],"title":"Integration proxy returns repeated error responses","description":"Post-processing notifications fail when calling /api/<int>/internal/integration-proxy/, as the client exhausts retries due to too many error responses from the integration proxy endpoint, preventing message delivery.","tags":["API","External System","Retries Exhausted","HTTP","Integration Proxy"],"cluster_size":3,"cluster_min_similarity":0.9626869805571338,"cluster_avg_similarity":0.968388056889344},{"project_ids":["11276"],"cluster_id":627,"group_ids":[6731197445,6741353264,6792088846],"issue_titles":["InternalServerError: PUT /organizations/{orgSlug}/explore/saved/267766/ 500","InternalServerError: PUT /organizations/{orgSlug}/discover/saved/32926/ 500","InternalServerError: PUT /organizations/{orgSlug}/explore/saved/295981/ 500"],"root_cause_summaries":["Python datetime objects were directly stored in a Django JSONField, causing serialization failure.","Raw `datetime` object in `lastVisited` field prevents JSON serialization of saved query response.","Backend failed to JSON serialize Python datetime objects during saved query update response/logging, causing a 500 error."],"transactions":["/explore/discover/results/","/explore/traces/","/explore/traces/compare/"],"title":"500 on PUT updating saved queries in Explore/Discover","description":"Client updates to saved queries are failing with InternalServerError on PUT endpoints under /organizations/{orgSlug}/explore/saved and /discover/saved. This impacts saving/updating queries from multiple UI flows, indicating a server-side issue on the saved query update API.","tags":["API","External System","Upstream Unavailable","HTTP 500","Sentry API","PUT","Saved Queries"],"cluster_size":3,"cluster_min_similarity":0.9659426018792184,"cluster_avg_similarity":0.9668519371740584},{"project_ids":["1"],"cluster_id":630,"group_ids":[6731409084,6741623953,6805611098],"issue_titles":["ApiError: {\"error\":{\"code\":\"BotDisabledByAdmin\",\"message\":\"The tenant admin disabled this bot\"}}"],"root_cause_summaries":["Microsoft Teams bot disabled by tenant admin, causing 403 Forbidden API responses when Sentry attempts to send notifications.","Microsoft Teams API rejected notification due to bot disabled by tenant admin, causing Sentry's `ApiError`.","Microsoft Teams bot disabled by admin, preventing Sentry from sending notifications."],"transactions":["sentry.shared_integrations.client.base in _request","sentry.tasks.activity.send_activity_notifications"],"title":"Microsoft Teams bot disabled causes 403 on send","description":"Notifications to Microsoft Teams fail with 403 Forbidden because the tenant admin disabled the bot, leading to ApiError BotDisabledByAdmin during message/card posts. Affects activity and digest notifications sent via the Teams client.","tags":["External System","API","Authorization","Microsoft Teams","Forbidden 403","BotDisabledByAdmin"],"cluster_size":3,"cluster_min_similarity":0.9771997268821693,"cluster_avg_similarity":0.9799779742195014},{"project_ids":["6178942"],"cluster_id":637,"group_ids":[6732075021,6774824895],"issue_titles":["ConsentError: Organization 1176005 has not consented to use GenAI","ConsentError: Organization qyon-brazil has not consented to use GenAI"],"root_cause_summaries":["Organization 1176005 lacks GenAI consent due to missing `PromptsActivity` record and `github-extension.enabled-orgs` entry.","Organization 'qyon-brazil' lacks GenAI consent in Sentry, causing `ConsentError` when `seer` attempts GenAI feature usage."],"transactions":["app.overwatch_request_endpoint","app.summarize_replay_breadcrumbs_endpoint"],"title":"GenAI requests blocked due to missing org consent","description":"Requests to GenAI endpoints are raising ConsentError because the target organization has not provided consent. Affected endpoints include overwatch and replay breadcrumb summarization, preventing AI features from running for non-consenting orgs.","tags":["Authorization","Configuration","API","ConsentError","GenAI"],"cluster_size":2,"cluster_min_similarity":0.9637505891281571,"cluster_avg_similarity":0.9637505891281571},{"project_ids":["1"],"cluster_id":638,"group_ids":[6732360703,6748499021],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.rules.processing.delayed_processing"],"root_cause_summaries":["Slow database query for parent notification message due to missing compound index on `sentry_rulefirehistory` causes task timeout.","Inefficient `NotificationMessage` query for Slack threading, lacking optimal indexing, cumulatively exceeds task deadline."],"transactions":["sentry.rules.processing.delayed_processing"],"title":"Delayed issue alert processing exceeds deadline","description":"Sentry's delayed rule processing for issue alert notifications times out while fetching the parent notification message (Slack thread lookup), triggering ProcessingDeadlineExceeded in the worker. This likely indicates slow downstream calls or database queries within the notification pipeline.","tags":["Queueing","API","Database","Timeout","Sentry","Slack","Issue Alerts"],"cluster_size":2,"cluster_min_similarity":0.963773232837871,"cluster_avg_similarity":0.963773232837871},{"project_ids":["6178942"],"cluster_id":639,"group_ids":[6732483871,6794070610],"issue_titles":["HTTPError: 503 Server Error: Service Unavailable for url: http://frontend-internal.sentry./api/0/internal/seer-rpc/get_issues_related_to_exception_type/","HTTPError: 503 Server Error: Service Unavailable for url: http://frontend-internal.sentry./api/0/internal/seer-rpc/get_transactions_for_project/"],"root_cause_summaries":["Malformed `SENTRY_BASE_URL` environment variable with trailing dot caused invalid hostname, leading to 503 Service Unavailable.","SentryRpcClient's base URL, sourced from environment, contained a trailing dot, causing malformed RPC endpoint and 503 HTTPError."],"transactions":["seer.automation.codegen.pr_review_step.pr_review_task","seer.explorer_index.index_org_task.index_org"],"title":"RPC calls receive 503 Service Unavailable","description":"RPC client requests fail with HTTP 503 during issue summary and org indexing operations, indicating the upstream service is unavailable.","tags":["External System","API","Upstream Unavailable","HTTP","RPC Client"],"cluster_size":2,"cluster_min_similarity":0.9816332375163025,"cluster_avg_similarity":0.9816332375163025},{"project_ids":["6178942"],"cluster_id":640,"group_ids":[6732487037,6732487047,6732487051,6732487054,6732487058,6732487060,6732487062,6732487066,6732487081,6732487085,6732487086,6732487087,6732487089,6732487091,6732487093,6732487094],"issue_titles":["KeyError in FastAPICustomTransport.capture_envelope: 'trace'"],"root_cause_summaries":["Sentry's `traces_sampler` disables tracing for health checks, causing envelopes to lack a 'trace' header, which the custom transport and its superclass then attempt to access, leading to `KeyError`.","Sentry's HttpTransport unexpectedly accesses 'trace' header for non-trace error events, causing KeyError despite custom transport's handling.","Sentry SDK 2.28.0's `HttpTransport.capture_envelope` incorrectly assumes 'trace' key presence in all envelopes, causing `KeyError` for non-trace events.","Sentry's `HttpTransport` expects a 'trace' header in all envelopes, but error envelopes from health checks lack it due to disabled tracing.","sentry-sdk 2.28.0's `HttpTransport.capture_envelope` incorrectly expects 'trace' key in non-trace envelopes, causing `KeyError`.","Custom Sentry transport attempts to modify non-existent 'trace' header on error event envelopes, causing repeated KeyErrors.","sentry-sdk 2.28.0's HttpTransport incorrectly accesses 'trace' header for non-trace error envelopes, causing KeyError.","Sentry's base HttpTransport unexpectedly accesses 'trace' header for non-trace envelopes, causing KeyError during error reporting.","Sentry SDK 2.28.0's HttpTransport unconditionally expects 'trace' key in all envelopes, causing KeyError for non-trace error events.","sentry_sdk's HttpTransport incorrectly accesses 'trace' header for error envelopes, causing KeyError.","Sentry SDK 2.28.0's HttpTransport fails on missing 'trace' header for non-trace error envelopes.","Sentry SDK 2.28.0's HttpTransport incorrectly accesses 'trace' key in non-trace envelopes, causing KeyError.","Sentry SDK 2.28.0's HttpTransport incorrectly expects 'trace' header in non-trace error envelopes, causing KeyError.","Custom Sentry transport unconditionally accesses 'trace' header, but trace sampler disables tracing for health checks, causing KeyError.","Custom transport's `capture_envelope` fails when parent method accesses missing 'trace' header after sampling.","Sentry-sdk 2.28.0's HttpTransport.capture_envelope incorrectly assumes 'trace' header presence for all envelopes, causing KeyError on non-trace error events."],"transactions":["/health/live"],"title":"FastAPI transport logs repeated KeyError in capture_envelope","description":"FastAPICustomTransport.capture_envelope is raising KeyError repeatedly during bootup logging, indicating missing expected keys in the envelope payload and causing noisy error logs.","tags":["API","Input Validation","FastAPI","KeyError"],"cluster_size":16,"cluster_min_similarity":0.9371899621514893,"cluster_avg_similarity":0.9658808605584166},{"project_ids":["6178942"],"cluster_id":643,"group_ids":[6733820295,6733821439],"issue_titles":["ValueError: The base_commit_sha must be set for bug prediction"],"root_cause_summaries":["Upstream system failed to provide `base_commit_sha` to `RepoDefinition`, causing `BugPredictionStep` to error.","Bug prediction failed because the initial request lacked `base_commit_sha`, a mandatory field for the bug prediction step."],"transactions":["seer.automation.codegen.pr_review_step.pr_review_task"],"title":"Bug prediction fails due to missing base_commit_sha","description":"Bug prediction step is raising a ValueError when base_commit_sha is not provided, causing PR review workflows to fail during bug prediction invocation.","tags":["Configuration","API","Input Validation","PR Review Pipeline","Missing Parameter"],"cluster_size":2,"cluster_min_similarity":0.9816171059932216,"cluster_avg_similarity":0.9816171059932216},{"project_ids":["11276"],"cluster_id":647,"group_ids":[6734177725,6789132874,6804101103,6806196050],"issue_titles":["Error: Invalid timestamp: NaN","RangeError: Invalid Date","RangeError: Invalid time value"],"root_cause_summaries":["Timestamp `NaN` passes `typeof number` check, leading to `new Date(NaN)` and `toISOString()` on invalid date.","Invalid `timestamp` query parameter bypasses date-time normalization, leading to `NaN` conversion and `toISOString()` failure on an invalid Date object.","Invalid timestamp query parameter converts to NaN, creating an invalid Date object, causing RangeError on toISOString().","Log data contains invalid timestamp strings; frontend attempts `new Date()` conversion without prior validation, resulting in `NaN` and error."],"transactions":["/explore/logs/","/issues/trace/:traceSlug/","/explore/traces/trace/:traceSlug/"],"title":"Invalid date parsing in Logs/Traces views","description":"React routes for Logs and Traces throw 'Invalid timestamp/Date/time value' when parsing table or query params, likely due to NaN or malformed date inputs in utils.tsx getTimeStampFromTableDateField. This breaks rendering within LogsPage/TraceView contexts.","tags":["Input Validation","Frontend","React","Date Parsing","Invalid Time Value"],"cluster_size":4,"cluster_min_similarity":0.9466272014535521,"cluster_avg_similarity":0.9656346175581824},{"project_ids":["1"],"cluster_id":651,"group_ids":[6734491049,6735466651,6737590611],"issue_titles":["SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/users.list)"],"root_cause_summaries":["Non-existent Slack channel name triggers expensive user enumeration, causing Slack API rate limits on `users.list`.","Slack user search paginates all users, causing excessive API calls that trigger Slack's rate limits.","Template variable `{{notification-channel}}` passed to Slack API lookup, triggering excessive `users.list` calls, leading to rate limiting."],"transactions":["sentry.integrations.slack.tasks.search_channel_id_for_alert_rule","sentry.integrations.slack.tasks.search_channel_id_for_rule","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/rules/"],"title":"Slack user lookup for channel resolution hits rate limits","description":"Slack API calls to list users during channel ID resolution for alert rules are being rate limited, causing failures in rule creation and background worker tasks. The issue occurs in get_slack_user_list during users.list pagination for Slack integrations.","tags":["External System","API","Rate Limiting","Slack","Users List","Alert Rule Channel Resolution"],"cluster_size":3,"cluster_min_similarity":0.9568714335924385,"cluster_avg_similarity":0.9647804703876458},{"project_ids":["300688"],"cluster_id":654,"group_ids":[6734654423,6734669847,6734671361],"issue_titles":["ClickhouseError: DB::Exception: Missing columns: 'gen_ai.usage.total_tokens_TYPE_INT' while processing query: 'SELECT toDateTime(1752001200 + (intDiv(toUnixTimestamp(timestamp) - 1752001200, 3600) * 3600)) AS time_slot, sumOrNullIf(`gen_ai.usage.total_tokens_TYPE_INT` *...","QueryException: DB::Exception: Missing columns: 'gen_ai.usage.total_tokens_TYPE_INT' while processing query: 'SELECT CAST(replaceAll(toString(trace_id), '-', ''), 'String') AS `sentry.trace_id_TYPE_STRING`, round(sumOrNullIf(1 / sampling_factor, has(`attributes_float_2...","QueryException: DB::Exception: Missing columns: 'gen_ai.usage.input_tokens_TYPE_INT' 'gen_ai.usage.output_tokens_TYPE_INT' while processing query: 'SELECT if(has(`attributes_string_28.keys`, 'gen_ai.request.model'), attributes_string_28['gen_ai.request.model'], NULL) A..."],"root_cause_summaries":["Clickhouse query references an aliased expression as a direct column, but the expression's definition is missing from the SELECT clause.","ClickHouse fails when complex coalesce expressions are directly embedded in aggregate functions, expecting physical column names.","ClickHouse query fails because a dynamically generated alias is referenced as a physical column, due to incorrect query construction."],"transactions":["EndpointTimeSeries__v1","EndpointTraceItemTable__v1"],"title":"ClickHouse queries missing gen_ai token columns","description":"Queries against traces/metrics reference gen_ai.usage token fields (total/input/output) that are not present in the ClickHouse schema, causing ServerException: Missing columns. This likely stems from a schema mismatch or un-migrated columns used by AI usage reporting.","tags":["Database","Schema Migration","ClickHouse","Missing Column","gen_ai.usage"],"cluster_size":3,"cluster_min_similarity":0.9530743352060214,"cluster_avg_similarity":0.9559718969371819},{"project_ids":["1"],"cluster_id":658,"group_ids":[6734896832,6794594484,6794594558,6794601016],"issue_titles":["HTTPError: 500 Server Error: Internal Server Error for url: http://seer-web-autofix/v1/automation/autofix/start"],"root_cause_summaries":["Frontend-internal Sentry service unavailability causes Seer's consent check to fail, leading to autofix task failure.","Sentry's internal RPC service returned 503, causing Seer to fail autofix request with 500.","Sentry's internal RPC service returned 503, preventing autofix consent verification, causing seer-web-autofix to fail with 500.","Autofix failed because `frontend-internal.sentry`'s consent endpoint returned 503, preventing `seer-web-autofix` from proceeding."],"transactions":["sentry.tasks.autofix.trigger_autofix_from_issue_summary"],"title":"Autofix requests to Seer return 500 errors","description":"The issue_summary autofix task fails when _call_autofix raises for HTTP 5xx from the Seer service, preventing run IDs from being created. Upstream Seer endpoint is returning Internal Server Error.","tags":["API","External System","Upstream Unavailable","HTTP 5xx","Seer","Autofix"],"cluster_size":4,"cluster_min_similarity":0.969941580984522,"cluster_avg_similarity":0.9766468779371119},{"project_ids":["1"],"cluster_id":660,"group_ids":[6734975641,6794077193],"issue_titles":["ApiGrant.DoesNotExist: ApiGrant matching query does not exist."],"root_cause_summaries":["ApiGrant not replicated to task worker's database replica before SentryAppInstallation serialization, causing DoesNotExist error.","Asynchronous webhook attempts to serialize `ApiGrant` after it's deleted by concurrent uninstallation, causing `DoesNotExist` error."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.installation_webhook"],"title":"Sentry App installation webhook serialization fails","description":"Installation webhook serialization for Sentry Apps is failing when accessing api_grant: the serializer logs an exception and raises ApiGrant.DoesNotExist followed by a KeyError on 'api_grant'. This likely stems from missing or deleted ApiGrant records during SentryAppInstallation serialization.","tags":["API","Serialization","Data Integrity","Sentry Apps","KeyError","Does Not Exist"],"cluster_size":2,"cluster_min_similarity":0.9598017373448445,"cluster_avg_similarity":0.9598017373448445},{"project_ids":["1"],"cluster_id":802,"group_ids":[6735618824,6770337465,6777598616,6778034004,6782756611,6782756628,6782756650,6782756661,6794964848,6803951855,6803951858,6803951864,6810511433],"issue_titles":["MovedError: 9864 192.168.208.40:11495","TimeoutError: Timeout connecting to server","MovedError: 9197 192.168.209.92:6310","MovedError: 4489 192.168.209.74:6300","MovedError: 15833 192.168.208.2:11148","TimeoutError: Timeout reading from socket","MovedError: 339 192.168.208.5:11035","MovedError: 7707 192.168.209.163:11018"],"root_cause_summaries":["Sentry's single-node Redis client fails to handle Redis Cluster's MOVED redirections, causing socket read timeouts.","Sentry's Redis client, configured for standalone, connects to a Redis Cluster, failing on `MovedError` due to missing cluster redirection logic.","Redis cluster client used with single-node Redis instance, causing `MovedError` and `TimeoutError` due to client/server type mismatch.","Redis client configured for cluster operations failed on non-cluster instance, causing `MovedError` due to configuration mismatch.","Redis deployment is a cluster, but Sentry's configuration treats it as a single instance, causing client-server protocol mismatch and connection timeouts.","Redis client misconfiguration: single-node client used with cluster, causing `MovedError` due to unhandled redirections.","Redis Cluster client attempts connecting to a single Redis instance, causing timeouts and `MOVED` errors due to configuration mismatch.","Sentry's Redis client misconfiguration for a cluster causes `FailoverRedis` to fail on `MovedError`, leading to socket timeouts.","Sentry's Redis configuration for a single node connects to a Redis Cluster, causing `MovedError` due to incorrect topology awareness.","Sentry's single-node Redis client failed to handle Redis Cluster's `MOVED` redirection, causing `MovedError` and `TimeoutError`.","Redis client configured for single instance but connected to unstable cluster, causing `MovedError` and `TimeoutError`.","Redis client configured for cluster mode, but connected to a single instance, causing connection timeouts and MOVED errors.","Sentry's Redis configuration expects a single instance, but connects to a cluster, causing client to mishandle `MOVED` redirects and timeout."],"transactions":["sentry.spans.buffer in done_flush_segments","sentry.tasks.store.save_event","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","sentry.tasks.store.save_event_transaction","sentry.models.featureadoption in get_all_cache","sentry.spans.buffer in flush_segments","monitors.monitor_consumer","sentry.lang.native.sources in get_last_upload","sentry.issues.tasks.post_process.post_process_group"],"title":"Redis timeouts and MOVED errors across processing pipeline","description":"Multiple components (monitor check-ins, event store, symbolication, and post-processing) are failing due to Redis connectivity issues, showing socket timeouts and MOVED redirections. This suggests Redis cluster unavailability or misconfiguration causing cache and state operations to fail.","tags":["Networking","Caching","Configuration","Redis","Timeout","MOVED Redirection"],"cluster_size":13,"cluster_min_similarity":0.9162608100673646,"cluster_avg_similarity":0.9461917842025743},{"project_ids":["300688"],"cluster_id":662,"group_ids":[6735651592,6752847049],"issue_titles":["<unknown>"],"root_cause_summaries":["Kafka client failed to resolve broker hostname due to unresolvable `DEFAULT_BROKERS` environment variable.","Kafka consumer failed to start due to unresolvable `bootstrap.servers` hostnames, preventing connection to brokers."],"transactions":[],"title":"Kafka client fails DNS resolution for bootstrap hosts","description":"librdkafka reports repeated 'Local: Host resolution failure' when resolving Kafka bootstrap hostnames, preventing message consumption. Likely DNS or service discovery issue in the environment.","tags":["Networking","External System","Kafka","DNS Resolution Failure","librdkafka"],"cluster_size":2,"cluster_min_similarity":0.9598026011001506,"cluster_avg_similarity":0.9598026011001506},{"project_ids":["300688"],"cluster_id":663,"group_ids":[6735653751,6735653770,6735653953,6749052142],"issue_titles":["ClickhouseError: Connection refused (10.0.0.1:9050)","ClickhouseError: Connection refused (10.0.0.1:9010)","ClickhouseError: Code: 210. Connection refused (snuba-metrics-1-3:9000)\""],"root_cause_summaries":["ClickHouse server at 10.0.0.1:9050 refused connection, preventing Snuba startup after retries.","ClickHouse server `snuba-metrics-1-3:9000` refused connection, preventing migration pre-checks due to an unavailable cluster node.","ClickHouse server at 10.0.0.1:9050 actively refused connection, indicating it's unreachable or misconfigured, preventing Snuba startup.","Clickhouse server at 10.0.0.1:9010 refused connection, preventing Snuba initialization after 60 retries."],"transactions":["[cli init] migrations","[cli init] subscriptions-executor","[cli init] consumer","[cli init] subscriptions-scheduler"],"title":"ClickHouse connections refused across services","description":"Multiple Snuba components (consumer, subscriptions executor/scheduler, migrations) fail to execute simple queries because ClickHouse refuses TCP connections, indicating the ClickHouse service or its endpoint is unavailable.","tags":["Networking","External System","Database","Connection Refused","ClickHouse","Snuba"],"cluster_size":4,"cluster_min_similarity":0.9356752451684411,"cluster_avg_similarity":0.9533996524831899},{"project_ids":["1"],"cluster_id":664,"group_ids":[6735897451,6814026749],"issue_titles":["IntegrationError: Failed to get Discord access token from API.","ApiInvalidRequestError: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid grant: authorization code is invalid\"}"],"root_cause_summaries":["Vercel authorization code reuse due to browser re-requesting callback URL causes invalid grant error.","Discord OAuth authorization code was reused, violating single-use policy, causing `invalid_grant` error."],"transactions":["/extensions/{provider_id}/setup/","/extensions/vercel/configure/"],"title":"Discord OAuth token exchange returns invalid_grant","description":"During Discord integration setup, the OAuth authorization code exchange fails with invalid_grant/Bad Request, indicating an invalid or expired code and preventing retrieval of the access token.","tags":["API","Authentication","External System","OAuth 2.0","Discord","Invalid Grant","Bad Request"],"cluster_size":2,"cluster_min_similarity":0.9558938790940797,"cluster_avg_similarity":0.9558938790940797},{"project_ids":["1"],"cluster_id":665,"group_ids":[6735904381,6792221012],"issue_titles":["IntegrationError: Error Communicating with Azure DevOps (HTTP 400): unknown error"],"root_cause_summaries":["PATCH request to Azure DevOps lacks 'value' fields, causing HTTP 400 due to serialization error.","Azure DevOps work item creation fails; Sentry sends oversized description/history fields, exceeding external API limits."],"transactions":["sentry.tasks.post_process.post_process_group","sentry.issues.tasks.post_process.post_process_group"],"title":"Azure DevOps work item creation returns 400","description":"Creating work items via the Azure DevOps integration is failing with HTTP 400 Bad Request, causing IntegrationError during post-process rule execution. Likely due to invalid request payload or missing fields expected by the Azure DevOps API.","tags":["External System","API","Configuration","Azure DevOps","HTTP 400","Bad Request","ApiInvalidRequestError"],"cluster_size":2,"cluster_min_similarity":0.9513651959702595,"cluster_avg_similarity":0.9513651959702595},{"project_ids":["1"],"cluster_id":677,"group_ids":[6738376726,6763623471,6808561776,6808561792],"issue_titles":["IntegrationError: Error Communicating with Jira (HTTP 429): unknown error","ApiRateLimitedError"],"root_cause_summaries":["Sentry's Jira integration lacks API rate limiting, causing excessive requests and Jira's 429 throttling.","Jira API rate-limits Sentry due to missing retry/backoff logic, causing continuous \"Too Many Requests\" errors.","Sentry's alert rule processing triggers multiple concurrent Jira issue creation requests, exceeding Jira's API rate limits.","Jira integration lacks global rate-limiting/backoff, causing repeated API calls after initial 429, exacerbating rate limits."],"transactions":["sentry.tasks.post_process.post_process_group","sentry.issues.tasks.post_process.post_process_group"],"title":"Jira issue creation hits 429 rate limits","description":"Post-process jobs fail when creating Jira issues due to Jira returning HTTP 429 Too Many Requests, surfaced as ApiRateLimitedError and wrapped IntegrationError. This likely stems from exceeding Jira API quotas during issue creation.","tags":["External System","API","Rate Limiting","Jira","HTTP 429","IntegrationError"],"cluster_size":4,"cluster_min_similarity":0.9682573500147622,"cluster_avg_similarity":0.9728996629391894},{"project_ids":["1"],"cluster_id":686,"group_ids":[6740958809,6747424558],"issue_titles":["RecursionError: maximum recursion depth exceeded"],"root_cause_summaries":["Trace with excessive span depth caused `_serialize_rpc_event` recursion to exceed Python's limit.","Recursive trace serialization exceeds Python's depth limit on deeply nested span hierarchies."],"transactions":["/api/0/organizations/{organization_id_or_slug}/trace/{trace_id}/"],"title":"Recursive RPC event serialization hits recursion limit","description":"serialize_rpc_event in organization_trace.py recurses through nested children without termination, causing Python to exceed the maximum recursion depth during RPC event serialization.","tags":["Serialization","Configuration","Python","RecursionError"],"cluster_size":2,"cluster_min_similarity":0.9786944472254214,"cluster_avg_similarity":0.9786944472254214},{"project_ids":["1"],"cluster_id":689,"group_ids":[6741579016,6782481981],"issue_titles":["ApiError: <!DOCTYPE html>"],"root_cause_summaries":["Jira Server rejected Sentry's OAuth1 request with 403 Forbidden, indicating invalid or insufficient credentials.","Jira integration's OAuth1 credentials expired/invalid, causing 403 Forbidden HTML response from Jira API."],"transactions":["sentry.integrations.tasks.sync_status_outbound"],"title":"Jira API GET /issue returns 403 Forbidden","description":"Outbound status sync fails when calling Jiras get_issue endpoint, raising HTTP 403 Forbidden from the Jira client. Likely due to missing permissions or revoked credentials on the Jira side, blocking issue reads during sync_status_outbound.","tags":["External System","API","Authentication","Upstream Unavailable","Jira","HTTP 403","sync_status_outbound"],"cluster_size":2,"cluster_min_similarity":0.9791932503920672,"cluster_avg_similarity":0.9791932503920672},{"project_ids":["1"],"cluster_id":697,"group_ids":[6745314496,6809836219],"issue_titles":["RuntimeError: Cannot reprocess group that is currently being reprocessed"],"root_cause_summaries":["Concurrent `reprocess_group` tasks race; second task fails when group status is already `REPROCESSING`.","Concurrent task initiation for same group causes race condition, leading to `RuntimeError` when status is already `REPROCESSING`."],"transactions":["/issues/:groupId/events/:eventId/","/issues/:groupId/"],"title":"Reprocessing blocked by concurrent group reprocessing","description":"Attempts to reprocess an event group fail because the same group is already undergoing reprocessing, causing a RuntimeError in the reprocessing workflow.","tags":["Concurrency","Job Processing","Sentry","Reprocessing","Already In Progress"],"cluster_size":2,"cluster_min_similarity":0.9697664541573394,"cluster_avg_similarity":0.9697664541573394},{"project_ids":["1"],"cluster_id":698,"group_ids":[6746018206,6775190689,6792960431],"issue_titles":["MovedError: 6235 192.168.208.186:11019","MovedError: 15412 192.168.208.184:11014"],"root_cause_summaries":["Redis cluster instability causes frequent slot migrations, leading to `MovedError` during `LPUSH` as `RetryingRedisCluster` cannot keep up.","Redis cluster resharding during pipeline execution caused `MovedError` on `EXPIRE` command, as client couldn't handle mid-pipeline slot migration.","Redis client fails to handle `MovedError` during pipeline execution, exposing cluster rebalancing issues when logging webhook timeouts."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"],"title":"Webhook delivery to Sentry Apps is timing out","description":"Resource change webhooks sent by the Sentry Apps service are timing out during HTTP requests, causing buffered webhook requests to fail in worker child processes. Timeouts occur while using SafeHTTPSConnectionPool during send_and_save_webhook_request.","tags":["Networking","API","External System","Timeout","HTTP","Sentry Apps","Webhooks"],"cluster_size":3,"cluster_min_similarity":0.9555817030867765,"cluster_avg_similarity":0.9568377192511831},{"project_ids":["1"],"cluster_id":706,"group_ids":[6746865674,6793705006],"issue_titles":["ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"],"root_cause_summaries":["API key with `org:read` scope cannot fetch incidents for `project: -1` (all projects), causing a 404.","Internal API key lacks project-level scope, causing incident query to return no projects and a 404."],"transactions":["sentry.incidents.tasks.handle_trigger_action"],"title":"Metric alert chart fetch returns API error in notifications","description":"Fetching metric issue open periods for alert chart generation fails with an API error, breaking both incident resolve and email notification paths. The charts client GET request raises ApiError with detail payload, preventing alert notifications from being sent.","tags":["API","Configuration","Serialization","HTTP","Client Error","Metric Alerts","Notifications"],"cluster_size":2,"cluster_min_similarity":0.9730657345324382,"cluster_avg_similarity":0.9730657345324382},{"project_ids":["1"],"cluster_id":713,"group_ids":[6748540761,6793721828],"issue_titles":["OperationalError: DeadlockDetected('deadlock detected\\nDETAIL:  Process 35930 waits for AccessExclusiveLock on tuple (16367238,62) of relation 46456 of database 16401; blocked by process 27797.\\nProcess 27797 waits for ShareLock on transaction 1532606209; blocked by proc...","OperationalError: deadlock detected"],"root_cause_summaries":["Concurrent artifact bundle cleanup tasks delete overlapping records, causing database lock contention and deadlock on `sentry_artifactbundleindex`.","Concurrent duplicate artifact bundle cleanup causes deadlock due to unconstrained `bundle_id` and cascade deletion lock contention."],"transactions":["sentry.tasks.assemble.assemble_artifacts"],"title":"PostgreSQL deadlock deleting artifact bundle index","description":"Concurrent artifact assembly tasks delete overlapping rows in sentry_artifactbundleindex, causing PostgreSQL to detect a deadlock during post_assemble cleanup. This impacts artifact bundle creation by failing the assemble_artifacts workflow.","tags":["Database","Concurrency","PostgreSQL","Deadlock","Artifact Assembly","DELETE Statement"],"cluster_size":2,"cluster_min_similarity":0.9801033210394723,"cluster_avg_similarity":0.9801033210394723},{"project_ids":["1"],"cluster_id":715,"group_ids":[6748914250,6760765605,6775115991],"issue_titles":["AssertionError"],"root_cause_summaries":["Rule data lacks `legacy_rule_id` in actions, causing assertion failure when `workflow-engine-ui-links` is off and `should_fire_workflow_actions` is on.","Slack message builder expects 'legacy_rule_id' in test rule data, but it's absent, causing assertion failure.","Workflow engine enabled, but rule's action data lacks `legacy_rule_id` due to incomplete migration, causing assertion failure."],"transactions":["sentry.tasks.post_process.post_process_group","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/rule-actions/","/extensions/slack/action/"],"title":"Slack issue alerts crash when legacy_rule_id missing","description":"Slack notification builders for issue alerts assume rules contain legacy_rule_id, causing AssertionError when the key is absent in rule data during post-processing and test actions.","tags":["Configuration","API","Notification","Slack","AssertionError","Missing Field"],"cluster_size":3,"cluster_min_similarity":0.9438467510375264,"cluster_avg_similarity":0.9554099712140367},{"project_ids":["1"],"cluster_id":718,"group_ids":[6750379434,6750615905],"issue_titles":["ApiError: {\"error\":{\"code\":\"ServiceError\",\"message\":\"Service Error\"}}"],"root_cause_summaries":["Adaptive Card payload contained un-serialized Python enum objects, causing Microsoft Teams API to return 502 Service Error.","Adaptive Card's `body.columns` serialized as strings, not objects, causing Microsoft Teams API to return 502."],"transactions":["/api/0/internal/rpc/{service_name}/{method_name}/","sentry.rules.processing.delayed_processing"],"title":"MS Teams notification requests return 502 Bad Gateway","description":"Sending incident/notification cards to Microsoft Teams fails with upstream 502 responses during POST to the activity endpoint, triggering ApiError(ServiceError) and error reporting in the worker and RPC paths. Impact: notifications are not delivered to Teams channels.","tags":["External System","API","Networking","Upstream Unavailable","HTTP 502","Microsoft Teams"],"cluster_size":2,"cluster_min_similarity":0.962032975591851,"cluster_avg_similarity":0.962032975591851},{"project_ids":["1"],"cluster_id":726,"group_ids":[6752023080,6760531827],"issue_titles":["ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')"],"root_cause_summaries":["Long-running Redis Lua script for span segment cleanup exceeds server timeout, causing connection reset.","Redis server terminates connection during long-running Lua script execution, causing 'Connection reset by peer' error."],"transactions":["sentry.tasks.post_process.post_process_group","sentry.spans.buffer in process_spans"],"title":"Redis socket connection resets during post-processing","description":"Multiple workers experience 'connection reset by peer' while executing post-processing steps, indicating unstable Redis socket connections that disrupt rule notifications and span processing.","tags":["Networking","Caching","Queueing","Connection Reset","Redis","Celery Worker"],"cluster_size":2,"cluster_min_similarity":0.9522498339996519,"cluster_avg_similarity":0.9522498339996519},{"project_ids":["1"],"cluster_id":728,"group_ids":[6752603344,6805824582],"issue_titles":["ValueError: Invalid address \"@MonsieurDart\""],"root_cause_summaries":["Invalid email address stored in database causes email sending failure.","Invalid email address \"@MonsieurDart\" used as recipient, failing Django's email validation during send."],"transactions":["sentry.utils.email.send in send_messages"],"title":"Email header parsing fails on invalid address","description":"Email sending fails due to malformed recipient/sender fields (e.g., '@MonsieurDart'), triggering HeaderParseError in the email parsing pipeline.","tags":["Input Validation","API","Email","HeaderParseError","Invalid Address"],"cluster_size":2,"cluster_min_similarity":0.9674144634080115,"cluster_avg_similarity":0.9674144634080115},{"project_ids":["1"],"cluster_id":731,"group_ids":[6753060155,6753060169],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\nCONTEXT:  while rechecking updated tuple (35368,81) in relation \"hybridcloud_regioncacheversion\"\\n')","OperationalError: QueryCanceled('canceling statement due to user request\\nCONTEXT:  while rechecking updated tuple (48402,40) in relation \"hybridcloud_regioncacheversion\"\\n')"],"root_cause_summaries":["Cache contains single JSON object, not array, causing TypeError during deserialization, triggering database query cancellation.","Malformed cache data caused TypeError, triggering database update that was cancelled, leading to OperationalError."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound"],"title":"PostgreSQL query canceled updating region cache version","description":"Updates to hybridcloud_regioncacheversion are being canceled by PostgreSQL while rechecking updated tuples, likely due to concurrent updates during cache invalidation in Sentry App installation lookup. This cancellation also triggers a follow-on TypeError when recording the failure.","tags":["Database","Concurrency","API","PostgreSQL","Query Canceled","Hybrid Cloud","Sentry Apps"],"cluster_size":2,"cluster_min_similarity":0.9748420678606373,"cluster_avg_similarity":0.9748420678606373},{"project_ids":["1"],"cluster_id":736,"group_ids":[6753815470,6793408501,6805654159],"issue_titles":["ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"5f4a676a5d294b89959506665b2f7abf\"}","ApiError: {\"detail\":\"Internal Error\",\"errorId\":\"bbc01e9292014173981cacb6834cffb7\"}","ApiError"],"root_cause_summaries":["GitHub API returned 500 Internal Server Error when fetching pull request files, causing Sentry's integration proxy to relay the failure.","Control silo's GitHub API calls fail due to network instability or overload, causing 500 errors to region silos.","GitHub API returned 500 Internal Server Error when Sentry requested pull request data, causing task failure."],"transactions":["sentry.integrations.source_code_management.tasks.open_pr_comment_workflow","sentry.tasks.process_commit_context"],"title":"PR comment workflow fails on upstream 500s","description":"Calls to the repository providers REST endpoints for PR files and commit-to-PR lookup are returning HTTP 500s, causing ApiError/HTTPError and aborting the open_pr_comment_workflow. Impact: PR comment generation and commit context processing are skipped when the upstream responds with Internal Server Error.","tags":["External System","API","Upstream Unavailable","HTTP 500","Pull Request Files","Commit Context"],"cluster_size":3,"cluster_min_similarity":0.9600843990810249,"cluster_avg_similarity":0.9633699691581764},{"project_ids":["1"],"cluster_id":737,"group_ids":[6754631354,6754896211],"issue_titles":["KeyError: Partition(topic=Topic(name='uptime-results'), index=48)","KeyError: Partition(topic=Topic(name='uptime-results'), index=113)"],"root_cause_summaries":["Race condition in `arroyo`'s Kafka consumer: concurrent modification of `__staged_offsets` between existence check and deletion causes `KeyError` during partition revocation.","Kafka rebalance revokes partition, causing commit loop to KeyError when accessing its removed lock."],"transactions":["sentry.remote_subscriptions.consumers.queue_consumer in _get_partition_lock","sentry.utils.kafka in run_processor_with_signals"],"title":"Kafka consumer KeyError on missing partition lock","description":"Consumer commit loop crashes with KeyError when accessing a per-partition lock that was not initialized or was removed during rebalance, causing offset commit failures.","tags":["Queueing","Concurrency","Kafka","Offset Commit","KeyError"],"cluster_size":2,"cluster_min_similarity":0.9608286320013592,"cluster_avg_similarity":0.9608286320013592},{"project_ids":["1"],"cluster_id":747,"group_ids":[6756108655,6795324934],"issue_titles":["APIConnectionError: Unexpected error communicating with Stripe.  If this problem persists,"],"root_cause_summaries":["Stripe Python client attempts to reuse HTTP connection already closed by Stripe API server, causing `RemoteDisconnected`.","Stripe API connection closed by remote end due to server-side timeout, unhandled by client's indefinite wait."],"transactions":["tasks.invoices.charge_invoice"],"title":"Stripe PaymentIntent creation sees remote disconnects","description":"During invoice charging in the billing worker, calls to Stripe PaymentIntent.create intermittently fail with RemoteDisconnected/connection aborted errors, indicating Stripe closed the connection without responding. This results in failed subscription charge attempts and retried Celery tasks.","tags":["Networking","External System","API","Stripe","Remote Disconnected","Connection Reset"],"cluster_size":2,"cluster_min_similarity":0.9594019766917813,"cluster_avg_similarity":0.9594019766917813},{"project_ids":["1"],"cluster_id":749,"group_ids":[6756238203,6756365445],"issue_titles":["OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: FATAL:  client_login_timeout (server down)"],"root_cause_summaries":["PgBouncer service was unavailable at 127.0.0.1:6432, causing database connection timeout during migration.","PgBouncer on port 6432 is unresponsive, preventing database connections for cleanup command."],"transactions":["sentry.new_migrations.monkey in _ensure_patched_impl","cleanup"],"title":"PostgreSQL login timeout during Django tasks","description":"Django management commands fail to connect to PostgreSQL, hitting client_login_timeout, indicating the database service is down or unreachable.","tags":["Database","Networking","PostgreSQL","Timeout","Django"],"cluster_size":2,"cluster_min_similarity":0.9537763604134671,"cluster_avg_similarity":0.9537763604134671},{"project_ids":["1"],"cluster_id":750,"group_ids":[6756900294,6783773383,6783773384,6794010644,6799157889],"issue_titles":["MaxRetryError: HTTPConnectionPool(host='seer-gpu-web-group-ingest', port=80): Max retries exceeded with url: /v0/issues/similar-issues/grouping-record/delete-by-hash (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e05b59e1ba0>: Failed to...","ConnectionError: HTTPConnectionPool(host='seer-web-autofix', port=80): Max retries exceeded with url: /v1/automation/autofix/state (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e42204c7530>: Failed to establish a new connection: [Errno 1...","ConnectionError: HTTPConnectionPool(host='tempest', port=80): Max retries exceeded with url: /latest-id (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7844025cd310>: Failed to establish a new connection: [Errno 111] Connection refused'))","ConnectionError: HTTPConnectionPool(host='tempest', port=80): Max retries exceeded with url: /crashes (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a38af9f1a90>: Failed to establish a new connection: [Errno 111] Connection refused'))","ConnectionError: HTTPConnectionPool(host='tempest', port=80): Max retries exceeded with url: /latest-id (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c539586e490>: Failed to establish a new connection: [Errno 111] Connection refused'))"],"root_cause_summaries":["Sentry's `SEER_AUTOFIX_URL` environment variable override points to `seer-web-autofix:80`, but no service listens there, causing connection refusal.","Sentry attempts to connect to Seer on default port 80, but Seer listens on 8081, causing connection refused errors.","Task worker connects to `tempest` on port 80, but service listens on 8123, causing connection refused.","Task worker's `SENTRY_TEMPEST_URL` resolves to default port 80, but Tempest service listens on port 8123, causing connection refusal.","Tempest service unreachable at `tempest:80`, causing connection refusal during crash fetching."],"transactions":["/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/autofix/","sentry.tempest.tasks.fetch_latest_item_id","sentry.seer.similarity.grouping_records in call_seer_to_delete_these_hashes","sentry.tempest.tasks.poll_tempest_crashes"],"title":"HTTP connections to Tempest/Seer are refused","description":"Workers and API handlers fail to reach Tempest (/latest-id, /crashes) and Seer (/v0/issues/.../delete-by-hash, /v1/automation/autofix/state), with urllib3/requests exhausting retries due to connection refusals. This suggests the upstream services are down or unreachable, breaking grouping record deletes, crash polling, and autofix state retrieval.","tags":["Networking","External System","API","Connection Refused","Retries Exhausted","urllib3","Requests"],"cluster_size":5,"cluster_min_similarity":0.9404669518146577,"cluster_avg_similarity":0.9588495821569405},{"project_ids":["1"],"cluster_id":757,"group_ids":[6760287578,6760293879],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.integrations.tasks.create_comment","ProcessingDeadlineExceeded: execution deadline of 45 seconds exceeded by sentry.integrations.source_code_management.tasks.pr_comment_workflow"],"root_cause_summaries":["Database contention on `hybridcloud_regioncacheversion` table, due to `select_for_update` locks, caused user lookup to timeout.","Concurrent tasks updating the same `PullRequestComment` record cause database lock contention, exceeding the task's 45-second processing deadline."],"transactions":["sentry.integrations.source_code_management.tasks.pr_comment_workflow","sentry.integrations.tasks.create_comment"],"title":"SCM integration comment tasks exceed processing deadline","description":"Background tasks for PR and issue comments in the Source Code Management integration time out, hitting the worker processing deadline while saving/updating comments and fetching user data. This causes comment creation/update to fail and generates error events.","tags":["Queueing","API","Resource Limits","Timeout","Background Task","SCM Integration","ProcessingDeadlineExceeded"],"cluster_size":2,"cluster_min_similarity":0.9561511621075177,"cluster_avg_similarity":0.9561511621075177},{"project_ids":["1"],"cluster_id":759,"group_ids":[6760531795,6760540220,6760540223,6760540236,6760626886,6760626921],"issue_titles":["ConnectionError: Error 111 connecting to 192.168.208.40:11499. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11485. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11509. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11469. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11459. Connection refused."],"root_cause_summaries":["Redis cluster client attempts connecting to a stale, unreachable node, causing connection refused errors during span buffer cleanup.","Redis connection refused during span cleanup, indicating Redis instance became unavailable or misconfigured between read and write operations.","Redis cluster node at 192.168.208.40:11485 actively refused connection, preventing span buffer cleanup.","Redis cluster node 192.168.208.40:11469 refused connection, preventing spans buffer cleanup.","Redis cluster node at 192.168.208.40:11459 refused connection, preventing span buffer cleanup due to infrastructure unavailability.","Redis cluster node 192.168.208.40:11485 refused connection, preventing span data cleanup due to partial cluster failure."],"transactions":["sentry.spans.buffer in done_flush_segments"],"title":"Flusher fails to connect during buffer segment flush","description":"Arroyo flusher processes encounter connection refused errors when executing buffer.done_flush_segments, indicating the target service is not accepting connections. This blocks segment flush execution and likely stalls downstream processing.","tags":["Networking","Queueing / Messaging","Connection Refused","Arroyo","Flusher"],"cluster_size":6,"cluster_min_similarity":0.9546127975285124,"cluster_avg_similarity":0.9619937295258881},{"project_ids":["1"],"cluster_id":760,"group_ids":[6760531799,6760531802,6760531804,6760531815,6760531828,6760540224,6760540240,6760546322,6760546327,6760555881,6760581883,6760581884,6760581890,6760581891,6760581896,6760581899,6760581940,6760593925,6760593949,6760626882,6760626884,6760626888,6760626890,6760626893,6760626898,6760626900,6760626902,6760626903,6760626911,6760626918,6760626923,6760626929,6760626936,6760626938],"issue_titles":["ConnectionError: Error 111 connecting to 192.168.208.40:11485. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11499. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11495. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.83:11480. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11487. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11459. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11501. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11469. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.83:11492. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11479. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11465. Connection refused."],"root_cause_summaries":["Redis connection refused due to external configuration pointing to an unreachable or non-existent Redis instance.","Redis server at 192.168.208.40:11485 is unreachable, causing connection refused errors during SpansBuffer operations.","Redis cluster node 192.168.208.83:11464 refused connection, preventing Sentry's span buffer from flushing data.","Redis node at 192.168.208.40:11499 refused connection, indicating infrastructure failure or misconfiguration.","Redis server at 192.168.208.40:11499 is unreachable, causing connection refused errors during span buffer cleanup.","Redis server at 192.168.208.40:11487 refused connection, indicating an infrastructure misconfiguration or unavailability.","Redis server at 192.168.208.40:11499 actively refused connection, indicating service unavailability or network blockage.","Redis server at 192.168.208.40:11499 actively refused connection, preventing span buffer cleanup.","Redis node `192.168.208.40:11499` refused connection during span buffer cleanup, indicating node unavailability or network partition.","Redis cluster node 192.168.208.40:11499 is unreachable, causing connection refused errors during span buffer operations.","Redis server at 192.168.208.40:11465 refused connection, preventing Sentry's span buffering.","Redis node at 192.168.208.40:11495 refused connection, halting span processing.","Redis cluster node 192.168.208.40:11499 is unreachable or not accepting connections, causing `ConnectionRefusedError` during span buffer operations.","Redis server at 192.168.208.40:11499 actively refused connection, preventing span buffer operations.","Redis server at 192.168.208.40:11501 refused connection, preventing span buffer operations.","Redis cluster node 192.168.208.40:11479 refused connection, preventing span data flushing.","Redis server at 192.168.208.40:11499 refused connection, halting span processing.","Redis cluster node 192.168.208.40:11487 refused connection, preventing span data retrieval.","Redis cluster node at 192.168.208.40:11487 refused connection, preventing span buffer operations.","Redis server at 192.168.208.40:11499 is unreachable, causing connection refusal during span buffer operations.","Redis server at 192.168.208.40:11459 refused connection, preventing span data retrieval.","Redis server at 192.168.208.40:11499 refused connection, indicating an infrastructure issue preventing span processing.","Redis cluster node 192.168.208.40:11499 refused connection, indicating node unavailability or network blockage.","Redis server at 192.168.208.40:11479 refused connection, preventing span buffer operations.","Redis connection refused: The application attempted to connect to a Redis instance at `192.168.208.40:11499`, but no service was listening on that port.","Redis server at `192.168.208.40:11499` refused connection, preventing span buffer operations.","Redis server at 192.168.208.83:11492 refused connection, preventing span buffer cleanup and causing data accumulation.","Redis cluster node at 192.168.208.40:11499 refused connection, indicating an infrastructure network configuration problem.","Redis server at 192.168.208.40:11487 refused connection, preventing span buffer operations.","Redis cluster node 192.168.208.40:11499 actively refused connection, preventing span buffer operations due to node unavailability.","Redis server at 192.168.208.40:11469 refused connection, preventing span buffer cleanup operations.","Redis cluster node at 192.168.208.40:11499 is unreachable or not accepting connections, causing connection refusal.","Redis service at 192.168.208.40:11499 actively refused connection, preventing span data retrieval.","Redis server at 192.168.208.40:11499 refused connection, indicating an infrastructure failure (server down, network issue, or misconfiguration)."],"transactions":["sentry.spans.buffer in _load_segment_data","sentry.spans.buffer in flush_segments","sentry.spans.buffer in done_flush_segments"],"title":"Buffer flusher connections to Redis are refused","description":"The flusher process in arroyo/buffer fails when executing pipeline commands (p.execute()), as connections to the Redis backend are refused. This prevents segment data from being loaded and flush completion from being recorded.","tags":["Networking","Caching","External System","Redis","Connection Refused","Pipeline Execute","Flusher"],"cluster_size":34,"cluster_min_similarity":0.9302153119094966,"cluster_avg_similarity":0.9598984142676604},{"project_ids":["1"],"cluster_id":761,"group_ids":[6760531819,6760540183,6760579483,6760579484],"issue_titles":["ConnectionError: Error 111 connecting to 192.168.208.40:11485. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11493. Connection refused.","ConnectionError: Error 111 connecting to 192.168.208.40:11501. Connection refused."],"root_cause_summaries":["Redis cluster node 192.168.208.40:11501 refused connection during SCRIPT LOAD, causing `RetryingRedisCluster` to fail and consumer to terminate.","Redis client's cluster-wide script check fails due to one master node refusing connection, halting span processing.","Redis Cluster master node 192.168.208.40:11501 is unreachable, causing cluster-wide operations to fail.","Redis cluster node 192.168.208.40:11485 actively refused connection, preventing Sentry's SpansBuffer from loading a required script."],"transactions":["sentry.spans.buffer in _ensure_script"],"title":"Redis connections to buffer store are failing","description":"The span buffer code fails when checking/loading Lua scripts because Redis is unreachable or unstable, resulting in connection refused, resets, and timeouts. This disrupts the Kafka consumers batch processing in the buffer pipeline.","tags":["Networking","External System","Queueing","Redis","Connection Refused","Connection Reset","Timeout"],"cluster_size":4,"cluster_min_similarity":0.9518858823006237,"cluster_avg_similarity":0.9617553589992768},{"project_ids":["1"],"cluster_id":766,"group_ids":[6760540211,6760540225],"issue_titles":["ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')"],"root_cause_summaries":["Redis `ConnectionResetError` occurs because `done_flush_segments` sends excessively large pipelines, overwhelming the Redis server.","Unbounded Redis pipeline size in `done_flush_segments` exceeds server/network limits, causing connection resets."],"transactions":["sentry.spans.buffer in done_flush_segments"],"title":"Flusher worker socket connection resets during segment flush","description":"Flusher process calls buffer.done_flush_segments and p.execute when the socket is reset by peer, causing repeated ConnectionResetError. Likely an upstream socket drop during flush operations in the Arroyo-based worker.","tags":["Networking","Queueing","Connection Reset","Arroyo","Flusher"],"cluster_size":2,"cluster_min_similarity":0.9766996663720616,"cluster_avg_similarity":0.9766996663720616},{"project_ids":["1"],"cluster_id":770,"group_ids":[6761349879,6792433167,6792867779],"issue_titles":["IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist.","ValueError: IncidentGroupOpenPeriod does not exist"],"root_cause_summaries":["Slack notification attempts `IncidentGroupOpenPeriod` lookup before its asynchronous creation/commit, causing `DoesNotExist` error.","Notification task attempts to retrieve `IncidentGroupOpenPeriod` before asynchronous incident creation completes, causing `DoesNotExist`.","Race condition: Notification handler accesses IncidentGroupOpenPeriod before its deferred creation completes via transaction.on_commit."],"transactions":["sentry.workflow_engine.tasks.trigger_action"],"title":"Missing IncidentGroupOpenPeriod during alert notifications","description":"Slack and email metric alert handlers attempt to load IncidentGroupOpenPeriod but no matching record exists, causing notification builds to fail. Likely a missing or already-closed incident period record referenced during alert rendering.","tags":["API","Data Integrity","Configuration","Django ORM","Record Not Found","Slack","Email Notifications"],"cluster_size":3,"cluster_min_similarity":0.9454305680915649,"cluster_avg_similarity":0.9553349546343419},{"project_ids":["1"],"cluster_id":775,"group_ids":[6762871448,6762871790],"issue_titles":["ValueError: Event not found: event_id=5b2c4a4900034938adaac472b3e94438, project_id=4508693801533440","ValueError: Event not found: event_id=2810e7b8bed341c586e93a8f8d072003, project_id=4506937772670976"],"root_cause_summaries":["Event data is deleted from nodestore before `process_workflows_event` task can fetch it, causing `ValueError` due to missing data.","Workflow task execution races event data persistence or deletion in Nodestore, causing `Event not found` due to data unavailability."],"transactions":["sentry.workflow_engine.tasks.process_workflows_event"],"title":"Workflow processing fails when event not found","description":"The workflows processor raises ValueError during process_workflows_event because event data cannot be retrieved by event_id/project_id, causing worker tasks to fail. Likely missing or deleted event records or lookup mismatch.","tags":["Data Integrity","API","Sentry","Not Found","Background Worker"],"cluster_size":2,"cluster_min_similarity":0.952425327863092,"cluster_avg_similarity":0.952425327863092},{"project_ids":["1"],"cluster_id":781,"group_ids":[6765491440,6813837928],"issue_titles":["ApiError: upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: immediate connect error: Resource temporarily unavailable","ApiError: upstream connect error or disconnect/reset before headers. reset reason: overflow"],"root_cause_summaries":["System resource exhaustion causes connection overflow to Discord API, preventing message delivery.","Outbound network connection failures to Discord API due to Sentry's infrastructure resource exhaustion."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.shared_integrations.client.base in _request"],"title":"Slack alert posts fail with upstream 503 errors","description":"Alerting and post-process workers fail to send Slack notifications due to upstream connect/reset errors, resulting in HTTP 503 Service Unavailable responses from the Slack API client.","tags":["Networking","External System","API","Upstream Unavailable","Connection Reset","Slack"],"cluster_size":2,"cluster_min_similarity":0.9650736808757423,"cluster_avg_similarity":0.9650736808757423},{"project_ids":["11276"],"cluster_id":784,"group_ids":[6765971664,6771472765],"issue_titles":["SyntaxError: Unexpected end of script"],"root_cause_summaries":["Frontend JavaScript files are truncated during delivery, causing browser parsing to fail due to unexpected end-of-script.","JavaScript bundles are truncated during transmission, causing browser parsing failures and `SyntaxError: Unexpected end of script`."],"transactions":["/issues/:groupId/"],"title":"JavaScript syntax error: unexpected end of script","description":"Multiple frontend TypeScript/JS files stop parsing due to incomplete code blocks (e.g., unterminated template literal or loop), causing build/runtime failures.","tags":["Configuration","Serialization","JavaScript","SyntaxError","TypeScript"],"cluster_size":2,"cluster_min_similarity":0.9802564330846818,"cluster_avg_similarity":0.9802564330846818},{"project_ids":["1"],"cluster_id":796,"group_ids":[6768949093,6797684393],"issue_titles":["TierNotFound: log_bytes: 0"],"root_cause_summaries":["Billing plan for `log_bytes` lacks a pricing tier for zero quantity, causing `TierNotFound` when calculating costs for zero usage.","Am3LogBytePlanItem's reserved_tiers lack a 0-quantity tier, causing TierNotFound when calculating price for 0 log bytes."],"transactions":["getsentry.tasks.usage.process_billing_notification_batch","/api/0/subscriptions/{organization_id_or_slug}/"],"title":"Tiered plan pricing fails with missing tier","description":"TieredPlanItem.get_tier raises TierNotFound when computing reserved pricing for current_plan_price, impacting subscription ACV serialization and billing forecast alerts. Likely caused by quantities not matching any defined pricing tier in the plan configuration.","tags":["Configuration","API","Data Integrity","Pricing Tier","Subscription Billing","TierNotFound"],"cluster_size":2,"cluster_min_similarity":0.9714309844043872,"cluster_avg_similarity":0.9714309844043872},{"project_ids":["6178942"],"cluster_id":801,"group_ids":[6770211816,6770211822],"issue_titles":["GithubException: 403 {\"message\": \"This installation has been suspended\", \"documentation_url\": \"https://docs.github.com/rest/reference/apps#create-an-installation-access-token-for-an-app\", \"status\": \"403\"}"],"root_cause_summaries":["GitHub App installation for `carta/fund-admin` suspended, preventing Seer from obtaining access tokens for repository write operations.","GitHub App installation for `carta/fund-admin` is suspended, preventing access token generation."],"transactions":["seer.automation.autofix.tasks.commit_changes_task"],"title":"GitHub App installation is suspended","description":"Calls to fetch repositories via the GitHub App fail because the target installation is suspended, causing client initialization to error when resolving repo by full name or external ID.","tags":["External System","API","GitHub","Installation Suspended"],"cluster_size":2,"cluster_min_similarity":0.9714997777172418,"cluster_avg_similarity":0.9714997777172418},{"project_ids":["1"],"cluster_id":806,"group_ids":[6770653447,6770664482,6771782221],"issue_titles":["TemplateSyntaxError: Invalid block tag on line 143: 'trans'. Did you forget to register or load this tag?"],"root_cause_summaries":["CSRF token length mismatch causes request rejection; subsequent error page rendering fails due to unrecognized template tag.","Django's i18n template tags are unregistered due to Sentry's custom settings loading, causing `trans` tag `KeyError`.","Salesforce SDK POSTed to a CSRF-protected Django endpoint, causing rejection and subsequent template rendering failure due to missing i18n tags."],"transactions":["/account/recover/","/api/[0]+/"],"title":"Django template tag 'trans' missing; CSRF failures","description":"Rendering the 403 CSRF failure page triggers a TemplateSyntaxError due to the unregistered 'trans' tag, while requests also fail CSRF checks (missing cookie or invalid token). This likely stems from missing i18n template tags or context processors combined with strict CSRF validation in the Sentry/Django middleware stack.","tags":["Configuration","Authentication","Input Validation","Django","TemplateSyntaxError","CSRF","Missing Template Tag"],"cluster_size":3,"cluster_min_similarity":0.9397975184593598,"cluster_avg_similarity":0.949732660162253},{"project_ids":["1"],"cluster_id":810,"group_ids":[6772708008,6773149010,6785429717],"issue_titles":["OperationalError: QueryCanceled('canceling statement due to user request\\nCONTEXT:  while updating tuple (5390843,61) in relation \"accounts_billingmetrichistory\"\\n')","OperationalError: QueryCanceled('canceling statement due to user request\\nCONTEXT:  while updating tuple (1361290,3) in relation \"accounts_billinghistory\"\\n')","OperationalError: canceling statement due to user request"],"root_cause_summaries":["Taskworker's `flush_usage_buffer` exceeds its 150-second processing deadline, triggering a `SIGALRM` that cancels the long-running Postgres `UPDATE` statement.","Database update in `tally_usage` exceeds `flush_usage_buffer` taskworker's 150-second processing deadline, causing query cancellation.","Database update exceeds task worker deadline, causing query cancellation and subsequent data inconsistency leading to assertion errors."],"transactions":["sentry.db.models.manager.base_query_set in update"],"title":"PostgreSQL updates to billing tables are canceled","description":"Update statements on accounts_billinghistory and accounts_billingmetrichistory are being canceled by user request during tally_usage in the billing pipeline, likely due to lock contention or explicit cancel/timeouts in workers.","tags":["Database","Concurrency","Configuration","PostgreSQL","Update Statement","Query Canceled","Lock Contention"],"cluster_size":3,"cluster_min_similarity":0.9667325767901569,"cluster_avg_similarity":0.9711220662724216},{"project_ids":["1"],"cluster_id":811,"group_ids":[6772907537,6774814403,6779047596,6793990277],"issue_titles":["RefreshError: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 503,\\n    \"message\": \"Unable to extract the resource from the request.\",\\n    \"status\": \"UNAVAILABLE\"\\n  }\\n}\\n')","RefreshError: ('Unable to acquire impersonated credentials', '<!DOCTYPE html>\\n<html lang=en>\\n  <meta charset=utf-8>\\n  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\\n  <title>Error 502 (Server Error)!!1</title>\\n  <style>\\n    ..."],"root_cause_summaries":["Google IAM service unavailability prevented Sentry from acquiring impersonated credentials for symbolication, causing a RefreshError.","GCP IAM service returned 503, preventing Sentry from acquiring impersonated credentials for symbolication.","GCP service account `system-symbols-0-reader` is inaccessible, preventing token generation for symbolication.","Google Cloud IAM API rejected service account impersonation due to malformed/non-existent service account identifier."],"transactions":["sentry.profiles.task.process_profile"],"title":"GCP impersonated credential refresh fails in symbolication","description":"Symbolication attempts to fetch Google Cloud tokens for project sources but fail to refresh impersonated credentials, returning UNAVAILABLE or a 5xx HTML error from Google. This blocks profile symbolication in the processing task.","tags":["External System","Authentication","API","Google Cloud","Impersonated Credentials","UNAVAILABLE","Upstream Unavailable"],"cluster_size":4,"cluster_min_similarity":0.9525893199785401,"cluster_avg_similarity":0.9690585556987837},{"project_ids":["1"],"cluster_id":813,"group_ids":[6773700243,6795558832],"issue_titles":["KafkaException: KafkaError{code=MSG_SIZE_TOO_LARGE,val=10,str=\"Unable to produce message: Broker: Message size too large\"}"],"root_cause_summaries":["Kafka message size exceeded due to large base64-encoded WebAssembly modules in profile function package fields.","Vroomrs includes full WASM binaries in function metrics, creating oversized Kafka messages exceeding broker limits."],"transactions":["sentry.profiles.task.process_profile"],"title":"Kafka produce fails: message size too large in profiling worker","description":"Celery profiling tasks fail when producing payloads to Kafka because the message exceeds the brokers max.message.bytes, causing KafkaException(MSG_SIZE_TOO_LARGE). This blocks transaction profile data from being published by the profile_functions_producer.","tags":["Queueing","Configuration","Kafka","Msg Size Too Large","Celery"],"cluster_size":2,"cluster_min_similarity":0.9820517317147199,"cluster_avg_similarity":0.9820517317147199},{"project_ids":["1"],"cluster_id":825,"group_ids":[6777158365,6792352326],"issue_titles":["SMTPResponseException: (451, b'4.4.2 Timeout waiting for data from client.')","SMTPServerDisconnected: Connection unexpectedly closed"],"root_cause_summaries":["SMTP server timed out waiting for Sentry's TLS handshake data, causing `SMTPResponseException`.","SMTP connection times out during EHLO command due to insufficient `mail.timeout` (10s default) and slow server response."],"transactions":["sentry.utils.email.send in send_messages"],"title":"SMTP timeouts and disconnects during email send","description":"Email sending tasks encounter SMTP timeouts and unexpected server disconnects while calling send_messages, causing message delivery failures in the worker process.","tags":["Networking","External System","API","SMTP","Timeout","Connection Reset"],"cluster_size":2,"cluster_min_similarity":0.9506037243871128,"cluster_avg_similarity":0.9506037243871128},{"project_ids":["1"],"cluster_id":828,"group_ids":[6777598641,6777598784],"issue_titles":["TimeoutError: Timeout connecting to server","MovedError: 1732 192.168.209.22:11133"],"root_cause_summaries":["Redis cluster instability causes non-critical cache cleanup to timeout, incorrectly failing successful transaction processing tasks.","Uncaught Redis errors during non-critical cache cleanup cause transaction processing tasks to fail, masking successful event persistence."],"transactions":["sentry.tasks.store.save_event_transaction"],"title":"Redis timeouts and MOVED errors during cache delete","description":"Workers deleting processing cache keys in save_event_transaction are failing with Redis timeouts and MOVED redirections, suggesting cluster slot reconfiguration or unreachable nodes causing client operations to stall or be redirected.","tags":["Caching","Networking","Redis","Timeout","MOVED Redirection","Cluster Rebalance"],"cluster_size":2,"cluster_min_similarity":0.9611412111299665,"cluster_avg_similarity":0.9611412111299665},{"project_ids":["1"],"cluster_id":832,"group_ids":[6778545504,6796619464],"issue_titles":["SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"],"root_cause_summaries":["Slack API returned 503 Service Unavailable with HTML, causing Slack SDK to raise SlackApiError due to non-JSON response.","Sequential Slack API calls without rate limiting or backoff cause Slack's API to return 503 Service Unavailable with non-JSON responses."],"transactions":["sentry.integrations.slack.tasks.send_activity_notifications_to_slack_threads"],"title":"Slack API returns HTML error to chat.postMessage","description":"Slack notifications fail when the Slack API responds with an HTML Server Error page instead of JSON, causing SlackApiError in the notification service during chat_postMessage calls.","tags":["External System","API","Serialization","Slack","Non-JSON Response","Server Error"],"cluster_size":2,"cluster_min_similarity":0.9539822051256833,"cluster_avg_similarity":0.9539822051256833},{"project_ids":["1"],"cluster_id":836,"group_ids":[6780305017,6780305018,6780305027,6780305029,6780305037,6780305038,6780305040,6780305041,6780305043,6780305053,6780305056,6780305062,6780305064,6780305066,6780305081,6780305083,6780305088],"issue_titles":["ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:d001fcec9c1773448c441de05efced7c m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:241456e30c9413cf2a142f8f22c59d7e m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:68ce63501014619a1540e0e28e8288b9 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:54a88d44ba72a1f716f68ceb957948b6 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:b5d4cbcd06067df27562052b968a51c1 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:1b2141b17f5c937b959c2c14c78f240a m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.grouprelease:6280cec1d7a45109bd9e6bc216a199b5 m sentry.models.grouprelease.GroupRelease) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:0f1081c257a39b0c80499a805464d6e9 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:b773ce71a1e04c934e19ba4abfc6ef7e m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:a4552877692d450f02d2a1c24820a5be m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:041319b682b78667c2e19a6e40cb7941 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:d0b1b86270485d80ad6133d1fda9d7f1 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:cc1e438e98633fbbf90ece5474cd5c8b m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:50af592871185eb317f5b8c16bd430a8 m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:ca5935f4d6fae7f801fbe477d8e66c1c m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica.","ReadOnlyError: Command # 1 (HSETNX b:k:sentry.group:6d00cd7ebef0308240d5746fabb7de8e m sentry.models.group.Group) of pipeline caused error: You can't write against a read only replica."],"root_cause_summaries":["Redis infrastructure misconfigured; Sentry attempts writes to a read-only replica, causing `ReadOnlyError`.","Redis client configured with `readonly_mode=True` routes write operations to read-only replicas, causing `ReadOnlyError`.","Redis buffer attempts writes on a read-only replica due to infrastructure misconfiguration.","Redis buffer attempts writes to a read-only replica, causing `ReadOnlyError` due to configuration mismatch.","Redis buffer cluster misconfigured, routing write operations to a read-only replica, causing `ReadOnlyError`.","Redis buffer attempts writes on a read-only replica due to connection routing misconfiguration.","Redis client attempted write operations on a read-only replica, likely due to stale cluster topology information after a failover.","Redis instance, expected writable by Sentry's buffer, is a read-only replica, causing write failures.","Redis write operations failed because the target Redis instance was an infrastructure-configured read-only replica.","RedisBuffer writes to a read-only replica due to Redis cluster routing misconfiguration.","Redis client attempts writes on a read-only replica, causing `ReadOnlyError` due to misconfigured routing.","Redis buffer write failed because `SENTRY_REDIS_HOST` pointed to a read-only replica, not a writable master.","Redis instance configured as read-only replica rejected application's write operation, causing ReadOnlyError.","Redis buffer attempted write on read-only replica; infrastructure misconfiguration.","Redis routing client incorrectly directs write operations to a read-only replica, causing `HSETNX` and subsequent commands to fail.","Redis instance `redis-buffer-3` is a read-only replica, rejecting application's write operations during buffering.","Redis buffer attempts writes to a read-only replica, causing `ReadOnlyError` and pipeline abortion."],"transactions":["ingest_consumer.process_event","issues.occurrence_consumer"],"title":"Redis writes hitting read-only replica during event save","description":"Event processing workers attempt to update Sentry group and group release counters via Redis pipelines, but the Redis node is a read-only replica, causing pipeline aborts. This misrouting/configuration leads to ExecAbortError after ReadOnlyError and prevents event aggregation updates.","tags":["Caching","Configuration","Redis","Read Only Replica","ExecAbortError","Pipeline"],"cluster_size":17,"cluster_min_similarity":0.9565325605970436,"cluster_avg_similarity":0.9744196956002019},{"project_ids":["1"],"cluster_id":837,"group_ids":[6780368263,6789304594,6790297390,6791435859],"issue_titles":["TypeError: TypeError('Object of type datetime is not JSON serializable')","TypeError: Object of type datetime is not JSON serializable"],"root_cause_summaries":["Datetime objects within the `query` JSONField are not converted to strings before database serialization, causing a TypeError.","Serializer's `validate` method passes non-JSON-serializable `datetime` objects to `JSONField`.","ExploreSavedQuerySerializer stores raw datetime objects in JSONField, causing TypeError during database serialization.","Datetime objects are stored in a JSONField without prior serialization, causing a TypeError."],"transactions":["/api/0/organizations/{organization_id_or_slug}/explore/saved/","/api/0/organizations/{organization_id_or_slug}/explore/saved/{id}/"],"title":"Datetime in query JSON breaks ExploreSavedQuery writes","description":"Create/update of ExploreSavedQuery fails when serializing the query payload to JSON because it contains a Python datetime, which the JSON encoder cannot serialize. This prevents inserts/updates during POST/UPDATE handlers in the Explore feature.","tags":["Database","Serialization","API","PostgreSQL","JSON Serialization Error","psycopg","TypeError"],"cluster_size":4,"cluster_min_similarity":0.9673485153332413,"cluster_avg_similarity":0.9716950983380227},{"project_ids":["1"],"cluster_id":839,"group_ids":[6780427944,6780933389],"issue_titles":["PluginError: Error Communicating with Pushover (HTTP 400): group has no users or active devices in it","PluginError: Error Communicating with Pushover (HTTP 400): application token is invalid, see https://pushover.net/api"],"root_cause_summaries":["Pushover integration failed because the configured group key lacked active users/devices, causing a 400 Bad Request.","Pushover API rejected notification due to invalid application token configured in Sentry integration."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"Pushover notifications fail due to invalid group/user or app token","description":"Sentry Pushover plugin requests to /messages.json are rejected by Pushover with Bad Request errors when the target group has no users/devices or the application token is invalid, preventing rule-based notifications from being delivered.","tags":["External System","API","Configuration","Pushover","HTTP 400","Invalid Credentials","Empty Recipient Group"],"cluster_size":2,"cluster_min_similarity":0.9518157609932512,"cluster_avg_similarity":0.9518157609932512},{"project_ids":["1"],"cluster_id":847,"group_ids":[6783626398,6790909939,6796137582],"issue_titles":["IntegrationError: Error Communicating with GitHub (HTTP 403): Repository was archived so is read-only."],"root_cause_summaries":["GitHub repository `Meltek-Inc/meltek_backend` is archived, causing 403 Forbidden on issue creation.","GitHub repository 'Meltek-Inc/meltek_backend' is archived, preventing Sentry from creating new issues.","GitHub integration failed to create issue because target repository was archived; Sentry lacked pre-validation for repository write permissions."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"GitHub issue creation fails on archived repo","description":"Post-processing attempts to create GitHub issues for an archived repository, and GitHub rejects the POST request because the repo is read-only, returning a Forbidden error. This blocks automated issue creation for that integration.","tags":["External System","API","Configuration","GitHub","Forbidden","Read-Only Repository"],"cluster_size":3,"cluster_min_similarity":0.9823130127235665,"cluster_avg_similarity":0.9836929865172519},{"project_ids":["1"],"cluster_id":863,"group_ids":[6789207809,6789207810,6789207815,6789207857,6789207858,6789207859,6789207863,6789207870,6789207871,6789207874,6789207887,6789207891,6789207893,6789207899,6789207900,6789207914,6789207916,6789207921,6789207931,6789207937,6789207938,6789207939,6789207940,6789207941,6789207944,6789207946,6789207953,6789207958,6789207962,6789207970,6789207974,6789207980,6789207991,6789207998,6789207999,6789208006,6789208011,6789208012,6789208026,6789208093,6789208096,6789208099,6789208103],"issue_titles":["ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=164664 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a89cdd84050>, 'Connection to symbolicator...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504577092550656 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fcbec075c70>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=113046 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7830087a1910>, 'Connection to symbolicator-j...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=6151949 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7e7b3ccc43e0>, 'Connection to symbolicato...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=4504611331244032 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7c99f8270e90>, 'Connection to sy...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505803031707648 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7ba9db1ee8d0>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1441773 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7880d05c1010>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=4507923204407296 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7c2eb44a7e30>, 'Connection to sy...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505803031707648 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7b64e4d9b770>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4505085246439425 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7cc04ae5c170>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4504652062261248 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7c4ad437ab10>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504963676831744 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7e8502eb8b90>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=257646 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a85c92c29f0>, 'Connection to symbolicator-j...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4507639048765440 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7e40b8243410>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1441773 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7b84f5421fd0>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1441773 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7e28c81b9910>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1385565 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x79393c481d90>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4504718419886080 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x78b02549d910>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1455246 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x782dc82b83b0>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4504888592367616 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x787efc7c7650>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=4505625393954816 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7830087a3890>, 'Connection to sy...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504963676831744 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x796e41adac30>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=4509351300235265 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a7a2c504770>, 'Connection to sy...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=1333470 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a2e22b72e70>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504084881932288 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x780f7c2cd130>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4506877890592768 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7da044442450>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504963676831744 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7b429ec53ad0>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=1224914 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7d6ae14cd5b0>, 'Connection to symbolicato...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /requests/0e9f5f7b-21fa-4734-88d2-7e384fa80a91?timeout=5&scope=5862847 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7d16f436e570...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4503967067537408 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fe07a643890>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505803031707648 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7d9a8c55dfd0>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504566679732224 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7a3f03c717f0>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4507639048765440 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f024a410770>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505803031707648 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7acb9416be30>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=5890145 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7e27443e79b0>, 'Connection to symbolicator-...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4508212697497600 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x79b41c609910>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=2299799 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7b565fb13530>, 'Connection to symbolicato...","ConnectTimeout: HTTPConnectionPool(host='symbolicator.sentry.', port=80): Max retries exceeded with url: /symbolicate?timeout=5&scope=4505504990691328 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f53b1f0b1d0>, 'Connection to symbolicat...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4506110904631296 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x785424608dd0>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504963676831744 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7abb257268d0>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /symbolicate-js?timeout=5&scope=4504963676831744 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7aa0aa093d10>, 'Connection to symb...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-jvm.sentry.', port=80): Max retries exceeded with url: /symbolicate-jvm?timeout=5&scope=4506159590408192 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x79a2cc41ad50>, 'Connection to sy...","ConnectTimeout: HTTPConnectionPool(host='symbolicator-js.sentry.', port=80): Max retries exceeded with url: /requests/1d05d499-df2a-472a-9916-26dee1a4e821?timeout=5&scope=4504314251771904 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fc..."],"root_cause_summaries":["Symbolicator services are unresponsive, causing TCP connection timeouts and MaxRetryError after all retries are exhausted.","Symbolicator service connection timed out; DNS resolved, but TCP handshake failed, indicating network or service unavailability.","Symbolicator JavaScript service unreachable, causing connection timeouts after all retries.","Symbolicator JVM service unreachable, causing connection timeouts after all retries are exhausted.","JVM symbolicator service unreachable, causing connection timeouts after all retries exhausted.","Kubernetes production environment misconfigures JavaScript symbolicator service, causing connection timeouts due to port mismatch.","Sentry worker fails to establish TCP connection to internal `symbolicator-js` service, exhausting retries due to persistent network connectivity issues.","Sentry attempts to connect to Symbolicator on port 80, but Symbolicator listens on port 3021, causing connection timeouts.","Symbolicator service unreachable: DNS resolution or network connectivity issues prevent connection to `symbolicator-js.sentry.` on port 80.","Sentry connects to Symbolicator JVM on port 80, but service listens on 3021, causing connection timeouts.","Symbolicator URL misconfigured without port, defaulting to port 80, causing connection timeouts to service listening on port 3021.","Symbolicator service became unresponsive during polling, causing connection timeouts and failed JavaScript symbolication.","Symbolicator service at `symbolicator-js.sentry.` is unreachable, causing connection timeouts during Node.js profile symbolication.","Symbolicator service unreachable; network connectivity or service availability issue prevents connection establishment.","Symbolicator URL configured without port, causing HTTP client to default to port 80, leading to connection timeout.","Symbolicator service connection timeouts occur due to network unreachability or unresponsiveness, causing profile processing task failures.","Symbolicator-JS service unreachable, causing connection timeouts and profile processing task failures.","Symbolicator service connection timed out after DNS resolution, indicating network or service availability issue at 192.168.208.101:80.","Sentry worker cannot establish TCP connection to `symbolicator-js.sentry.:80` due to network-level timeout, preventing profile symbolication.","JavaScript symbolicator service is unreachable, causing connection timeouts and profile processing failures.","Sentry failed to connect to the JavaScript symbolicator service due to a network timeout on port 80, caused by an incorrect service URL configuration.","TCP connection to `symbolicator-js.sentry.:80` timed out, indicating network or service unavailability.","TCP connection to `symbolicator-jvm.sentry.:80` timed out, likely due to service unavailability or network/firewall blocking.","Symbolicator service unreachable; network or service health prevents connection to `symbolicator.sentry.:80`.","Symbolicator connection timed out; `symbolicator-js.sentry.` resolved to an unreachable host/port 80, overriding expected localhost:3021.","Connection to symbolicator-jvm.sentry. timed out due to external network configuration preventing reachability.","TCP connection to symbolicator-js.sentry. timed out after 6 seconds, exhausting all retries, indicating network or service availability issue.","Sentry cannot connect to symbolicator service due to network timeout, likely misconfigured URL/port or service unavailability.","JavaScript symbolicator service at 'symbolicator-js.sentry.' is unreachable, causing connection timeouts after multiple retries.","Symbolicator service connection timed out after 6 seconds, exhausting retries, indicating service unavailability or network blockage.","Symbolicator services were unreachable or overloaded, causing connection timeouts and exhausting retries during profile symbolication.","Symbolicator JVM service unreachable; TCP connection attempts timed out after 6s, exhausting all retries.","Symbolicator pool URLs unconfigured, causing fallback to incorrect default hostname and port, leading to connection timeouts.","Symbolicator service URL misconfigured to port 80, causing connection timeouts to expected port 3021.","Symbolicator-js service is unreachable, causing connection timeouts and preventing JavaScript profile symbolication.","JavaScript symbolicator URL lacks port, causing connection attempts to default port 80, while service listens on 3021, leading to timeouts.","JVM symbolicator service unreachable; network connection to `symbolicator-jvm.sentry.` timed out after multiple retries.","Symbolicator-js service unreachable/unresponsive, causing connection timeouts after retries, preventing profile symbolication.","Symbolicator service unreachable, causing connection timeouts and profile processing failures.","Symbolicator service unreachable; TCP connection to `symbolicator.sentry.:80` timed out after multiple retries.","Symbolicator service unreachable; network connection to `symbolicator-js.sentry.` on port 80 timed out after multiple retries.","Symbolicator-JVM service unreachable; TCP connection attempts timed out, exhausting all retries.","Symbolicator service unreachable; TCP connection to its resolved IP on port 80 timed out."],"transactions":["sentry.tasks.symbolicate_jvm_event","sentry.tasks.symbolicate_js_event","sentry.tasks.store.symbolicate_event","sentry.profiles.task.process_profile"],"title":"Symbolicator service timeouts during stacktrace processing","description":"Workers fail to symbolicate JS, JVM, and generic stacktraces due to connect timeouts to the Symbolicator endpoints (symbolicate-js, symbolicate-jvm, symbolicate), exhausting retries and causing task failures.","tags":["Networking","External System","API","Timeout","Retries Exhausted","urllib3","Symbolicator"],"cluster_size":43,"cluster_min_similarity":0.9188531343684193,"cluster_avg_similarity":0.9597152150168374},{"project_ids":["1"],"cluster_id":864,"group_ids":[6789207876,6794633765],"issue_titles":["HTTPError: 500 Server Error: Internal Server Error for url: http://seer-web-autofix/v1/automation/autofix/start"],"root_cause_summaries":["Seer's autofix service returns 500 due to unhandled exceptions during GitHub API interactions, caused by external system instability.","Seer's synchronous GitHub API calls block request handlers, causing service unresponsiveness and cascading timeouts."],"transactions":["sentry.tasks.autofix.trigger_autofix_from_issue_summary"],"title":"Autofix requests to Seer return 500 errors","description":"Calls from issue_summary.trigger_autofix to the Seer autofix endpoint are failing with HTTP 500, causing autofix runs to not be created. The failures occur during response.raise_for_status() when invoking the external Seer service.","tags":["External System","API","Upstream Unavailable","HTTP 500","Seer","Autofix"],"cluster_size":2,"cluster_min_similarity":0.9517805961802928,"cluster_avg_similarity":0.9517805961802928},{"project_ids":["1"],"cluster_id":865,"group_ids":[6789207950,6789208066],"issue_titles":["MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /3b/45b1/3c972040dbb5edaa441d568f29 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7aadcc2ea210>, 'Connection to 192.168.208.181 timed o...","MaxRetryError: HTTPConnectionPool(host='192.168.208.181', port=8080): Max retries exceeded with url: /f5/8df2/810b084ddcb20684b0291f1e1d (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f14aa511b50>, 'Connection to 192.168.208.181 timed o..."],"root_cause_summaries":["File storage service at `192.168.208.181:8080` unreachable, causing TCP connection timeouts and retry exhaustion.","Filestore service at 192.168.208.181:8080 is unreachable, causing connection timeouts and MaxRetryError during file deletion."],"transactions":["getsentry.filestore in increment","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/files/difs/assemble/"],"title":"Filestore DELETE requests time out from workers","description":"Worker processes fail while deleting file blobs due to connect timeouts to the filestore HTTP endpoint, leading to urllib3 MaxRetryError after retries. Impact: orphaned file records and stalled cleanup tasks.","tags":["Networking","External System","API","Timeout","Retries Exhausted","HTTP","urllib3"],"cluster_size":2,"cluster_min_similarity":0.9722798014741397,"cluster_avg_similarity":0.9722798014741397},{"project_ids":["1"],"cluster_id":944,"group_ids":[6789664049,6804478145],"issue_titles":["RetryError: Timeout of 5.0s exceeded"],"root_cause_summaries":["Taskworker Bigtable connection failed due to network handshake error, exhausting 5s timeout.","Bigtable nodestore read timed out due to persistent TLS handshake failures, preventing secure connection to the service."],"transactions":["sentry.utils.kvstore.bigtable in get","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound"],"title":"Bigtable gRPC handshake failures causing nodestore timeouts","description":"Reads to the nodestore backend backed by Bigtable are failing with gRPC UNAVAILABLE during TLS handshake, leading to RetryError timeouts in post-processing and Sentry app resource change flows.","tags":["Networking","External System","API","Timeout","Upstream Unavailable","gRPC","Google Cloud Bigtable"],"cluster_size":2,"cluster_min_similarity":0.9709904399673127,"cluster_avg_similarity":0.9709904399673127},{"project_ids":["1"],"cluster_id":869,"group_ids":[6792517060,6792517354,6792517729,6792517991,6793959003,6793961015,6794695854],"issue_titles":["OperationalError: ","InterfaceError: connection already closed"],"root_cause_summaries":["Long-running consumer threads encounter idle database connection closures, causing `InterfaceError` during subsequent database operations.","Taskworker child process skips database initialization due to incorrect process type, leading to `connection already closed` error.","Taskworker child processes lack database connection lifecycle management, leading to stale connections and `InterfaceError` when used after server-side closure.","Multiprocessing 'spawn' method copies parent's stale database connection objects to child, causing 'connection already closed' on first DB access.","Synchronous ORM calls in taskworker child processes fail due to invalid database connections and Django's async context protection.","Multiprocessing worker's inherited database connection became stale, causing `OperationalError` during query execution, which reconnection logic failed to resolve.","Forked process uses inherited, closed database connection for cache versioning, causing `InterfaceError`."],"transactions":["sentry.tasks.assemble.assemble_dif","sentry.sentry_apps.tasks.sentry_apps.process_resource_change_bound","sentry.integrations.tasks.sync_status_inbound","ingest_consumer.process_event","monitors.monitor_consumer","getsentry.models.billinghistory in get_current"],"title":"Django DB connections close during multiprocessing tasks","description":"Multiple background workers (sentry app signals, DIF assembly, monitor consumer) raise InterfaceError: connection already closed when performing ORM queries, suggesting database connections are not safely managed across multiprocessing child processes. Impact includes failed app installation processing, artifact assembly, org sync, and monitor error logging.","tags":["Database","Concurrency","Configuration","Django ORM","Multiprocessing","Connection Closed","InterfaceError"],"cluster_size":7,"cluster_min_similarity":0.9165475113577126,"cluster_avg_similarity":0.9469379644028609},{"project_ids":["1"],"cluster_id":873,"group_ids":[6792867749,6792867782],"issue_titles":["IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist."],"root_cause_summaries":["Missing `IncidentGroupOpenPeriod` record due to incomplete two-phase creation, causing lookup failure.","IncidentGroupOpenPeriod records are not created from placeholders because the conversion method is never invoked, causing subsequent lookups to fail."],"transactions":["sentry.workflow_engine.tasks.trigger_action"],"title":"Missing IncidentGroupOpenPeriod during alerts","description":"Alert notifications for metric incidents (Slack/Discord handlers) attempt to load IncidentGroupOpenPeriod but the record is absent, causing DoesNotExist exceptions and failing notification builds.","tags":["Data Integrity","API","Configuration","Django ORM","DoesNotExist","Slack","Discord"],"cluster_size":2,"cluster_min_similarity":0.967816323161817,"cluster_avg_similarity":0.967816323161817},{"project_ids":["1"],"cluster_id":876,"group_ids":[6793004152,6793004633,6793005874,6793005956,6793007516,6793007884,6793008483,6793009213,6793009269,6793009639,6793009656,6793009700,6793010309,6793010408,6793011245,6793011354,6793011682,6793012196,6793013839,6793014040,6793014123,6793014183,6793014452,6793014953,6793015262,6793015332,6793017005,6793017128,6793017182,6793017205,6793017363,6793017437,6793017959,6793018330,6793019080,6793019218,6793019289,6793019441,6793019778,6793021252,6793021491,6793022437,6793024096,6793024195,6793024246,6793026878,6793027660,6793029656,6793036509,6793039293,6793040764,6793044510,6793047275,6793048690,6793048784,6793049022,6793049946,6793049967,6793050688,6793051268,6793051538,6793057160,6793060331,6793063958,6793076428,6793078670,6793079980,6793082362],"issue_titles":["RedisClusterException: ERROR sending 'cluster slots' command to redis server: {'host': 'rc-default-cache.service.us-central1.consul.', 'port': 6300}","ConnectionError: Error 104 while writing to socket. Connection reset by peer.","ConnectionError: Connection closed by server.","ResponseError: Command # 1 (EXPIRE re2:info:6655251982 2592000) of pipeline caused error: max number of clients + cluster connections reached","ResponseError: max number of clients + cluster connections reached","ResponseError: Command # 2 (EXPIRE re2:count:6655251982 2592000) of pipeline caused error: max number of clients + cluster connections reached"],"root_cause_summaries":["Concurrent worker processes each establish separate Redis connection pools, collectively exceeding the Redis cluster's `maxclients` limit.","Redis cluster connection limit reached during client initialization due to high concurrent task load and aggressive connection pooling settings.","Redis connection pool exhaustion due to high concurrent profile processing tasks exceeding `RetryingRedisCluster`'s `max_connections` limit.","Redis cluster's `maxclients` limit is exceeded by concurrent Sentry workers initializing clients, causing connection rejections.","Redis server's `maxclients` limit is exceeded by cumulative connections from multiple taskworker processes during cluster discovery.","High event volume causes concurrent Redis client initializations, exhausting Redis's max client limit.","Single Redis instance for feature adoption caching exhausted connections during high-frequency deploy signals.","Concurrent `assemble_dif` tasks exhaust Redis connection pool (16/node), causing churn and cluster-wide client limit breach.","Redis client's hardcoded 16 max connections per node exhausted by concurrent artifact bundle indexing tasks.","Concurrent profile processing tasks overwhelm Redis, exhausting client connections during lazy cluster initialization.","Taskworker's concurrent spawned processes each create Redis clients with connection pools, exhausting Redis's client limit.","Redis connection pool exhausted due to low `max_connections` (16) and high concurrent, long-running profile processing tasks.","Taskworker processes exhaust Redis connections due to independent connection pool creation, exceeding server limits.","Redis client's hardcoded 16-connection limit per node caused connection pool exhaustion during high-demand release assemble API calls.","Taskworker concurrency (32) exceeds Redis client's max connections (16), causing connection pool exhaustion and server-side rejections exacerbated by connection instability.","Redis connection pool exhausted due to hardcoded 16 connections/node limit, insufficient for concurrent profile processing.","Redis cluster exhausted connections due to high-frequency feature adoption tracking, exceeding server's `maxclients` limit.","Feature adoption's per-event Redis cache lookup exhausted connection pool under high concurrency.","Redis connection exhaustion due to frequent client initialization and connection attempts under high concurrent event processing load.","Multiprocessing workers each create new Redis connection pools, exhausting the cluster's client limit.","Redis cluster connection exhaustion due to high taskworker concurrency exceeding server's `maxclients` limit.","Redis `maxclients` exceeded due to `SimpleLazyObject` not being thread-safe, causing excessive `RetryingRedisCluster` connection pool creation.","FeatureAdoption's per-event Redis reads exhaust connection pool, causing ResponseError.","Redis connection pool (max 16 per node) exhausted by concurrent tasks, causing 'max clients' and 'broken pipe' errors.","High Celery concurrency combined with per-worker Redis client instantiation and aggressive connection limits exhausted Redis server's client capacity.","Taskworker recycling under high load creates excessive Redis client connections, exhausting the cluster's `maxclients` limit.","Redis connection limit reached due to concurrent `fetch_commits` tasks triggered by rapid release creation without inline commit data.","Taskworker processes' Redis clients open 16 connections per node, exceeding Redis's max client limit.","Redis connection pool exhaustion due to insufficient `max_connections` and high concurrent task load, causing client connection failures.","Redis cluster exhausted connections due to high user feedback volume, inefficient caching, and aggressive retries on an unstable instance.","High concurrent profile processing tasks hold Redis connections too long, exceeding Redis cluster's client limit.","Redis cluster connection limit reached due to high-volume feature adoption tracking, causing client initialization failures and cascading errors.","Redis cluster connection exhaustion due to high concurrent profile processing tasks repeatedly fetching last upload timestamp.","Redis server's `maxclients` limit is reached by concurrent taskworker processes, preventing new client connections during initialization.","Debounce cache's synchronous Redis checks, triggered by high event volume, exhaust Redis connections.","Redis cluster exhausted connection capacity, rejecting new client connections during feature adoption caching triggered by deploy creation.","Frequent `SISMEMBER` Redis calls for already-adopted features exhaust Redis connections, due to redundant cache checks.","Redis debounce cache's 16-connection limit exhausted by concurrent event processing, causing connection failures.","High event volume and frequent Redis pipeline operations exhaust the Redis cluster's client connection limit.","Feature adoption tracking's per-event Redis `SMEMBERS` calls exhaust Redis connections under high event volume.","Redis client configured for cluster, but connects to single instance, limiting connections to 16, causing exhaustion.","Redis `maxclients` limit reached; each of 16 taskworker processes opened 96 connections, exhausting server capacity.","Redis cluster connection pool exhaustion due to high concurrency and inefficient connection management, leading to client rejection.","Concurrent taskworker processes each create Redis clients, exhausting the Redis server's connection limit.","Multi-process consumers' frequent, uncoordinated Redis health checks exhaust limited per-process connection pools, saturating the Redis cluster.","High-volume issue assignments trigger frequent Redis calls for feature adoption tracking, exhausting Redis client connections.","Redis connection pool for feature adoption cache exhausted due to concurrent project creation, exceeding `max_connections`.","Redis connection pool (max 16) exhausted by high-volume `post_process_group` tasks performing `SMEMBERS` operations.","Redis client's 16-connection limit is exhausted by concurrent `assemble_artifacts` tasks, preventing new connections.","Redis `maxclients` limit reached due to excessive connection attempts from concurrent tasks, exacerbated by inefficient connection pool reuse.","Concurrent task processing repeatedly initializes Redis clients, exhausting cluster connection limits.","Redis cluster connection exhaustion occurs during frequent lazy client initialization, overwhelming server's client limit during topology discovery.","Concurrent `post_process_group` tasks performing expensive `SMEMBERS` operations exhaust Redis client connections.","Multiple consumer processes each create Redis clients, cumulatively exceeding Redis's `maxclients` limit, causing connection rejections.","Redis client's 16-connection limit exhausted by concurrent feature adoption tasks, causing connection failures during event post-processing.","Concurrent `RetryingRedisCluster` initializations from release operations exhaust Redis's `maxclients` limit, causing connection failures.","Concurrent Redis `sismember` calls on a shared client exhaust the cluster's connection limit, causing `ResponseError` unhandled by `RetryingRedisCluster`.","Redis connection pool (16) exhausted by high concurrent profile processing tasks (32+), causing connection rejections and task failures.","Redis cluster connection exhaustion due to high-concurrency event processing exceeding `maxclients` limit during topology discovery.","Aggressive Redis client connection pooling in a multi-process environment exhausts the Redis cluster's `maxclients` limit.","Concurrent task worker processes' lazy Redis client initialization creates connection storms, exhausting Redis's `maxclients` limit during cluster topology discovery.","Redis `maxclients` exceeded due to per-worker connection pool limits multiplying across many concurrent Celery workers.","High event volume causes concurrent lazy Redis client initializations, exhausting cluster connections during discovery.","Redis cluster connection pool exhaustion due to concurrent `cluster slots` commands during high-volume event processing.","Hardcoded 16-connection Redis client pool exhausted by concurrent profile processing tasks, causing connection rejections and timeouts.","High profile ingestion rate exhausts shared Redis cluster's connection limit, causing `ResponseError` and cascading failures.","Redis server's `maxclients` limit was reached due to high concurrent load, causing connection rejections during profile deobfuscation.","Redis client connection pool exhaustion due to high concurrent profile processing tasks exceeding the configured client-side limit."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.processing.backpressure.health in is_consumer_healthy","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/user-feedback|user-reports/","ingest_consumer.process_event","sentry.tasks.assemble in set_assemble_status","sentry.tasks.assemble.assemble_dif","sentry.issues.tasks.post_process.post_process_group","/api/0/organizations/{organization_id_or_slug}/processing-errors/","sentry.tasks.commits.fetch_commits","sentry.utils.redis in cluster_factory","issues.occurrence_consumer","sentry.tasks.assemble.assemble_artifacts","/issues/:groupId/","/api/0/organizations/{organization_id_or_slug}/releases/{version}/assemble/","/api/0/organizations/{organization_id_or_slug}/releases/","/api/0/organizations/{organization_id_or_slug}/releases/{version}/deploys/","/api/0/teams/{organization_id_or_slug}/{team_id_or_slug}/projects/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/","sentry.profiles.task.process_profile"],"title":"Redis Cluster client limit reached in post-processing","description":"Post-processing and profiling tasks fail when accessing Redis Cluster (smembers/get) due to exceeding the max clients + cluster connections, causing ResponseError and RedisClusterException during feature adoption cache and symbolicator source lookups.","tags":["Caching","Resource Limits","External System","Redis","Client Limit Exceeded","Redis Cluster","Post-Processing"],"cluster_size":68,"cluster_min_similarity":0.9209374460033045,"cluster_avg_similarity":0.9545529078702985},{"project_ids":["1"],"cluster_id":877,"group_ids":[6793004272,6793005961,6793007180,6793007448,6793007676,6793007956,6793008537,6793008762,6793008983,6793009142,6793009701,6793009815,6793009840,6793010228,6793010384,6793010801,6793011058,6793011119,6793011366,6793011534,6793011951,6793012157,6793012206,6793012680,6793012722,6793013345,6793013830,6793014230,6793014956,6793015007,6793015034,6793015638,6793016293,6793016757,6793016878,6793017302,6793017769,6793018360,6793018934,6793019117,6793019337,6793019514,6793021575,6793022044,6793022743,6793023536,6793029194,6793029846,6793040705,6793047749,6793049384,6793051768,6793054959,6793059683,6793063104,6793066973,6794022490,6794111965],"issue_titles":["ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')","ConnectionError: Error 32 while writing to socket. Broken pipe."],"root_cause_summaries":["Aggressive 3-second Redis socket timeout, combined with multiprocessing connection management, causes BrokenPipeError during high-concurrency Redis access.","Redis server's idle timeout closed connection; client attempted write on stale socket, causing BrokenPipeError.","Taskworker's stale Redis connection, closed by server's idle timeout, caused BrokenPipeError during symbol cache lookup.","Redis connection broken pipe during lock check, indicating underlying network/server instability.","Multiprocessing worker termination invalidates Redis connections, causing `BrokenPipeError` on subsequent Redis operations.","Taskworker attempted to write to Redis via a connection prematurely closed by the server, causing a BrokenPipeError.","Redis connection inherited by child process breaks when parent closes or server times out, causing write failure.","Redis connection idles during long symbolication, exceeding external timeout, causing BrokenPipeError on subsequent write.","Redis connection pool reuses stale connections, causing `BrokenPipeError` during write to closed socket.","Taskworker child processes use stale Redis connections, causing ConnectionResetError during pipeline execution due to unexpected server closure.","Child process inherits stale Redis connection from parent, causing `BrokenPipeError` on first use.","Redis cluster or network infrastructure forcibly closes connections during write operations, causing persistent BrokenPipeErrors despite retries.","Redis connection unexpectedly closed by server or network during write operation, causing `BrokenPipeError` in taskworker.","Redis connections inherited by forked child processes become stale, leading to BrokenPipeError on write.","Redis client reuses stale connection; server/network prematurely closed it, causing BrokenPipeError on write.","Taskworker uses stale Redis connection from pool; server-side idle timeout closes socket, causing `BrokenPipeError` on write.","Taskworker child process inherits invalid Redis socket file descriptor from parent, causing BrokenPipeError.","Redis server prematurely closed socket, causing `BrokenPipeError` during write after health check passed.","Redis connection idles, server closes it. Subsequent client write attempts on the stale socket cause BrokenPipeError.","Redis connection broke during write operation, likely due to network instability or server-side closure, causing BrokenPipeError.","Redis connection idled during symbolication, then infrastructure closed it, causing BrokenPipeError on subsequent write.","Taskworker's Redis connection pool reuses idle connections, which are silently terminated by server/network, causing `BrokenPipeError` on subsequent writes.","Forked child processes inherit parent's open Redis sockets, causing write conflicts and broken pipe errors.","Redis connection instability causes `BrokenPipeError` during GET, exhausting `RetryingRedisCluster` retries, indicating infrastructure issue.","Redis connections inherited by forked child processes become stale, causing BrokenPipeError during health checks.","Taskworker's Redis client attempted to use a stale connection, leading to a `BrokenPipeError` during socket write.","Multiprocessing 'spawn' context prevents socket inheritance; child process attempts Redis write on invalid, inherited socket, causing BrokenPipeError.","Redis cluster instability causes dropped connections during socket writes, preventing lock acquisition despite retries.","Forked taskworker child processes inherit stale Redis connections, causing broken pipe errors during socket write operations.","Taskworker child process inherits invalid Redis socket, causing BrokenPipeError on write due to multiprocessing spawn.","Redis server prematurely closed connection during write, causing BrokenPipeError due to network instability or resource exhaustion.","Redis server closed idle connection during symbolication, causing BrokenPipeError on subsequent write attempt.","Redis cluster network instability or node failure caused repeated broken pipe errors during lock acquisition.","Redis server closes idle connections; taskworker attempts write on stale socket, causing BrokenPipeError.","Stale Redis connection, undetected by health checks, caused BrokenPipeError during write, due to premature server-side closure.","Redis socket inherited by spawned child process closed by another process, causing write failure.","Feature adoption cache Redis configuration missing for non-US regions, causing connection failures.","Redis connection became stale in a taskworker child process, leading to a BrokenPipeError during a GET operation.","Redis server closed idle connection; application lacked explicit retry, causing BrokenPipeError on subsequent use.","Redis connection silently closed by server/network idle timeout; client attempts write on stale socket, causing BrokenPipeError.","Redis server closed idle connection during long symbolication task, causing client's subsequent write to fail with BrokenPipeError.","Redis server's idle connection timeout closes connections, leading to BrokenPipeError when taskworkers reuse stale connections.","Redis cluster network instability causes `BrokenPipeError` during lock acquisition, exhausting retries and failing usage buffer flush.","Redis connection unexpectedly closed during write, causing `BrokenPipeError` due to network instability or server issues.","Redis server closes connections due to load, causing client's stale connection to break pipe during symbolication.","Long-running task's idle Redis connection closed by server/network, causing `BrokenPipeError` on subsequent write attempt.","Taskworker's Redis connection became stale; subsequent write attempts failed with BrokenPipeError.","Redis connection unexpectedly closed by server, causing `BrokenPipeError` during non-critical data fetch, aborting symbolication.","Child taskworker processes inherit stale Redis connections from parent, leading to `BrokenPipeError` when attempting to write to already-closed sockets.","Redis connection to 192.168.209.74:6310 experienced persistent network instability, causing BrokenPipeError during socket write operations.","Redis connections, lazily initialized, become invalid in multiprocessing spawned children due to inherited, closed sockets or race conditions.","Redis node `192.168.209.74:6300` prematurely closed the connection, causing a broken pipe during a write operation.","Taskworker child process attempts Redis write on inherited, invalidated socket connection, causing BrokenPipeError.","Redis client writes to server-closed socket, causing BrokenPipeError due to stale connection.","Forked taskworker processes inherit stale Redis connections from the parent, leading to connection resets when used.","JVM symbolication's unnecessary Redis `get_last_upload` call failed due to a broken Redis socket connection.","Redis client's internal socket state is corrupted by multiprocessing 'spawn' serialization, causing write failures.","Redis server unexpectedly closed connection during write, causing a broken pipe error in the client."],"transactions":["sentry.profiles.task.process_profile","sentry.tasks.store.symbolicate_event","sentry.tasks.symbolicate_js_event","sentry.tasks.symbolicate_jvm_event","/api/0/organizations/{organization_id_or_slug}/members/{member_id}/","sentry.tasks.process_commit_context","sentry.buffer.redis in _process_single_incr","/api/0/relays/projectconfigs/","/api/0/organizations/{organization_id_or_slug}/artifactbundle/assemble/","sentry.tasks.assemble.assemble_dif","sentry.tasks.commits.fetch_commits","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","/api/0/organizations/{organization_id_or_slug}/issues|groups/{issue_id}/summarize/","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/files/difs/assemble/","/api/0/organizations/{organization_id_or_slug}/releases/{version}/","sentry.issues.tasks.post_process.post_process_group","sentry.tasks.drain_outbox_shards"],"title":"Broken pipe when fetching project symbol sources","description":"Symbolication requests (JVM and native) fail while reading the last upload from the internal source cluster, causing socket write BrokenPipeError/ConnectionError during _get_cluster().get().","tags":["Networking","Caching","API","Connection Reset","Broken Pipe","Symbolicator","Internal Source Cluster"],"cluster_size":58,"cluster_min_similarity":0.8973321043888298,"cluster_avg_similarity":0.9452005756704746},{"project_ids":["1"],"cluster_id":878,"group_ids":[6793005282,6793005880,6793007967,6793008485,6793008540,6793008709,6793008818,6793009033,6793010204,6793010535,6793011236,6793011681,6793011728,6793011785,6793012485,6793012953,6793013194,6793013247,6793013820,6793014041,6793014062,6793014259,6793014384,6793014613,6793014640,6793015194,6793015217,6793015573,6793016089,6793016430,6793016534,6793016585,6793016735,6793016926,6793017270,6793017299,6793017686,6793017842,6793017954,6793018194,6793018212,6793018501,6793018688,6793018720,6793018876,6793019120,6793019280,6793019939,6793020066,6793020110,6793020326,6793020659,6793021092,6793021776,6793021987,6793025400,6793027055,6793027639,6793031253,6793037828,6793040856,6793041160,6793044812,6793045989,6793048000,6793050269,6793051520,6793051566,6793056035,6793056617,6793062290,6793066560,6793066710,6793067005,6793073397,6793076738],"issue_titles":["ConnectionError: Error 104 while writing to socket. Connection reset by peer.","ConnectionError: Error 32 while writing to socket. Broken pipe."],"root_cause_summaries":["Redis `maxclients` limit exceeded due to high connection demand from many Sentry worker processes, causing connection resets.","Redis `maxclients` limit reached due to high-volume distributed lock contention, causing connection failures and task timeouts.","Redis cluster overload from concurrent lock existence checks during rapid release creation causes connection resets.","Redis cluster's `maxclients` limit reached, causing connection rejections and subsequent broken pipe errors.","Redis connection pool exhaustion (max_connections=16) during concurrent lock acquisition causes connection resets, broken pipes, and task timeouts.","Redis cluster overloaded by concurrent lock acquisitions, causing connection resets and task failures.","Redis cluster connection exhaustion due to high concurrent demand from multiple Sentry components caused `BrokenPipeError`.","Setup wizard's polling pattern overloads Redis cluster, causing connection resets due to max clients reached.","Concurrent profiling tasks exhaust Redis client connection pools and the Redis server's `maxclients` limit, causing `BrokenPipeError` during Redis operations.","Redis cluster connection limit reached, forcing premature socket closures, causing BrokenPipeError during write operations.","Unbounded Redis set growth for feature adoption causes `SMEMBERS` overload, exhausting Redis cluster connections, leading to `ConnectionResetError`.","Frequent `SMEMBERS` calls on large Redis sets overload an unstable Redis cluster, causing connection failures.","Redis connection limit exhaustion caused socket write failures, leading to BrokenPipeError during SISMEMBER operations.","Redis connection pool exhausted due to high-volume per-profile lock acquisitions, causing socket write failures.","Redis server, under high load, resets connections during write operations due to resource exhaustion, causing task failures.","Redis connection pool exhaustion, exacerbated by high task concurrency and cluster instability, causes socket write failures.","Redis `maxclients` limit reached due to high concurrent `SMEMBERS` calls from every event processed.","Redis cluster connection exhaustion due to high-volume feature adoption lookups causes broken pipe errors.","Redis cluster connection limits are exhausted, causing server to drop connections and client writes to fail with broken pipe.","Task concurrency exceeds Redis connection limits, causing socket errors during lock acquisition.","Redis connection pool exhaustion and network instability cause lock acquisition failures during profile processing.","Feature adoption's Redis client defaults to an unsuitable cluster, causing connection errors and max client issues under load.","Redis cluster connection limits exhausted by concurrent profile processing tasks holding 10-second locks, causing broken pipes and task timeouts.","Multiprocess concurrency exceeding hardcoded Redis connection pool limits causes connection exhaustion and broken pipe errors.","Redis cluster connection limit exceeded, causing server to close connections, leading to client-side BrokenPipeError during socket write.","Redis cluster `rc-default-cache` connection limits reached, causing `ConnectionResetError` during `DEL` operation.","Redis connection exhaustion from high-concurrency task lock acquisitions causes broken pipes and task failures.","Redis cluster overload, due to high task concurrency exceeding connection limits, causes `BrokenPipeError` when writing to closed sockets.","Redis connection pool exhaustion due to high concurrent event processing tasks exceeding per-node connection limits.","Redis cluster overutilization from widespread Sentry feature usage causes connection limits and dropped connections.","Redis cluster instability causes socket write failures (connection reset/broken pipe), leading to repeated `ConnectionError` despite client retries.","Redis connection exhaustion from per-profile lookups during high-volume symbolication causes broken pipes and task timeouts.","Redis cluster resource exhaustion, likely max clients reached, causes dropped connections, leading to `BrokenPipeError` during Redis operations.","Redis cluster connection exhaustion due to `maxclients` limit causes broken pipes during feature adoption lookups.","Redis cluster connection exhaustion causes server to drop connections, leading to BrokenPipeError during lock acquisition by auto_source_code_config task.","Redis cluster drops connections due to overload during billing writes, causing broken pipe errors.","Concurrent long-running tasks exhaust Redis connection pool, causing server to drop connections, leading to BrokenPipeError during Redis operations.","Redis connection pool exhausted under high concurrent load, causing `BrokenPipeError` during `sismember` operation.","High concurrency and aggressive Redis client timeouts overwhelm Redis, causing connection exhaustion and broken pipes during profile processing.","Redis connection reset during distributed lock acquisition for artifact bundle indexing, likely due to Redis cluster instability or network issues under concurrent load.","Redis connection pool exhausted by high lock contention during concurrent ProjectSDK updates from profile processing.","Redis connection pool exhaustion on `rc-default-cache` due to high `post_process_group` task concurrency, causing `BrokenPipeError`.","Redis cluster connection limits are reached, causing `BrokenPipeError` during lock acquisition due to dropped connections.","Redis connection exhaustion caused by high task worker churn, leading to lock acquisition failures and task timeouts.","Redis connection exhaustion from high lock contention during `ProjectSDK` updates in `process_profile_task` causes `ConnectionResetError`.","Redis cluster connection limit reached, causing `BrokenPipeError` during lock acquisition, preventing task execution.","Feature adoption Redis operations on every event overload Redis, causing connection resets and exhaustion.","Redis cluster connection exhaustion caused by high load during feature adoption checks.","Unthrottled Redis `SMEMBERS` calls for feature adoption on every event overload the Redis cluster.","Redis `maxclients` limit reached, causing connection rejections and subsequent `ConnectionResetError` during lock acquisition.","Inefficient `SMEMBERS` calls for feature adoption overwhelm Redis, causing connection resets due to high load.","Redis connection instability during SSO authentication, likely due to connection pool exhaustion or server resource constraints, causes `BrokenPipeError`.","Redis cluster overload and connection exhaustion cause server-side connection resets and broken pipes during lock acquisition.","Redis `maxclients` limit exceeded, causing connection rejections and `BrokenPipeError` during lock acquisition for profile processing.","Redis connection pool exhaustion caused server to drop connection during write, leading to BrokenPipeError.","High concurrent profile processing tasks exhaust Redis connection pool, causing server overload and connection resets.","Redis cluster overload and connection management issues cause `BrokenPipeError` during lock acquisition.","Redis cluster connection limit reached, causing `ConnectionResetError` during `sismember` operation.","Redis connection pool exhaustion due to high task concurrency, limited per-node connections, and task timeouts preventing connection release.","Task concurrency overwhelms Redis connection limits, causing pool exhaustion, connection resets, and task timeouts.","Redis `maxclients` limit reached, causing `BrokenPipeError` during feature adoption Redis writes.","Sentry's concurrent processes with high per-process Redis connection limits exhaust Redis's `maxclients`, causing connection resets.","Redis cluster connection exhaustion due to high profile processing load causes broken pipes and connection errors.","Redis cluster overload causes connection exhaustion, leading to socket write failures and broken pipe errors.","Overloaded Redis cluster drops connections, causing broken pipe errors during lock acquisition for profile processing.","Redis cluster overload and connection exhaustion caused `BrokenPipeError` during lock acquisition.","Redis connection exhaustion during high-volume feature adoption lookups causes broken pipes.","Redis connection exhaustion, exacerbated by task timeouts causing ungraceful worker termination and connection leakage, leads to socket errors.","Redis cluster connection limits are exhausted by high concurrent profile processing tasks, causing connection errors and task timeouts.","Redis cluster connection limits are exhausted by frequent lock acquisitions during high-volume profile processing, causing connection resets.","Overloaded Redis cluster resets connections, preventing error logging and creating a feedback loop during high monitor traffic.","Redis connection pool exhaustion, due to low `max_connections` and high concurrent demand, causes socket errors and task timeouts.","Redis server connection limit reached, actively rejecting new connections, causing client's `sendall` to fail.","Redis connection exhaustion under high profile processing load causes lock acquisition failures and task timeouts.","Redis `maxclients` limit reached during concurrent profile symbolication, causing socket write failures and task timeouts.","Redis connection exhaustion due to concurrent `RetryingRedisCluster` instantiation, exceeding `maxclients` and causing `BrokenPipeError`."],"transactions":["/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/","/api/0/organizations/{organization_id_or_slug}/releases/","sentry.tasks.assemble.assemble_dif","sentry.profiles.task.process_profile","/api/0/organizations/{organization_id_or_slug}/releases/{version}/","sentry.models.counter.refill_cached_short_ids","sentry.tasks.commits.fetch_commits","sentry.tasks.store.save_event","sentry.tasks.process_commit_context","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.issues.tasks.post_process.post_process_group","/api/0/wizard/{wizard_hash}/","sentry.tasks.process_suspect_commits","/auth/sso/","sentry.tasks.assemble.assemble_artifacts","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/","sentry.tasks.auto_source_code_config","getsentry.billing.tasks.usagebuffer.flush_usage_buffer","/api/0/organizations/{organization_id_or_slug}/monitors/{monitor_id_or_slug}/","sentry.debug_files.tasks.backfill_artifact_bundle_db_indexing","sentry.tasks.digests.deliver_digest","monitors.monitor_consumer"],"title":"Redis socket writes fail with broken pipe/reset","description":"Multiple workers hit socket write failures to Redis during lock acquisition and set/smembers operations, causing Connection Reset and Broken Pipe errors. Likely transient Redis or network instability affecting inter-process locking and cache reads.","tags":["Networking","Caching","Queueing / Messaging","Redis","Connection Reset","Broken Pipe"],"cluster_size":77,"cluster_min_similarity":0.8993549206683606,"cluster_avg_similarity":0.9476685219480014},{"project_ids":["1"],"cluster_id":879,"group_ids":[6793007206,6793007973,6793008102,6793008139,6793008411,6793009110,6793009163,6793009271,6793009548,6793009609,6793009977,6793010712,6793010917,6793010923,6793011404,6793011568,6793011688,6793011699,6793011774,6793012587,6793012906,6793012979,6793013228,6793013364,6793013415,6793015736,6793015882,6793016141,6793016187,6793016414,6793016431,6793017948,6793019015,6793019448,6793019917,6793020904,6793021391,6793028849,6793029712,6793030259,6793031516,6793048556,6793053428,6793060663,6793076345,6798462777,6799659034,6805657627],"issue_titles":["ConnectionError: Error 104 while writing to socket. Connection reset by peer.","ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')","ConnectionError: Error 32 while writing to socket. Broken pipe."],"root_cause_summaries":["Redis infrastructure instability causes connection resets during symbolication task's write operations to the socket.","Redis connection reset due to high-frequency `GET` operations from symbolication tasks overwhelming the cluster.","Redis cluster connection limit reached, causing connection resets and broken pipes during write operations.","Redis server closed idle connection; client attempted write on stale socket, causing ConnectionResetError.","Redis connection instability during JVM symbolication causes connection resets, preventing timestamp retrieval.","Redis connection instability causes socket write failures during symbolication tasks.","Redis connection unexpectedly closed during write, likely due to server instability or network issues, preventing symbolication data retrieval.","Redis connection reset by peer during symbolication task due to network instability or Redis server overload, exacerbated by long-lived idle connections.","Redis connection to `192.168.209.92:6310` reset during GET operation, indicating network instability or Redis cluster issues.","Persistent network instability to Redis node 192.168.209.92:6310 causes connection resets, exhausting retries during lock acquisition.","Redis server instability or overload causes connection resets during socket writes, overwhelming client retries and preventing debounce cache access.","Non-critical Redis cache-busting failure during symbolication lacks graceful degradation, causing task failure.","Redis connection reset by peer during GET operation, indicating network instability or server-side connection termination.","Redis server at 192.168.209.92:6300 forcefully reset the connection during a write operation, indicating network instability or server-side resource exhaustion.","Redis server at 192.168.209.92:6310 abruptly closed the TCP connection during a socket write, causing a ConnectionResetError.","Redis cluster node `192.168.209.74:6310` intermittently resets connections, preventing symbolication data retrieval.","Redis cluster or network actively resets connection during GET operation, causing symbolication task failure.","Redis cluster overload causes connection resets and broken pipes during profile processing, leading to task timeouts.","Redis server or network infrastructure actively resets connections under high concurrent load, causing `ConnectionResetError` during `GET` operations.","Redis connection instability during non-critical cache-busting operation causes symbolication task to fail entirely.","Optional Redis optimization lacks error handling, causing symbolication failure when Redis connection resets.","Redis cluster node at 192.168.209.92:6300 unexpectedly reset the TCP connection during a write operation, causing symbolication to fail.","Redis connection reset by peer during socket write, indicating server overload or network instability, causing symbolication task failure.","Network infrastructure terminates Redis connection due to idle timeout or connection limit, causing ConnectionResetError during socket write.","Long-running symbolication tasks cause Redis connections to idle, leading to server-side closure and subsequent client-side `ConnectionResetError` on reuse.","Redis server overload or network instability causes connection resets during symbolication's `GET` operation, failing the task.","Taskworker's Redis client attempts to reuse idle connection, but Redis server closed it due to timeout, causing write failure.","Redis server overload or network instability caused connection resets during event data retrieval for attachment processing.","Redis connection reset by peer during event data retrieval, caused by transient network instability or server-side connection termination.","Redis client's aggressive 3-second socket timeout causes connection resets when writing to stale connections from the pool.","Redis server's idle timeout closes connections, causing Sentry's client to fail on subsequent write attempts to the stale socket.","Redis connection reset by peer during symbolication, indicating Redis server overload or network instability under load.","Redis connection instability causes symbolication task failure when fetching non-critical cache-busting timestamp.","Redis connection reset during symbolication due to external infrastructure instability, blocking event processing.","Redis server instability at 192.168.209.75:6300 causes connection resets during write operations, disrupting symbolication tasks.","Redis connection reset by peer during lock check, indicating severe network instability or Redis server issues.","Redis cluster node at 192.168.209.92:6310 reset connection during socket write, indicating network or server instability.","Redis server, under symbolication load, actively resets connections, causing `ConnectionResetError` during `GET` operations.","Redis cluster instability or overload causes connection resets during operations, leading to `ConnectionError` despite client-side retries.","Redis server at 192.168.209.74:6310 reset connection during GET, indicating infrastructure instability or overload.","Redis server or network infrastructure forcibly closed TCP connection during GET operation, causing ConnectionResetError.","Redis server, under load, actively reset client connections during a GET operation, causing ConnectionResetError.","Redis connection reset during non-critical `get_last_upload` call, causing symbolication task failure due to unhandled exception.","Redis client reused stale connection; server reset it due to timeout or overload.","Redis connection reset by peer during `SISMEMBER` command, indicating network or Redis instance instability.","Redis client's single retry insufficient for persistent connection resets from an unstable Redis cluster.","Symbolication fails due to Redis connection reset during cache-busting timestamp retrieval, a critical path dependency.","Redis server or network instability caused connection reset during `GET` operation, failing symbolication."],"transactions":["/api/0/organizations/{organization_id_or_slug}/members/","sentry.tasks.commits.fetch_commits","sentry.tasks.store.process_event","sentry.tasks.store.symbolicate_event","sentry.tasks.assemble.assemble_artifacts","sentry.tasks.symbolicate_jvm_event","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/issues|groups/","sentry.tasks.store.save_event_attachments","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/releases/","sentry.issues.tasks.post_process.post_process_group","sentry.dynamic_sampling.boost_low_volume_transactions_of_project","sentry.profiles.task.process_profile"],"title":"Symbolicator cache lookups hit connection resets","description":"Symbolication paths calling get_last_upload via the internal cache/cluster are failing with Connection Reset by Peer, breaking JVM, native, and profiling symbolication. Likely an unstable cache backend or network issue between symbolicator and the cache cluster.","tags":["Networking","Caching","API","Connection Reset","Socket Write Error","Symbolicator","get_last_upload"],"cluster_size":48,"cluster_min_similarity":0.9098582393089185,"cluster_avg_similarity":0.9522981091551393},{"project_ids":["1"],"cluster_id":880,"group_ids":[6793012227,6793013527],"issue_titles":["ConnectionError: Error 104 while writing to socket. Connection reset by peer."],"root_cause_summaries":["Redis client misconfigured as cluster type for non-cluster Redis, causing connection resets due to protocol mismatch.","Redis client used cluster protocol on standalone server due to implicit configuration, causing connection resets."],"transactions":["sentry.issues.tasks.post_process.post_process_group","sentry.tasks.symbolicate_jvm_event"],"title":"Redis connections reset during post-processing and symbolication","description":"Workers performing feature adoption cache reads and JVM symbolication lookups are hitting connection resets to the Redis cluster, causing socket write/read failures. This disrupts event post-processing and symbolication workflows relying on Redis-backed caches.","tags":["Networking","Caching","External System","Connection Reset","Redis","Post-Processing","Symbolication"],"cluster_size":2,"cluster_min_similarity":0.967215487202147,"cluster_avg_similarity":0.967215487202147},{"project_ids":["1"],"cluster_id":881,"group_ids":[6793033653,6793037072],"issue_titles":["ConnectionError: Error 104 while writing to socket. Connection reset by peer."],"root_cause_summaries":["Dynamic sampling's periodic Redis operations overload the shared cluster, causing connection resets during project config debounce checks.","Redis server connection resets due to overload from high-volume dynamic sampling debounce key operations."],"transactions":["sentry.dynamic_sampling.boost_low_volume_projects_of_org","sentry.dynamic_sampling.boost_low_volume_transactions_of_project"],"title":"Redis connection resets during project config invalidation","description":"Calls to the debounce cache (client.get) in the project config invalidation path are failing with connection reset by peer, likely due to Redis or network instability. This impacts boost_low_volume* tasks that schedule invalidate_project_config via on_commit.","tags":["Networking","Caching","API","Redis","Connection Reset","Project Config Invalidation"],"cluster_size":2,"cluster_min_similarity":0.9654040768061992,"cluster_avg_similarity":0.9654040768061992},{"project_ids":["1"],"cluster_id":887,"group_ids":[6793316979,6794069688],"issue_titles":["ApiError: {\"error\":{\"code\":\"ServiceError\",\"message\":\"Service Error\"}}"],"root_cause_summaries":["Microsoft Teams API returned 502 Bad Gateway to Sentry's integration-proxy, indicating external service unavailability.","Redis instability causes Sentry's integration proxy to return 502s, preventing Microsoft Teams API calls."],"transactions":["sentry.shared_integrations.client.base in _request","sentry.issues.tasks.post_process.post_process_group"],"title":"Microsoft Teams API returns 502 during notifications","description":"Attempts to fetch conversation IDs and send cards to Microsoft Teams are failing with 502 Bad Gateway errors from the Teams API, causing digest and rule-based notifications to error out. The issue appears to be an upstream service error while posting to conversation and activity endpoints.","tags":["External System","API","Upstream Unavailable","Microsoft Teams","Bad Gateway"],"cluster_size":2,"cluster_min_similarity":0.9620916569873474,"cluster_avg_similarity":0.9620916569873474},{"project_ids":["1"],"cluster_id":898,"group_ids":[6794060493,6794060507],"issue_titles":["TimeoutError: Timeout connecting to server"],"root_cause_summaries":["Forked worker processes inherit stale Redis connections from the parent, causing connection timeouts on invalid sockets.","Redis connection pools initialized in parent process are inherited by child processes, leading to invalid socket file descriptors and connection timeouts."],"transactions":["ingest_consumer.process_event"],"title":"Redis timeouts during event grouping increments","description":"Event ingestion fails while incrementing group counters in the process buffer because Redis commands time out during pipeline execute. This disrupts error event grouping and may drop or delay updates to times_seen.","tags":["Caching","Queueing","Networking","Redis","Timeout"],"cluster_size":2,"cluster_min_similarity":0.9726153399859934,"cluster_avg_similarity":0.9726153399859934},{"project_ids":["1"],"cluster_id":901,"group_ids":[6794077737,6794100164],"issue_titles":["ConnectionError: Error while reading from socket: (104, 'Connection reset by peer')"],"root_cause_summaries":["Redis `zrange` on `pending_key` retrieves all entries, causing large response that exceeds Redis output limits, leading to connection reset.","Redis `ZRANGE` on large buffer exceeds 3-second socket timeout, causing connection reset due to unbounded operation."],"transactions":["sentry.buffer.redis in process_pending"],"title":"Redis cluster connections reset during buffer processing","description":"Workers processing the buffer fail when Redis cluster connections are reset while reading from the socket, causing ConnectionResetError in process_pending.","tags":["Networking","External System","Queueing","Redis","Connection Reset"],"cluster_size":2,"cluster_min_similarity":0.9730617991450004,"cluster_avg_similarity":0.9730617991450004},{"project_ids":["1"],"cluster_id":911,"group_ids":[6794582041,6794594623],"issue_titles":["ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.regenerate_service_hooks_for_installation"],"root_cause_summaries":["Sentry App update fans out to thousands of concurrent RPC calls, causing RPC timeouts and database deadlocks, exceeding task deadline.","ServiceHook updates cause database contention due to missing `application_id` index and inefficient row-by-row saves within long transactions."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.regenerate_service_hooks_for_installation"],"title":"Sentry App hook regeneration times out on remote call","description":"The regenerate_service_hooks_for_installation task is exceeding its processing deadline due to read timeouts when posting to the remote silo service, causing repeated task failures. Network latency or slow upstream responses in the remote dispatch path are likely impacting Sentry App webhook/event updates.","tags":["Networking","API","Queueing / Messaging","Timeout","Upstream Unavailable","HTTP","Sentry Apps"],"cluster_size":2,"cluster_min_similarity":0.965747590621097,"cluster_avg_similarity":0.965747590621097},{"project_ids":["1"],"cluster_id":910,"group_ids":[6794589205,6794604501,6794690371],"issue_titles":["HTTPError: 500 Server Error: Internal Server Error for url: http://seer-web-autofix/v1/automation/autofix/start"],"root_cause_summaries":["Seer's autofix endpoint fails due to unhandled 502 Bad Gateway from Sentry's GenAI consent RPC, causing a 500 error.","Seer's autofix failed because `seer-rpc` returned 502, preventing GenAI consent verification, causing a `ConsentError` and a 500 response.","`seer-rpc` service failure (502 Bad Gateway) during GenAI consent check blocks autofix initiation."],"transactions":["sentry.tasks.autofix.trigger_autofix_from_issue_summary"],"title":"Autofix calls to Seer return 500 Internal Server Error","description":"The issue_summary autofix task fails when _call_autofix raises for HTTP 500 responses from the Seer service, preventing autofix runs from being initiated.","tags":["API","External System","Retries Exhausted","HTTP 500","Seer","Autofix"],"cluster_size":3,"cluster_min_similarity":0.9704752034388129,"cluster_avg_similarity":0.9750288420752063},{"project_ids":["1"],"cluster_id":912,"group_ids":[6794603539,6794621740],"issue_titles":["RegionMappingNotFound"],"root_cause_summaries":["Organization mapping missing due to RPC service instability preventing cross-silo synchronization during provisioning.","RPC service unhealthiness prevents OrganizationMapping creation/synchronization, causing RegionMappingNotFound when resolving organization region."],"transactions":["sentry.sentry_apps.tasks.sentry_apps.regenerate_service_hooks_for_installation"],"title":"Region mapping missing for Sentry App webhook update","description":"SentryApp hook regeneration fails when resolving the organizations region; OrganizationMapping lookup returns no record, causing RegionMappingNotFound and aborting webhook/event updates.","tags":["Configuration","API","Data Integrity","Sentry Apps","RegionMappingNotFound","OrganizationMapping.DoesNotExist"],"cluster_size":2,"cluster_min_similarity":0.9696516154233585,"cluster_avg_similarity":0.9696516154233585},{"project_ids":["1"],"cluster_id":914,"group_ids":[6794695819,6794695899,6794695932,6794695944,6794695947,6794696023,6794696024,6794696036,6794696047,6794696051,6794696053,6794696690],"issue_titles":["OperationalError: "],"root_cause_summaries":["Database query timeouts prevent organization object loading, causing subsequent KeyError when accessing project.organization.slug.","Database query timeouts for organization data cause `KeyError` during feature flag evaluation, leading to profile processing task failures.","Database resource exhaustion causes `OperationalError` during organization data retrieval, leading to `KeyError` when accessing incomplete `Project` objects.","Database connection instability prevents organization object loading, causing KeyError when accessing project.organization.slug and task timeouts.","PostgreSQL query timeouts during organization lookup cause `OperationalError` and `KeyError`, failing profile symbolication.","Database connection exhaustion causes `OperationalError: query_wait_timeout`, preventing organization object loading, leading to `KeyError` during feature flag evaluation.","Database connection pool exhaustion causes query timeouts, preventing organization data retrieval, leading to task deadline exceedance.","PostgreSQL query timeout prevents organization retrieval, causing KeyError during feature flag evaluation.","Database connection instability causes `OperationalError` during organization lookup; reconnection logic fails due to `can_reconnect` not handling the specific error, leading to task failure.","Multiprocessing task workers' concurrent organization lookups overwhelm the database, causing query timeouts and subsequent Django ORM cache KeyErrors.","PostgreSQL `query_wait_timeout` on `sentry_organization` table prevents organization data retrieval, causing `OperationalError` and subsequent `KeyError`.","Unstable database connections cause `OperationalError` during organization lookup, leading to `KeyError` when accessing `project.organization.slug`."],"transactions":["sentry.profiles.task.process_profile"],"title":"Project.organization missing during symbolication/deobfuscation","description":"Profiling tasks call Symbolicator and feature checks using project.organization.slug, but the project lacks an organization field, leading to KeyError and subsequent database lookups failing with OperationalError when fetching the organization. Impacts profiling symbolication and JVM deobfuscation paths.","tags":["Configuration","API","Data Integrity","Sentry","Symbolicator","KeyError","OperationalError"],"cluster_size":12,"cluster_min_similarity":0.9355216216275388,"cluster_avg_similarity":0.9571262626695239},{"project_ids":["1"],"cluster_id":916,"group_ids":[6794695887,6794695951,6794696248,6794696258,6794696296,6794697316,6803663656],"issue_titles":["OperationalError: ","OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: FATAL:  no more connections allowed (max_client_conn)"],"root_cause_summaries":["PostgreSQL connection pool exhausted due to high concurrent task database demands exceeding `max_client` limit.","Database connection pool exhaustion prevents new connections, causing `OperationalError` during `post_process_group` pipeline steps.","Database connection pool exhaustion prevents new connections, causing `OperationalError` during organization member lookup in event post-processing.","Database connection failures due to resource exhaustion from high concurrent task load, bypassing reconnection logic.","Database connection pool exhaustion prevents tasks from acquiring connections, causing `OperationalError` and task failures.","Database connection pool exhausted by high event ingestion and frequent connection churn from post-processing tasks.","ThreadPoolExecutor threads opening new database connections exhaust PostgreSQL's `max_client_conn` limit, causing `OperationalError`."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"Post-processing tasks hit OperationalError on PostgreSQL","description":"Multiple post_process pipeline steps are failing on simple SELECT queries across Sentry tables due to database OperationalError, suggesting a transient or systemic PostgreSQL issue affecting read queries during event processing.","tags":["Database","Queueing","PostgreSQL","OperationalError","Post-Processing Pipeline"],"cluster_size":7,"cluster_min_similarity":0.9364626522009226,"cluster_avg_similarity":0.95866692342426},{"project_ids":["1"],"cluster_id":917,"group_ids":[6794695891,6794695955,6794696390,6794697015],"issue_titles":["OperationalError: "],"root_cause_summaries":["Database auto-reconnection for `OperationalError` is disabled because `can_reconnect` ignores it, causing query failures.","Database auto-reconnect mechanism is disabled for `OperationalError`, causing pipeline failures during connection instability.","Sentry's database auto-reconnection logic fails for `OperationalError` due to commented-out handling code, causing task failures.","Database connection instability causes `OperationalError` during `post_process_group` tasks, leading to pipeline step failures."],"transactions":["sentry.issues.tasks.post_process.post_process_group"],"title":"Post-process jobs hit database OperationalError","description":"Multiple Sentry post_processing steps (rules evaluation, auto-assignment, plugin options, and escalation checks) fail with OperationalError during SQL reads, indicating a database access issue impacting event processing workflows.","tags":["Database","API","PostgreSQL","OperationalError","Post-Processing","Read Queries"],"cluster_size":4,"cluster_min_similarity":0.9343215931443551,"cluster_avg_similarity":0.9505031410334362},{"project_ids":["1"],"cluster_id":929,"group_ids":[6799641644,6804163773],"issue_titles":["AssertionError: unreachable: rpc0:d18283625a8fa8492a3e54e84576de650e6a7968bfea435769f2cb1c7a8fbcc3"],"root_cause_summaries":["RPC signature authentication returns raw string, not `AuthenticatedToken`, causing access log middleware to assert.","RpcSignatureAuthentication sets request.auth to a raw string, not AuthenticatedToken, causing AssertionError in access logging."],"transactions":["/api/0/organizations/{organization_id_or_slug}/chunk-upload/"],"title":"API access logging crashes on unknown auth type","description":"Access log middleware asserts on an unexpected authentication object (e.g., rpc0: hash), causing AssertionError in _get_token_name during API request handling.","tags":["API","Configuration","Input Validation","Middleware","AssertionError"],"cluster_size":2,"cluster_min_similarity":0.9839974573664217,"cluster_avg_similarity":0.9839974573664217},{"project_ids":["6178942"],"cluster_id":932,"group_ids":[6800794526,6800794662,6807405581,6809287689],"issue_titles":["UnknownObjectException: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/git/trees#get-a-tree\", \"status\": \"404\"}"],"root_cause_summaries":["GitHub API returns 404 for empty Git tree SHA, causing `UnknownObjectException` when fetching repository files.","GitHub API returns 404 for empty Git tree SHA, indicating repository object store corruption or inaccessibility.","GitHub API returns 404 for Git's empty tree SHA, preventing repository content access.","GitHub API fails to serve the empty tree object (4b825dc642cb6eb9a060e54bf8d69288fbee4904) for the specified repository."],"transactions":["seer.automation.codegen.pr_review_step.pr_review_task","seer.automation.autofix.steps.root_cause_step.root_cause_task"],"title":"GitHub tree lookup returns Not Found during repo file listing","description":"Calls to repo.get_git_tree for a commit/tree SHA are returning Not Found, causing tools that enumerate valid file paths (tree, semantic search, document expansion) to fail. Likely an invalid or missing commit/tree reference or insufficient access to the repository via the GitHub API.","tags":["External System","API","Configuration","GitHub","Not Found","Repository Tree","UnknownObjectException"],"cluster_size":4,"cluster_min_similarity":0.973093467566109,"cluster_avg_similarity":0.9782517510678405},{"project_ids":["1"],"cluster_id":938,"group_ids":[6803829007,6805878869,6806016370,6814801890,6815503136],"issue_titles":["UptimeMonitorNoSeatAvailable"],"root_cause_summaries":["Organization exhausted pay-as-you-go uptime monitor quota, preventing automatic graduation of onboarding monitor to active status.","Organization lacks pay-as-you-go uptime monitor seats; quota backend rejected assignment, preventing monitor activation.","Proprietary quota backend denied uptime monitor seat assignment due to insufficient plan seats, causing `UptimeMonitorNoSeatAvailable`.","Uptime monitor creation failed because organization's pay-as-you-go budget for uptime monitors was exhausted.","Organization lacks pay-as-you-go budget for uptime monitors; automatic monitor graduation fails due to quota exhaustion."],"transactions":["monitors.uptime.result_consumer.ResultProcessor"],"title":"Uptime monitor onboarding fails due to no seats","description":"Enabling the uptime detector during project onboarding raises UptimeMonitorNoSeatAvailable in the result consumer, indicating seat capacity is exhausted and preventing subscription activation.","tags":["Configuration","Authorization","Uptime Monitoring","Seat Capacity Exhausted"],"cluster_size":5,"cluster_min_similarity":0.9562661923103439,"cluster_avg_similarity":0.9692008706057781},{"project_ids":["1"],"cluster_id":942,"group_ids":[6804142103,6804142111,6804142119],"issue_titles":["OperationalError: canceling statement due to lock timeout"],"root_cause_summaries":["High-frequency bulk updates on primary cause replica contention, leading to read query lock timeouts during webhook processing.","Replica's recovery applying high WAL volume conflicts with concurrent `drain_mailbox` reads, causing lock timeouts.","Concurrent primary updates and replica reads on `hybridcloud_webhookpayload` cause hot standby conflicts, leading to read query cancellations."],"transactions":["sentry.hybridcloud.tasks.deliver_webhooks in drain_mailbox"],"title":"Lock timeouts reading webhook payloads table","description":"Queries against hybridcloud_webhookpayload in deliver_webhooks are canceled due to lock timeouts, indicating contention while selecting payloads for processing. This blocks mailbox draining and delays webhook delivery.","tags":["Database","Concurrency","PostgreSQL","Lock Timeout","hybridcloud_webhookpayload","deliver_webhooks"],"cluster_size":3,"cluster_min_similarity":0.962483770245932,"cluster_avg_similarity":0.9656212794424627},{"project_ids":["1"],"cluster_id":945,"group_ids":[6804953344,6805741419,6806031322,6806035044,6806119413,6806132628,6806311325],"issue_titles":["HTTPError: 500 Server Error: Internal Server Error for url: http://seer-web-autofix/v1/automation/summarize/feedback/title","HTTPError: 500 Server Error: Internal Server Error for url: http://seer-web-autofix/v1/automation/summarize/feedback/labels"],"root_cause_summaries":["Seer-web-autofix's generic exception handler converts a `ConsentError` into a 500 Internal Server Error, obscuring the actual missing GenAI consent.","Sentry's `has_seer_access` incorrectly permitted GenAI request, causing Seer to reject it with a 500 error.","Sentry's `has_seer_access` incorrectly allowed GenAI request to Seer, which correctly rejected it with a 500.","Organization lacks GenAI consent; Seer's broad exception handling converts `ConsentError` into a generic HTTP 500.","Sentry's feature flag allowed GenAI request, but Seer's authoritative consent check correctly denied it, causing a 500.","Feedback title endpoint lacks explicit GenAI consent check, causing generic 500 error when consent is missing.","Seer's generic exception handling converts a `ConsentError` into a 500 Internal Server Error, masking the actual lack of GenAI consent."],"transactions":["sentry.tasks.update_user_reports","/api/0/projects/{organization_id_or_slug}/{project_id_or_slug}/user-feedback|user-reports/","/api/embed/error-page/","sentry.tasks.store.save_event_feedback","sentry.issues.tasks.post_process.post_process_group"],"title":"Seer AI endpoints return 5xx during feedback processing","description":"Calls to Seer for feedback label and title generation are failing with HTTP 5xx, causing create_feedback_issue to abort across multiple entry points (user reports, post-processing, and embeds). Impact: user feedback issues are not enriched or created reliably.","tags":["External System","API","Upstream Unavailable","HTTP 5xx","Seer","User Feedback"],"cluster_size":7,"cluster_min_similarity":0.9300335776979602,"cluster_avg_similarity":0.9525497991059699},{"project_ids":["1"],"cluster_id":954,"group_ids":[6806509080,6806511951],"issue_titles":["ModuleNotFoundError: No module named 'sentry.services.eventstore'"],"root_cause_summaries":["Pickled digest records referencing `sentry.services.eventstore` fail to deserialize in taskworker due to missing module in its environment.","Task worker subprocess lacks `sentry.services.eventstore` initialization, preventing pickled object deserialization."],"transactions":["sentry.digests.codecs in decode"],"title":"ModuleNotFoundError for sentry.services.eventstore in digest processing","description":"Digest workers fail while decoding Redis-stored records due to a missing sentry.services.eventstore module during pickle load, breaking digest delivery tasks.","tags":["Configuration","Queueing","Serialization","Redis","Python Pickle","ModuleNotFoundError","Sentry"],"cluster_size":2,"cluster_min_similarity":0.9710628537054428,"cluster_avg_similarity":0.9710628537054428},{"project_ids":["1"],"cluster_id":956,"group_ids":[6806957672,6807003148,6807098979],"issue_titles":["ModuleNotFoundError: No module named 'sentry.filestore'"],"root_cause_summaries":["File storage backend configured with incorrect module path `sentry.filestore.gcs`, missing `services` component.","Filestore backend configured with incorrect module path `sentry.filestore.gcs`, not `sentry.services.filestore.gcs`.","Python environment lacks `sentry.services.filestore` package, preventing `sentry.filestore` shim from loading storage backend."],"transactions":["/avatar/{avatar_id}/","/organizations/{organization_slug}/documents/{policy_slug}/","/api/0/users/{user_id}/avatar/"],"title":"Missing sentry.filestore backend breaks file storage","description":"Django views that read or write FileBlob-backed objects fail because the configured storage backend imports sentry.filestore, which is not installed or no longer available. This prevents fetching policy/user avatar files and other blob-backed assets.","tags":["Configuration","Serialization","Django","Module Not Found","File Storage Backend"],"cluster_size":3,"cluster_min_similarity":0.9591065398743814,"cluster_avg_similarity":0.9674507657831857},{"project_ids":["1"],"cluster_id":959,"group_ids":[6808410799,6808413073,6813921704],"issue_titles":["OutboxFlushError: Could not flush shard category=30 (SENTRY_APP_UPDATE)","OutboxFlushError: Could not flush shard category=9 (API_APPLICATION_UPDATE)"],"root_cause_summaries":["Outbox processing failed due to stale database entries referencing a previously configured, now removed, region.","Stale outbox entries for a decommissioned region cause processing failure due to region resolution error.","Outbox entry created with now-invalid region 's4s2' due to configuration change, causing processing failure."],"transactions":["sentry.tasks.drain_outbox_shards_control"],"title":"Outbox flush fails due to unknown region 's4s2'","description":"Control outbox processing for API/Sentry App updates errors during region routing because 's4s2' is not a configured region (expected 'us' or 'de'), preventing shards from flushing.","tags":["Configuration","Queueing","API","Region Resolution","Outbox","Sentry","Invalid Region"],"cluster_size":3,"cluster_min_similarity":0.9625487421996749,"cluster_avg_similarity":0.9697622457659545}]
