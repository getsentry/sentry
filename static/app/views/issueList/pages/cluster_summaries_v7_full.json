[
  {
    "cluster_id": 13,
    "project_ids": [],
    "group_ids": [4918185292, 5168709733, 6617701043, 6690381475, 6743529496, 6882555644],
    "issue_titles": [
      "ChunkedEncodingError: (\"Connection broken: ConnectionResetError(104, 'Connection reset by peer')\", ConnectionResetError(104, 'Connection reset by peer'))",
      "ChunkedEncodingError: ('Connection broken: IncompleteRead(1182900 bytes read, 2049249 more expected)', IncompleteRead(1182900 bytes read, 2049249 more expected))",
      "ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "ProtocolError: ('Connection broken: IncompleteRead(507 bytes read, 3589 more expected)', IncompleteRead(507 bytes read, 3589 more expected))",
      "ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
    ],
    "title": "HTTP response reading failures in urllib3 connections",
    "description": "Multiple stacktraces show urllib3 failing to read complete HTTP responses, with IncompleteRead errors and socket read failures occurring across Django middleware and multiprocessing workers.",
    "tags": ["Networking", "HTTP", "External System", "urllib3", "Incomplete Read"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9285686971387906,
    "cluster_avg_similarity": 0.9520900740660531,
    "fixability_score": 0.23699264228343964
  },
  {
    "cluster_id": 21,
    "project_ids": [],
    "group_ids": [5288360693, 6532776250, 6536481522, 6921207226, 7007152009],
    "issue_titles": [
      "KafkaException: KafkaError{code=_DESTROY,val=-197,str=\"Failed to get committed offsets: Local: Broker handle destroyed\"}",
      "RetryException",
      "KafkaException: KafkaError{code=_DESTROY,val=-197,str=\"Commit failed: Local: Broker handle destroyed\"}"
    ],
    "title": "Kafka commit fails during consumer shutdown with destroyed broker",
    "description": "StreamProcessor partition revocation callback crashes during shutdown, leaving the Kafka broker handle in a destroyed state when attempting to commit offsets.",
    "tags": [
      "Messaging",
      "Queueing",
      "Kafka",
      "Shutdown Failure",
      "Partition Revocation"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9508190803959216,
    "cluster_avg_similarity": 0.9659682984526523,
    "fixability_score": 0.30813637375831604
  },
  {
    "cluster_id": 22,
    "project_ids": [],
    "group_ids": [5314268382, 7022674841],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"sentry_identity_idp_id_47d379630426f630_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_organizationslugreservation_slug_key\""
    ],
    "title": "Database query failure in Sentry API endpoint",
    "description": "PostgreSQL query execution is failing in the internal RPC API endpoint, with additional failures occurring during Django's error handling process.",
    "tags": ["Database", "API", "PostgreSQL", "Django"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9587265759644403,
    "cluster_avg_similarity": 0.9587265759644403,
    "fixability_score": 0.4137519896030426
  },
  {
    "cluster_id": 24,
    "project_ids": [],
    "group_ids": [5353134098, 5647494211, 6632448159],
    "issue_titles": [
      "ApiHostError: Unable to reach host: sentry-rpc-prod-control.us.sentry.internal:8999"
    ],
    "title": "SCM integration HTTP socket read failures",
    "description": "Source code management integrations (GitLab stacktrace links, repository choices, PR workflows) are failing when making HTTP requests due to socket-level read operations failing during response processing.",
    "tags": ["External System", "Networking", "HTTP", "Socket Read Failure"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9557680195263318,
    "cluster_avg_similarity": 0.9582710228173221,
    "fixability_score": 0.21311357617378235
  },
  {
    "cluster_id": 26,
    "project_ids": [],
    "group_ids": [
      5360930327, 6656608692, 6672091732, 6675765299, 6675905688, 6676253500, 6712110232,
      6713214240, 6713727616, 6713929018, 6735586674, 6782675336, 6789207839, 6792288439,
      6792968209, 6838418923, 6840698708, 6843751363, 6977945398
    ],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='filestore-default.ilb.de.sentry.internal.', port=8080): Read timed out. (read timeout=5)",
      "ReadTimeoutError: HTTPConnectionPool(host='vroom', port=80): Read timed out. (read timeout=15)",
      "ReadTimeoutError: HTTPConnectionPool(host='192.168.208.181', port=8080): Read timed out. (read timeout=5)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=15)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-gpu-web-group-seer', port=80): Read timed out. (read timeout=0.6)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=5)"
    ],
    "title": "HTTP connection failure during exception handling",
    "description": "Sentry's exception handling middleware is failing to establish HTTP connections when attempting to capture and report errors, causing secondary failures during error processing.",
    "tags": [
      "Networking",
      "External System",
      "HTTP",
      "Connection Reset",
      "Django",
      "Urllib3"
    ],
    "cluster_size": 19,
    "cluster_min_similarity": 0.9287319354634074,
    "cluster_avg_similarity": 0.9543497202900616,
    "fixability_score": 0.22617070376873016
  },
  {
    "cluster_id": 31,
    "project_ids": [],
    "group_ids": [5539974675, 6659813316, 6719756970],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.scheduleMessage)"
    ],
    "title": "Slack channel validation fails with channel_not_found",
    "description": "Slack integration cannot validate channel accessibility when creating alert rules, likely due to the bot not being invited to private channels or lacking required permissions.",
    "tags": ["External System", "Authentication", "Slack", "Channel Not Found"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9537888236991839,
    "cluster_avg_similarity": 0.9653532022138016,
    "fixability_score": 0.35476887226104736
  },
  {
    "cluster_id": 32,
    "project_ids": [],
    "group_ids": [5563126522, 7024922639],
    "issue_titles": [
      "ApiUnauthorized: {\"message\":\"401 Unauthorized\"}",
      "ApiUnauthorized: {\"code\":\"internal\",\"message\":\"Error\"}"
    ],
    "title": "HTTP errors in external service integration calls",
    "description": "Sentry integration endpoints are receiving HTTP error responses when making requests to external services, causing request failures in the API layer.",
    "tags": ["External System", "API", "HTTP Error", "Django"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9630286484385442,
    "cluster_avg_similarity": 0.9630286484385442,
    "fixability_score": 0.2843374013900757
  },
  {
    "cluster_id": 33,
    "project_ids": [],
    "group_ids": [5589731605, 6962588483],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/conversations.info)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.scheduleMessage)"
    ],
    "title": "Slack channel validation fails with bare channel names",
    "description": "The channel validation logic attempts to verify channel existence using chat.scheduleMessage with bare channel names (e.g., 'lady'), but Slack API requires properly formatted identifiers like channel IDs or names with '#' prefix, causing channel_not_found errors.",
    "tags": ["External System", "API", "Input Validation", "Slack", "Channel Not Found"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9625292685213518,
    "cluster_avg_similarity": 0.9625292685213518,
    "fixability_score": 0.383302241563797
  },
  {
    "cluster_id": 34,
    "project_ids": [],
    "group_ids": [5637871250, 6840520387],
    "issue_titles": ["ApiTimeoutError: Timed out attempting to reach host: discord.com"],
    "title": "Discord notification SSL read failure in task worker",
    "description": "Task workers processing Discord issue alerts are encountering SSL read failures when attempting to send notifications, causing the notification delivery to fail.",
    "tags": ["Networking", "External System", "TLS", "Discord", "SSL Read Failure"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.966982022089167,
    "cluster_avg_similarity": 0.966982022089167,
    "fixability_score": 0.21006160974502563
  },
  {
    "cluster_id": 37,
    "project_ids": [],
    "group_ids": [5859179121, 6805580390],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/users.list)"
    ],
    "title": "Slack API error during user/channel lookup",
    "description": "Slack integration tasks are failing when attempting to retrieve user lists or resolve channel IDs, resulting in SlackApiError exceptions during rule processing.",
    "tags": ["External System", "API", "Slack", "SlackApiError"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9760309199835241,
    "cluster_avg_similarity": 0.9760309199835241,
    "fixability_score": 0.24834586679935455
  },
  {
    "cluster_id": 40,
    "project_ids": [],
    "group_ids": [6032121515, 6789342363, 7001894808, 7015274361, 7015274408],
    "issue_titles": [
      "Failed to decode Slack API response: Received a response in a non-JSON format: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"><html><head><title>503 Service Unavailable</title>...",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)",
      "Unrecognized Slack API message. Api Message: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"
    ],
    "title": "Slack API 503 error with HTML response causes unhandled exception",
    "description": "Slack API returned a 503 Service Unavailable error with HTML content instead of JSON, causing the Slack SDK to raise a SlackRequestError that isn't caught by Sentry's error handling, which only catches SlackApiError.",
    "tags": [
      "External System",
      "API",
      "Slack",
      "Service Unavailable",
      "JSON Parsing Error"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.93546555442882,
    "cluster_avg_similarity": 0.9531360301474896,
    "fixability_score": 0.3318910598754883
  },
  {
    "cluster_id": 44,
    "project_ids": [],
    "group_ids": [
      6190077117, 7004867951, 7009473561, 7009542179, 7009542527, 7009583136, 7009583311,
      7009583340, 7009583523, 7010115661, 7010116084
    ],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth state mismatch from corrupted callback parameters",
    "description": "OAuth callback validation fails when the state parameter gets corrupted or URL-encoded incorrectly during the redirect flow, causing security validation to reject the authentication request.",
    "tags": [
      "Authentication",
      "OAuth",
      "External System",
      "URL Encoding",
      "State Validation"
    ],
    "cluster_size": 11,
    "cluster_min_similarity": 0.9184212018762338,
    "cluster_avg_similarity": 0.9495528148054213,
    "fixability_score": 0.3103523254394531
  },
  {
    "cluster_id": 47,
    "project_ids": [],
    "group_ids": [6268620370, 6268915744],
    "issue_titles": ["AssertionError"],
    "title": "Empty queue access in batch builder reset",
    "description": "The batch builder is attempting to pop from an empty input_blocks queue during reset operations, indicating a synchronization issue in the multiprocessing strategy.",
    "tags": ["Queueing", "Concurrency", "Arroyo", "Empty Queue"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9840100646274983,
    "cluster_avg_similarity": 0.9840100646274983,
    "fixability_score": 0.2543649971485138
  },
  {
    "cluster_id": 48,
    "project_ids": [],
    "group_ids": [6269047879, 6270530195],
    "issue_titles": [
      "KafkaException: KafkaError{code=UNKNOWN_MEMBER_ID,val=25,str=\"Commit failed: Broker: Unknown member\"}"
    ],
    "title": "Arroyo message processing failures in thread strategy",
    "description": "Messages are being rejected during processing in Arroyo's thread-based strategy, with additional failures in the multiprocessing batch builder reset.",
    "tags": ["Queueing", "Concurrency", "Arroyo", "Message Rejected"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9587829769223226,
    "cluster_avg_similarity": 0.9587829769223226,
    "fixability_score": 0.29818978905677795
  },
  {
    "cluster_id": 50,
    "project_ids": [],
    "group_ids": [6291262905, 6304574025, 6929873784],
    "issue_titles": ["AssertionError"],
    "title": "Message processing backpressure lost during rebalance",
    "description": "Kafka consumer pause state is cleared during partition rebalancing, causing buffered messages to be processed despite backpressure from MessageRejected exceptions in the processing pipeline.",
    "tags": ["Messaging", "Concurrency", "Kafka", "Message Rejected", "Race Condition"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9640066963755543,
    "cluster_avg_similarity": 0.9659953450803537,
    "fixability_score": 0.3246645927429199
  },
  {
    "cluster_id": 51,
    "project_ids": [],
    "group_ids": [
      6351856285, 6624256751, 6656458218, 6673102888, 6680920814, 6711459865, 6725380019,
      6725380254, 6782612724, 6782644221, 6782679792, 6791854756, 6792450203, 6792515143,
      6792656623, 6792751930, 6792752935, 6793135079, 6793537536, 6793840201, 6793968470,
      6793972067, 6794198699, 6794201570, 6794593243, 6794593334, 6794670713, 6795050820,
      6795050852, 6795338666, 6798275869, 6800373349, 6802615426, 6802615434, 6841165034,
      6842708313, 6842780823, 6844034163, 6875382663, 6926399661
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 18 exceeds limit of 16', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'r...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 21 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 26 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 52 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'f...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 121 exceeds limit of 120', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 17 exceeds limit of 16', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 21 exceeds limit of 20', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 22 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e..."
    ],
    "title": "Snuba queries hitting rate limits across API endpoints",
    "description": "Multiple Sentry API endpoints and background tasks are being rate limited when querying Snuba for tag data, group statistics, and search operations.",
    "tags": ["External System", "Rate Limiting", "API", "Snuba", "RateLimitExceeded"],
    "cluster_size": 40,
    "cluster_min_similarity": 0.9392873848125134,
    "cluster_avg_similarity": 0.9654612616589429,
    "fixability_score": 0.2436578869819641
  },
  {
    "cluster_id": 52,
    "project_ids": [],
    "group_ids": [6360138421, 6972399590],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "Django PostgreSQL connection failure during request",
    "description": "Django web requests are failing when attempting to establish new database connections to PostgreSQL, likely due to connection pool exhaustion or database unavailability.",
    "tags": ["Database", "Networking", "Django", "PostgreSQL", "Connection Reset"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9915158951784351,
    "cluster_avg_similarity": 0.9915158951784351,
    "fixability_score": 0.26699313521385193
  },
  {
    "cluster_id": 54,
    "project_ids": [],
    "group_ids": [
      6395607447, 6662361418, 6705259139, 6794396371, 6794586191, 6819785886, 6834638487,
      6841165038, 6871496142, 6876832922, 7015017604
    ],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 2 exceeds limit of 1', 'overrides': {'organization_id__3741...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 32 exceeds limit of 30', 'overrides': {}, 'storage_key': 's...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e..."
    ],
    "title": "Snuba concurrent query limit exceeded in API endpoints",
    "description": "Multiple API endpoints are spawning concurrent query threads that exceed Snuba's rate limiting policies, causing queries to be rejected when the concurrent execution limit is reached.",
    "tags": [
      "Rate Limiting",
      "Concurrency",
      "API",
      "External System",
      "Snuba",
      "Retries Exhausted"
    ],
    "cluster_size": 11,
    "cluster_min_similarity": 0.9329345625862301,
    "cluster_avg_similarity": 0.9539409969111632,
    "fixability_score": 0.2385142743587494
  },
  {
    "cluster_id": 55,
    "project_ids": [],
    "group_ids": [6457005475, 6613139051, 6673722701, 6923332877],
    "issue_titles": [
      "UnqualifiedQueryError: Validation failed for entity events: missing required conditions for project_id",
      "UnqualifiedQueryError: Validation failed for entity search_issues: missing required conditions for project_id"
    ],
    "title": "Snuba queries missing required project_id conditions",
    "description": "TSDB queries fail when project_id inference from group_id lookups returns empty results, causing Snuba to reject queries that lack required project_id conditions.",
    "tags": ["Database", "Input Validation", "Snuba", "UnqualifiedQueryError"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9357895524975065,
    "cluster_avg_similarity": 0.9534445082075108,
    "fixability_score": 0.5545795559883118
  },
  {
    "cluster_id": 63,
    "project_ids": [],
    "group_ids": [
      6587578534, 6707919991, 6837754302, 6870261730, 6870304852, 6887933835, 7004545656,
      7008251292, 7015879866
    ],
    "issue_titles": [
      "DecodeError: Error parsing message",
      "RetryException: Could not successfully execute <function query_trace.<locals>.<lambda> at 0x7d325d324540> within 273.113 seconds (7 attempts.)"
    ],
    "title": "Snuba RPC protobuf parsing fails on HTTP 503 errors",
    "description": "When Snuba returns HTTP 503 Service Unavailable, the RPC client attempts to parse the non-protobuf response body as an ErrorProto message, causing DecodeError instead of gracefully handling the service unavailability.",
    "tags": [
      "External System",
      "API",
      "Serialization",
      "Snuba",
      "Protobuf",
      "Service Unavailable"
    ],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9326926775926125,
    "cluster_avg_similarity": 0.9663415298302126,
    "fixability_score": 0.3174983561038971
  },
  {
    "cluster_id": 64,
    "project_ids": [],
    "group_ids": [
      6591547772, 6712676758, 6712677141, 6715902423, 6729521164, 6775572003, 6794932421,
      6995215168, 6999513150, 6999514210
    ],
    "issue_titles": [
      "InvalidSearchQuery: Invalid value '['AWAY-RESORTS-WEBSITE-QJ']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['NODE-UI-2KF']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['ORCA-HZ', 'ORCA-HN', 'ORCA-M6', 'ORCA-2QG', 'ORCA-6N1', 'ORCA-43D', 'ORCA-831']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['HW-ADMIN-JS-ANX']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['CHRONICLER-10J']' for 'issue:' filter"
    ],
    "title": "Snuba subscription fails on non-existent group lookup",
    "description": "Alert rule comparison queries are failing because they contain issue filters referencing groups that no longer exist in the database, causing Group.DoesNotExist exceptions during query processing.",
    "tags": ["Database", "Data Integrity", "Messaging", "Group Lookup Failed", "Snuba"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9275819973572328,
    "cluster_avg_similarity": 0.9617364891885993,
    "fixability_score": 0.5279834866523743
  },
  {
    "cluster_id": 66,
    "project_ids": [],
    "group_ids": [6592963812, 6879549535, 6886076510, 6889217196],
    "issue_titles": [
      "IdentityNotValid",
      "ApiError: 404 page not found",
      "ApiUnauthorized: {\"message\":\"401 Unauthorized\"}",
      "ApiInvalidRequestError"
    ],
    "title": "GitLab OAuth token refresh failing in integration proxy",
    "description": "The GitLab integration proxy in the control silo fails to refresh expired OAuth tokens, returning 400 Bad Request to region silos when IdentityNotValid errors occur during token refresh attempts.",
    "tags": [
      "External System",
      "Authentication",
      "API",
      "GitLab",
      "OAuth Token Refresh",
      "Integration Proxy",
      "IdentityNotValid"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.933159989288668,
    "cluster_avg_similarity": 0.9509310219561511,
    "fixability_score": 0.26826757192611694
  },
  {
    "cluster_id": 68,
    "project_ids": [],
    "group_ids": [6603108012, 6619447005, 6619447007, 6783693043, 6933429386, 6933429606],
    "issue_titles": [
      "QueryOutsideRetentionError: Invalid date range. Please try a more recent date range."
    ],
    "title": "Query date range exceeds retention policy",
    "description": "API requests are failing because the requested date ranges fall outside the configured data retention period, triggering QueryOutsideRetentionError across multiple endpoints.",
    "tags": ["API", "Data Integrity", "Configuration", "Query Outside Retention"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9581338833225538,
    "cluster_avg_similarity": 0.9702877421610857,
    "fixability_score": 0.38353291153907776
  },
  {
    "cluster_id": 69,
    "project_ids": [],
    "group_ids": [6603108054, 6614051917],
    "issue_titles": [
      "JSONDecodeError: Input is a zero-length, empty document: line 1 column 1 (char 0)"
    ],
    "title": "MS Teams integration parser failing on request body",
    "description": "The Microsoft Teams integration middleware is encountering errors when parsing incoming request data using orjson.loads() on the request body.",
    "tags": ["API", "External System", "Serialization", "Microsoft Teams", "orjson"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9826961901977936,
    "cluster_avg_similarity": 0.9826961901977936,
    "fixability_score": 0.6063884496688843
  },
  {
    "cluster_id": 70,
    "project_ids": [],
    "group_ids": [6603110796, 6614836650],
    "issue_titles": ["KeyError: 'HTTP_X_GITLAB_TOKEN'"],
    "title": "GitLab webhook missing required token header",
    "description": "GitHub webhooks are being incorrectly routed to GitLab webhook endpoints, causing KeyError when GitLab parser attempts to access the missing HTTP_X_GITLAB_TOKEN header. This indicates customer webhook misconfiguration.",
    "tags": [
      "External System",
      "Configuration",
      "Input Validation",
      "GitLab",
      "Missing Header"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9739566502936402,
    "cluster_avg_similarity": 0.9739566502936402,
    "fixability_score": 0.6011207699775696
  },
  {
    "cluster_id": 71,
    "project_ids": [],
    "group_ids": [6603111918, 6603259068],
    "issue_titles": ["AtlassianConnectValidationError: Signature is invalid"],
    "title": "Jira JWT signature verification failing on shared secret",
    "description": "JWT tokens from Atlassian Jira are failing signature verification, indicating the shared secret stored in Sentry's integration metadata no longer matches the secret Jira is using to sign tokens.",
    "tags": [
      "External System",
      "Authentication",
      "Security",
      "JWT",
      "Atlassian",
      "Invalid Signature"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9636244477200857,
    "cluster_avg_similarity": 0.9636244477200857,
    "fixability_score": 0.2965473532676697
  },
  {
    "cluster_id": 72,
    "project_ids": [],
    "group_ids": [6603118689, 6623441649],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Unknown function failure_rate: While processing toDateTime(intDiv(toUInt32(timestamp), 1800) * 1800, 'Universal') AS _snuba_time, count() / (1800 / 60) AS _snuba_epm, failure_rate() AS _snuba_failure_rate. Stack trace:",
      "QueryExecutionError: DB::Exception: Unknown function failure_rate: While processing `tags.value`[indexOf(`tags.key`, 'product')] AS `_snuba_tags[product]`, `_snuba_tags[product]`, failure_rate() AS _snuba_failure_rate. Stack trace:"
    ],
    "title": "ClickHouse GROUP BY validation error in events stats",
    "description": "Organization events stats queries fail when sorting by project with aggregations due to auto-generated column aliases not being included in the GROUP BY clause, violating ClickHouse aggregation requirements.",
    "tags": ["Database", "API", "ClickHouse", "Aggregation", "Query Building"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9592624726629262,
    "cluster_avg_similarity": 0.9592624726629262,
    "fixability_score": 0.4897254705429077
  },
  {
    "cluster_id": 73,
    "project_ids": [],
    "group_ids": [6603120581, 6613705158],
    "issue_titles": ["ValueError: too many values to unpack (expected 3)"],
    "title": "GitLab webhook token parsing fails with custom ports",
    "description": "Webhook tokens from GitLab instances using non-standard ports contain extra colons, causing unpacking failures when parsing the token format.",
    "tags": ["External System", "API", "GitLab", "Input Validation"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9765217256735055,
    "cluster_avg_similarity": 0.9765217256735055,
    "fixability_score": 0.6039056181907654
  },
  {
    "cluster_id": 75,
    "project_ids": [],
    "group_ids": [
      6603148180, 6619019742, 6655096517, 6675762509, 6678435574, 6710520999, 6725352853,
      6725386635, 6725421827, 6725487246, 6725487270, 6725517699, 6725580573, 6725580735,
      6725665272, 6725691300, 6775316556, 6793011113, 6796173506, 7020208264
    ],
    "issue_titles": [
      "InvalidSearchQuery: Invalid value '['ROZPOCETPRO-WEB-49']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['V2XHUB-1C']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['SAFE365-77']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['OCTOPUS-WINDY-BACKEND-2S']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['TRACKER-V2-106']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['NODE-UI-2KF']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['SENTRY-14ZS']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['WXG-TABLET-CLIENT-559']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['CHATFORM-1G5']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['HW-ADMIN-JS-ANX']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['CLOUD-FUNCTIONS-B90']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['SILVERFIN-2QJC']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['FRONTEND-Q09']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['NEW-ORDER-SERVICE-EY']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['PROD-SONGWHIP-LOOKUP-2Z']' for 'issue:' filter"
    ],
    "title": "Group lookup failures in search filter processing",
    "description": "Multiple components are encountering DoesNotExist exceptions when attempting to resolve Group objects by qualified short IDs during search filter conversion. This affects subscription processing, API requests, and task workers.",
    "tags": [
      "Database",
      "API",
      "Data Integrity",
      "Group Lookup Failure",
      "Search Filter Processing"
    ],
    "cluster_size": 20,
    "cluster_min_similarity": 0.9267991802840768,
    "cluster_avg_similarity": 0.9631162686609894,
    "fixability_score": 0.6657784581184387
  },
  {
    "cluster_id": 76,
    "project_ids": [],
    "group_ids": [6603150949, 6710256302],
    "issue_titles": ["Http404", "KeyError: 'id'"],
    "title": "GitLab webhook missing project field in payload",
    "description": "GitLab webhook events are arriving without the required 'project' object in the payload, likely due to a custom webhook template configuration that excludes this field. The webhook handler cannot extract the project ID and raises Http404 exceptions.",
    "tags": ["External System", "Input Validation", "API", "GitLab", "Missing Field"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9664667365229663,
    "cluster_avg_similarity": 0.9664667365229663,
    "fixability_score": 0.6773137450218201
  },
  {
    "cluster_id": 79,
    "project_ids": [],
    "group_ids": [6604468660, 7024562390, 7024592603],
    "issue_titles": [
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-transactions-tiger-mz-3-2:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.40 GiB (attempt to allocate chunk of 4363554 bytes), maximum: 9.31 GiB.: (while reading column transaction_name): (while rea...",
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-1-6:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.43 GiB (attempt to allocate chunk of 1073741824 bytes), maximum: 9.31 GiB.: While executing ReplacingSorted. Stack trace:",
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-2-5:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.67 GiB (attempt to allocate chunk of 1073740336 bytes), maximum: 9.31 GiB.. Stack trace:"
    ],
    "title": "ClickHouse query execution errors in Snuba backend",
    "description": "Multiple API endpoints are failing due to ClickHouse query execution errors when retrieving replay counts and tag data through the Snuba query layer.",
    "tags": ["Database", "External System", "API", "ClickHouse", "Query Execution Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9712789051395123,
    "cluster_avg_similarity": 0.9756689348885411,
    "fixability_score": 0.25664636492729187
  },
  {
    "cluster_id": 81,
    "project_ids": [],
    "group_ids": [6612807396, 6620320653, 6620861558],
    "issue_titles": [
      "AtlassianConnectValidationError: No integration found",
      "AtlassianConnectValidationError: No token parameter"
    ],
    "title": "Jira integration missing JWT token validation error",
    "description": "Requests to Jira issue endpoints are failing because JWT tokens are missing from query parameters or Authorization headers, causing AtlassianConnectValidationError during integration parsing.",
    "tags": ["External System", "Authentication", "API", "Jira", "Missing Token"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9666845401754749,
    "cluster_avg_similarity": 0.9745929904748091,
    "fixability_score": 0.5513900518417358
  },
  {
    "cluster_id": 83,
    "project_ids": [],
    "group_ids": [6612844028, 6738967728, 7001617188],
    "issue_titles": [
      "UnqualifiedQueryError: Validation failed for entity events: Tag keys (span_op_breakdowns) not resolved",
      "UnqualifiedQueryError: Validation failed for entity search_issues: Tag keys (flags_key) not resolved",
      "UnqualifiedQueryError: Validation failed for entity events: Tag keys (tags[mac catalyst app.name]) not resolved"
    ],
    "title": "Snuba queries fail with invalid tag keys containing spaces",
    "description": "Tag keys containing spaces (e.g., 'mac catalyst app.name') are causing Snuba validation failures when querying for empty value counts, as Snuba's tag key validation rejects keys with spaces.",
    "tags": ["Database", "Input Validation", "API", "Snuba", "UnqualifiedQueryError"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9492857047883572,
    "cluster_avg_similarity": 0.9531576863656194,
    "fixability_score": 0.4723585844039917
  },
  {
    "cluster_id": 88,
    "project_ids": [],
    "group_ids": [6613380217, 6615866385],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Cannot convert string 2025-11-13T11:26:46+00:00 to type DateTime: While processing ((environment AS _snuba_environment) = 'production') AND ((deleted = 0) AND has(_tags_hash_map, cityHash64('application=vendor-details-microfrontend')) AND...",
      "QueryExecutionError: DB::Exception: Cannot convert string 2025-11-11T00:00:00+00:00 to type DateTime: While processing (ifNull(release AS `_snuba_tags[sentry:release]`, '') IN ['com.nvent.olarm@2.0.15+16', 'com.olarm.olarm1@2.0.15+215016']) AND ((deleted = 0) AND (match(mes..."
    ],
    "title": "ClickHouse DateTime conversion fails with timezone",
    "description": "First query returns timestamp values as ISO strings with timezone info, but second query cannot convert these timezone-aware strings to DateTime type for comparison with toStartOfDay() results.",
    "tags": ["Database", "Data Integrity", "ClickHouse", "Timestamp Conversion"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9714381792078206,
    "cluster_avg_similarity": 0.9714381792078206,
    "fixability_score": 0.572837769985199
  },
  {
    "cluster_id": 89,
    "project_ids": [],
    "group_ids": [
      6613478263, 6623877582, 6632499142, 6635924641, 6643954095, 6643954097, 6644183937,
      6659351110, 6666638254, 6683783925, 6705191185, 6712727430, 6718939887, 6726293844,
      6734120739, 6734120945, 6745748294, 6750567715, 6792404555, 6792414712, 6792416414,
      6792468106, 6793495579, 6793982063, 6799535383, 6803845693, 6804300361, 6814172929,
      6837753300, 6852720532, 6905210369, 6913434304, 6920188494, 6921690153, 6921691201,
      7016793655
    ],
    "issue_titles": [
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "Snuba HTTP read timeout on API requests",
    "description": "Multiple Django API endpoints are failing with 30-second read timeouts when making HTTP requests to Snuba for data analytics queries. The timeouts occur across different endpoints including replay counts, issue stats, project stats, metrics data, and tag values.",
    "tags": ["Networking", "External System", "API", "Django", "Snuba", "Timeout"],
    "cluster_size": 36,
    "cluster_min_similarity": 0.9073972784222826,
    "cluster_avg_similarity": 0.938599162437738,
    "fixability_score": 0.23075251281261444
  },
  {
    "cluster_id": 91,
    "project_ids": [],
    "group_ids": [6613725819, 6616569843],
    "issue_titles": [
      "TypeError: unsupported operand types(s) or combination of types: 'NoneType' and 'str'"
    ],
    "title": "Heroku webhook HMAC validation fails with missing header",
    "description": "The Heroku release webhook handler attempts to validate signatures using hmac.compare_digest() but fails when the required Heroku-Webhook-Hmac-SHA256 header is missing, passing None to a function expecting string inputs.",
    "tags": [
      "External System",
      "Input Validation",
      "Security",
      "Heroku",
      "HMAC Validation"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9712736986736364,
    "cluster_avg_similarity": 0.9712736986736364,
    "fixability_score": 0.6438634991645813
  },
  {
    "cluster_id": 93,
    "project_ids": [],
    "group_ids": [6613950202, 6802477486],
    "issue_titles": ["File.DoesNotExist: File matching query does not exist."],
    "title": "ArtifactBundle cleanup accesses deleted File objects",
    "description": "The post-delete signal handler for ArtifactBundle tries to access and delete File objects that were already cascade-deleted by Django's ORM, causing File.DoesNotExist errors during cleanup operations.",
    "tags": [
      "Database",
      "Django ORM",
      "Signal Handlers",
      "Cascade Delete",
      "Data Integrity"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9714398762560824,
    "cluster_avg_similarity": 0.9714398762560824,
    "fixability_score": 0.5135361552238464
  },
  {
    "cluster_id": 96,
    "project_ids": [],
    "group_ids": [6614447366, 6615279706, 6696880291, 6734430244, 6841202580],
    "issue_titles": [
      "Detector.MultipleObjectsReturned: get() returned more than one Detector -- it returned 2!",
      "Repository.MultipleObjectsReturned: get() returned more than one Repository -- it returned 2!"
    ],
    "title": "Repository.MultipleObjectsReturned from ambiguous queries",
    "description": "Multiple repositories with the same name or external_id exist in organizations, causing Django .get() queries to fail when not filtering by provider or unique identifiers.",
    "tags": ["Database", "Data Integrity", "Django", "Multiple Objects Returned"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9433229811762585,
    "cluster_avg_similarity": 0.9593309792338889,
    "fixability_score": 0.45422402024269104
  },
  {
    "cluster_id": 102,
    "project_ids": [],
    "group_ids": [6614888006, 6621958996],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e..."
    ],
    "title": "Snuba concurrent rate limit exceeded by dashboard widgets",
    "description": "Dashboard widget queries are triggering excessive concurrent Snuba requests through dataset discovery logic, exceeding the organization's 18-query concurrent limit and causing rate limit rejections.",
    "tags": ["Rate Limiting", "API", "Concurrency", "Dashboard", "Snuba"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9598321311845347,
    "cluster_avg_similarity": 0.9598321311845347,
    "fixability_score": 0.23241761326789856
  },
  {
    "cluster_id": 104,
    "project_ids": [],
    "group_ids": [
      6615110650, 6661409875, 6669329931, 6790267383, 6806357457, 6809137192, 6906089410,
      6920552014
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'BytesScannedRejectingPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'project_id 1223698 is over the bytes scanned limit of 320000000000 for referrer tagst...",
      "RateLimitExceeded: Query scanned more than the allocated amount of bytes",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'BytesScannedRejectingPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'project_id 1447267 is over the bytes scanned limit of 150000000000 for referrer repla..."
    ],
    "title": "Snuba rate limiting from unbounded user count queries",
    "description": "Snooze validation queries scan entire group history without time constraints, causing queries to exceed Snuba's byte allocation limits for high-volume projects.",
    "tags": ["Rate Limiting", "External System", "Snuba", "Query Optimization"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9247465763851057,
    "cluster_avg_similarity": 0.9526430104825502,
    "fixability_score": 0.2996459901332855
  },
  {
    "cluster_id": 108,
    "project_ids": [],
    "group_ids": [
      6615425161, 6623877702, 6659145162, 6659148952, 6659357448, 6659410713, 6665586681,
      6665979788, 6667514267, 6668404896, 6671504118, 6671993133, 6672965611, 6672965735,
      6672965958, 6672965965, 6672966357, 6673109215, 6673723150, 6673723306, 6673723357,
      6673723510, 6673723648, 6674926170, 6675538863, 6678002632, 6684476185, 6689654993,
      6693844110, 6696789901, 6696789902, 6696789961, 6709237746, 6712322036, 6712727452,
      6713083961, 6713162944, 6717462080, 6719467316, 6725404370, 6726293839, 6752741393,
      6783281778, 6786887317, 6788631847, 6792415595, 6792442851, 6792468318, 6792469745,
      6792948230, 6793472562, 6793495560, 6797461617, 6802818968, 6803204984, 6803205131,
      6806423177, 6812736658, 6838185572, 6857524795, 6867024086, 6905209336, 6912902777,
      6912905641, 6936293528
    ],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "Socket errors during HTTP requests to external service",
    "description": "Multiple stacktraces showing socket-level failures during HTTP communication, occurring both during API request handling and serialization logging operations.",
    "tags": ["Networking", "External System", "Connection Reset", "Django", "urllib3"],
    "cluster_size": 65,
    "cluster_min_similarity": 0.9452285432624958,
    "cluster_avg_similarity": 0.9725947226397575,
    "fixability_score": 0.2321964055299759
  },
  {
    "cluster_id": 114,
    "project_ids": [],
    "group_ids": [
      6616732997, 6689732332, 6689732381, 6689739785, 6689740124, 6689740141, 6689740219,
      6689740250, 7009473367, 7009473391, 7009473493, 7009473512, 7009473538, 7009542016,
      7009542028, 7009542098, 7009542126, 7009542131, 7009542138, 7009542195, 7009542237,
      7009542277, 7009542292, 7009542298, 7009542335, 7009542564, 7009583196, 7009583236,
      7009583249, 7009583280, 7009583300, 7009583345, 7009583353, 7009583386, 7009583400,
      7009583402, 7009583425, 7009583426, 7009583682
    ],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth error parameter injection vulnerability",
    "description": "Unsanitized OAuth error parameters are being directly interpolated into error messages and logs, allowing attackers to inject malicious payloads through crafted callback URLs.",
    "tags": ["Security", "Input Validation", "OAuth", "Injection Attack"],
    "cluster_size": 39,
    "cluster_min_similarity": 0.9345119865309864,
    "cluster_avg_similarity": 0.9682937024588304,
    "fixability_score": 0.38941001892089844
  },
  {
    "cluster_id": 116,
    "project_ids": [],
    "group_ids": [6617779764, 6909613563],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Unknown function isHandled: While processing (isHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-11-05T18:34:25', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-11-06T18:35:25', 'Universal')) AND ((project_i...",
      "QueryExecutionError: DB::Exception: Unknown function isHandled: While processing (isHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-11-12T02:25:50', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-11-13T02:26:50', 'Universal')) AND ((project_i..."
    ],
    "title": "ClickHouse unknown function isHandled() in transactions",
    "description": "The error.handled field is generating an isHandled() function call that doesn't exist in ClickHouse when querying the transactions dataset. The field alias converter lacks dataset validation that exists in the filter converter.",
    "tags": ["Database", "API", "ClickHouse", "Query Execution Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9572946756023213,
    "cluster_avg_similarity": 0.9572946756023213,
    "fixability_score": 0.5521491169929504
  },
  {
    "cluster_id": 119,
    "project_ids": [],
    "group_ids": [6618594859, 6701639376],
    "issue_titles": ["AssertionError"],
    "title": "SAML/OAuth SSO assertion failure on invite acceptance",
    "description": "Race condition during SSO login where invite acceptance returns None when user membership already exists, causing assertion failure in auth pipeline.",
    "tags": [
      "Authentication",
      "Concurrency",
      "SAML",
      "OAuth",
      "Django",
      "Assertion Error"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9692561725469814,
    "cluster_avg_similarity": 0.9692561725469814,
    "fixability_score": 0.40053024888038635
  },
  {
    "cluster_id": 120,
    "project_ids": [],
    "group_ids": [6618877476, 6724510724, 6867384936],
    "issue_titles": ["SnubaRPCError: code: 408"],
    "title": "Snuba RPC timeout on high accuracy timeseries queries",
    "description": "High accuracy timeseries queries over large time periods (90 days) are timing out in Snuba when scanning massive amounts of spans data. Snuba suggests querying in normal mode instead of highest accuracy to avoid timeouts.",
    "tags": ["External System", "API", "Snuba", "Timeout", "Query Optimization"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9611213854685559,
    "cluster_avg_similarity": 0.9718477834019194,
    "fixability_score": 0.22059981524944305
  },
  {
    "cluster_id": 123,
    "project_ids": [],
    "group_ids": [
      6623445231, 6657556453, 6675806277, 6675836331, 6676162528, 6697003643, 6708645866,
      6748168934, 6789103509, 6792160715, 6794699090, 6794929605, 6796441705, 6796492717,
      6839216422, 6849461741, 6849461805, 6901424380, 6933245202, 7016209597
    ],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2eb862dd90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d95dd89e570>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79d3e182fad0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /rpc/EndpointTraceItemTable/v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7affc03eac30>: Failed to establish a new connection: [Errno 111] C...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e97ac708170>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c078876a7b0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /generic_metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6e75195130>: Failed to establish a new connection: [Errno 111] Connection...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c9ff4192e70>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /search_issues/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7afa2849f2f0>: Failed to establish a new connection: [Errno 111] Connection r...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe822bb9fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7afff45fd5b0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b18d0697650>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d427c60f770>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /rpc/EndpointTimeSeries/v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x781aec51e8d0>: Failed to establish a new connection: [Errno 111] Conne...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ca060457650>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /rpc/EndpointTraceItemTable/v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7affa0660ef0>: Failed to establish a new connection: [Errno 111] C...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a4808648290>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bbebef671d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f86901d8710>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7adfa4656b10>: Failed to establish a new connection: [Errno 111] Connection refused'))"
    ],
    "title": "Snuba RPC connection refused on snuba-api:80",
    "description": "EAP incident subscription processing fails when attempting to connect to the Snuba RPC service. The snuba-api service is either not running, listening on a different port, or blocked by network policies.",
    "tags": ["External System", "Networking", "Connection Reset", "Snuba"],
    "cluster_size": 20,
    "cluster_min_similarity": 0.9344935709133488,
    "cluster_avg_similarity": 0.9645471767542151,
    "fixability_score": 0.23172231018543243
  },
  {
    "cluster_id": 125,
    "project_ids": [],
    "group_ids": [6624154292, 6672775391],
    "issue_titles": ["IndexError: list index out of range"],
    "title": "Redis rate limit retrieval failures in Sentry",
    "description": "Rate limiting checks are failing when attempting to retrieve current limit values from Redis backend, affecting both monitor check-ins and request processing.",
    "tags": ["Rate Limiting", "Caching", "Redis", "Backend Failure"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9665256078469814,
    "cluster_avg_similarity": 0.9665256078469814,
    "fixability_score": 0.3679281771183014
  },
  {
    "cluster_id": 127,
    "project_ids": [],
    "group_ids": [6626382005, 6736513829, 6776183779, 6781511608, 6866171483],
    "issue_titles": [
      "NotImplementedError: Haven't handled all the search expressions yet"
    ],
    "title": "EAP search resolver fails on malformed boolean expressions",
    "description": "The EAP search query resolver encounters unhandled expression types when processing malformed queries containing empty boolean operators like '( OR )' or '( AND )', resulting in a generic NotImplementedError instead of proper input validation.",
    "tags": [
      "API",
      "Input Validation",
      "EAP",
      "Search Query Parser",
      "NotImplementedError"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9489644448020113,
    "cluster_avg_similarity": 0.9559956335256545,
    "fixability_score": 0.6028211116790771
  },
  {
    "cluster_id": 128,
    "project_ids": [],
    "group_ids": [6626412431, 6925768396],
    "issue_titles": [
      "QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_2' in table 'events': While processing events._snuba_gen_2 AS _snuba_gen_2: While processing SELECT events.`_snuba_events.time` AS `_snuba_events.time`, count() AS _snuba_count FROM (SELECT toStartOfHo...",
      "QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_2' in table 'events': While processing events._snuba_gen_2 AS _snuba_gen_2: While processing SELECT events.`_snuba_issue.id` AS `_snuba_issue.id`, count() AS _snuba_count, uniq(events.`_snuba_events.ta..."
    ],
    "title": "ClickHouse query fails with missing generated column",
    "description": "Snuba's subquery generator creates invalid SQL when processing OR conditions that span multiple subqueries, generating column references that don't exist in the SELECT clause.",
    "tags": ["Database", "API", "ClickHouse", "Query Generation"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9646151157915817,
    "cluster_avg_similarity": 0.9646151157915817,
    "fixability_score": 0.6158624291419983
  },
  {
    "cluster_id": 132,
    "project_ids": [],
    "group_ids": [
      6640678616, 7022187299, 7022187303, 7022535393, 7022535577, 7022535673, 7022535746,
      7022535775, 7022535783, 7022535804, 7022550551, 7022550558, 7022563375
    ],
    "issue_titles": [
      "QueryExecutionError: Connection reset by peer (10.0.0.1:9016)",
      "QueryExecutionError: Connection reset by peer (10.0.0.1:9010)"
    ],
    "title": "ClickHouse query execution failures across multiple endpoints",
    "description": "Multiple API endpoints and background tasks are experiencing query execution errors when communicating with ClickHouse through Snuba, affecting event stats, replay counts, user data retrieval, and webhook processing.",
    "tags": ["Database", "External System", "ClickHouse", "Query Execution Error"],
    "cluster_size": 13,
    "cluster_min_similarity": 0.9535898493951335,
    "cluster_avg_similarity": 0.968739991118931,
    "fixability_score": 0.2523552477359772
  },
  {
    "cluster_id": 136,
    "project_ids": [],
    "group_ids": [6643954102, 6677148219],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "Snuba query timeout with duplicate project_id filters",
    "description": "Project statistics queries are timing out after 30 seconds due to duplicate and malformed project_id conditions in SnQL queries, causing inefficient query planning in ClickHouse.",
    "tags": [
      "API",
      "External System",
      "Database",
      "Snuba",
      "ClickHouse",
      "Timeout",
      "Duplicate Conditions"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9685323665141673,
    "cluster_avg_similarity": 0.9685323665141673,
    "fixability_score": 0.21733048558235168
  },
  {
    "cluster_id": 138,
    "project_ids": [],
    "group_ids": [
      6646074554, 6674161870, 6683949171, 6684066154, 6713625898, 6719906014, 6792744042,
      6802764539, 6948382488, 6977020312
    ],
    "issue_titles": [
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4504849057775616:Member:2745935:ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4504849057775616:Member:2745935:ActiveMembers'",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'usagebuffer.usage_flush_lock:4508357979930704'>> within 4.931 seconds (52 attempts.)",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4507644678569984.1'>> within 4.978 seconds (48 attempts.)",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'subscription:uptime_monitor:cff5cebc9ce84b368ca9a7d41bc15487'>> within 9.909 seconds (70 attempts.)",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'deploy-notify:95172894'>> within 9.966 seconds (71 attempts.)",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:1224075:Member:1868843:ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:1224075:Member:1868843:ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509810457313280:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509810457313280:IssueOwners::ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509729370603600:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509729370603600:IssueOwners::ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509042865209424:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509042865209424:IssueOwners::ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509276312961104:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509276312961104:IssueOwners::ActiveMembers'"
    ],
    "title": "Redis lock acquisition failed in task worker",
    "description": "Task workers are unable to acquire Redis locks due to concurrent lock contention or locks not being properly released within the TTL window, causing distributed lock failures.",
    "tags": ["Concurrency", "Caching", "Redis", "Lock Acquisition Failed"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9196075400980455,
    "cluster_avg_similarity": 0.9507852925118261,
    "fixability_score": 0.28593555092811584
  },
  {
    "cluster_id": 150,
    "project_ids": [],
    "group_ids": [6656669874, 6795629190, 7024656370],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/conversations.info)"
    ],
    "title": "Slack API error during channel validation for alerts",
    "description": "Alert rule configuration is failing when validating Slack channel IDs through the Slack API conversations.info endpoint, preventing proper alert setup.",
    "tags": ["External System", "API", "Slack", "Input Validation"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9669013317089505,
    "cluster_avg_similarity": 0.9743035536384129,
    "fixability_score": 0.4795694351196289
  },
  {
    "cluster_id": 152,
    "project_ids": [],
    "group_ids": [6656884667, 7005277326, 7005538229],
    "issue_titles": [
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-2-5:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.32 GiB (attempt to allocate chunk of 4728496 bytes), maximum: 9.31 GiB.: (while reading column tags.value): (while reading from pa...",
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-1-2:9000. DB::Exception: Memory limit (total) exceeded: would use 56.66 GiB (attempt to allocate chunk of 0 bytes), maximum: 56.53 GiB. OvercommitTracker decision: Memory overcommit has freed not enough...",
      "QueryMemoryLimitExceeded: DB::Exception: Memory limit (for query) exceeded: would use 9.31 GiB (attempt to allocate chunk of 4316704 bytes), maximum: 9.31 GiB.: (avg_value_size_hint = 0, avg_chars_size = 1, limit = 93491): while receiving packet from snuba-errors-tiger-mz-2-3:90..."
    ],
    "title": "ClickHouse memory limit exceeded on Snuba query aggregation",
    "description": "Large aggregation queries from Sentry API endpoints are consuming excessive memory in ClickHouse, causing queries to exceed the configured memory limits. Multiple endpoints including group tags, group details, and tag statistics are affected.",
    "tags": ["Database", "Memory", "API", "ClickHouse", "Query Execution Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9514020527326523,
    "cluster_avg_similarity": 0.9601093290722833,
    "fixability_score": 0.28402385115623474
  },
  {
    "cluster_id": 153,
    "project_ids": [],
    "group_ids": [6657194570, 6881767222, 6883115329, 6886352273, 6912693767],
    "issue_titles": [
      "KeyError: <ExternalProviders.DISCORD: 140>",
      "KeyError: <ExternalProviders.MSTEAMS: 120>"
    ],
    "title": "MSTEAMS notification provider missing from registry",
    "description": "MS Teams notification handler not being imported during taskworker initialization, causing KeyError when dispatching notifications to users with MSTEAMS provider preference.",
    "tags": ["Configuration", "Messaging", "Deployment", "MSTEAMS", "Module Import"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9376231376886022,
    "cluster_avg_similarity": 0.9564928357682418,
    "fixability_score": 0.5928953289985657
  },
  {
    "cluster_id": 164,
    "project_ids": [],
    "group_ids": [6667014357, 6672210447],
    "issue_titles": ["Group.DoesNotExist: Group matching query does not exist."],
    "title": "Event references non-existent Group after deletion",
    "description": "Events stored in Snuba reference group IDs that no longer exist in the database, causing DoesNotExist exceptions during lazy-loading of the group property. This occurs due to a race condition between group deletion and event cleanup, where events persist in the event store after their associated groups are removed.",
    "tags": ["Database", "Data Integrity", "Concurrency", "Django", "Group Not Found"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9727442706653225,
    "cluster_avg_similarity": 0.9727442706653225,
    "fixability_score": 0.522657573223114
  },
  {
    "cluster_id": 165,
    "project_ids": [],
    "group_ids": [6668503039, 6871341515, 6985404975],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bc8520156d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /discover/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2ebbb68290>: Failed to establish a new connection: [Errno 111] Connection refuse...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /functions/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78f564199a30>: Failed to establish a new connection: [Errno 111] Connection refus..."
    ],
    "title": "Snuba API connection refused on port 80",
    "description": "Connection attempts to snuba-api service on port 80 are being refused, likely due to misconfigured SNUBA environment variable or service not listening on expected port.",
    "tags": [
      "Networking",
      "Configuration",
      "External System",
      "Connection Reset",
      "Snuba"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9448428325264466,
    "cluster_avg_similarity": 0.9605102215955168,
    "fixability_score": 0.2289375364780426
  },
  {
    "cluster_id": 174,
    "project_ids": [],
    "group_ids": [6672140751, 7021565235],
    "issue_titles": [
      "InvalidReleaseErrorBadCharacters: invalid release: bad characters in release name"
    ],
    "title": "Semver parsing failure in search query filters",
    "description": "The search event builder is failing to parse semantic version strings when processing search filter conditions, causing errors in the relay parsing component.",
    "tags": [
      "API",
      "Input Validation",
      "Serialization",
      "Search Query",
      "Version Parsing"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.974725552235273,
    "cluster_avg_similarity": 0.974725552235273,
    "fixability_score": 0.43919289112091064
  },
  {
    "cluster_id": 180,
    "project_ids": [],
    "group_ids": [6672345079, 6843213166],
    "issue_titles": [
      "AvataxException: StringLengthError: \"Field 'postalCode' has an invalid length.\""
    ],
    "title": "Avalara tax calculation fails on invalid zip/state combo",
    "description": "Customer billing addresses with postal codes invalid for their specified state/region are sent to Avalara without validation, causing tax calculation failures during subscription changes.",
    "tags": [
      "External System",
      "Input Validation",
      "Avalara",
      "Billing",
      "Address Validation"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9598998033619058,
    "cluster_avg_similarity": 0.9598998033619058,
    "fixability_score": 0.33996862173080444
  },
  {
    "cluster_id": 187,
    "project_ids": [],
    "group_ids": [
      6673035749, 6684018267, 6684120265, 6702927502, 6708494666, 6794594324, 6872759161
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.clear_region_cache",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by getsentry.tasks.quotas._send_reserved_quota_thresholds_notification",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.middleware.integrations.tasks.convert_to_async_slack_response",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by getsentry.tasks.quotas.react_to_spike_protection",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.drain_outbox_shards"
    ],
    "title": "Task worker processes timing out on RPC calls",
    "description": "Multiple task worker processes are hitting processing deadlines while making HTTP requests to remote silos, causing ProcessingDeadlineExceeded exceptions during various operations including outbox processing, notifications, and cache clearing.",
    "tags": ["Networking", "API", "Timeout", "Processing Deadline Exceeded", "RPC"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9354701172979778,
    "cluster_avg_similarity": 0.954345307602348,
    "fixability_score": 0.22040601074695587
  },
  {
    "cluster_id": 253,
    "project_ids": [],
    "group_ids": [
      6673053586, 6675782295, 6675804049, 6698196149, 6770410453, 6792667881, 6858111400,
      6867998872, 6868960925, 6869388543, 6871897761, 6879549779, 6928118656, 6965989772
    ],
    "issue_titles": [
      "ConnectTimeout: HTTPSConnectionPool(host='api.codecov.io', port=443): Max retries exceeded with url: /webhooks/sentry (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f6c14599d10>, 'Connection to api.codecov.io timed out. (connect timeout...",
      "ReadTimeout: SafeHTTPSConnectionPool(host='applications.zoom.us', port=443): Read timed out. (read timeout=5)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=15)",
      "ReadTimeout: HTTPSConnectionPool(host='api.codecov.io', port=443): Read timed out. (read timeout=10)",
      "ReadTimeout: SafeHTTPSConnectionPool(host='api.linear.app', port=443): Read timed out. (read timeout=2.0)",
      "ApiTimeoutError: Timed out attempting to reach host: smba.trafficmanager.net",
      "ApiTimeoutError: Timed out attempting to reach host: api.github.com"
    ],
    "title": "Codecov API timeout in SSL read operation",
    "description": "HTTP requests to api.codecov.io are timing out during SSL read operations, taking over 3 seconds and failing with a 10-second read timeout. Multiple occurrences across API endpoints and background tasks suggest upstream latency or connectivity issues.",
    "tags": ["External System", "Networking", "TLS", "Codecov", "Timeout"],
    "cluster_size": 14,
    "cluster_min_similarity": 0.9139555296538227,
    "cluster_avg_similarity": 0.9525072049730888,
    "fixability_score": 0.2300945520401001
  },
  {
    "cluster_id": 190,
    "project_ids": [],
    "group_ids": [
      6673116275, 6725380229, 6792379115, 6794440815, 6794593956, 6798243190, 6807139069,
      6855873772, 6878014820, 7016735138
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 103 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 102 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e..."
    ],
    "title": "Snuba query rate limit exceeded during webhook serialization",
    "description": "Multiple concurrent workflow notification tasks are triggering unnecessary Snuba queries for group statistics during webhook serialization, exceeding the concurrent query limit of 18-22 queries.",
    "tags": [
      "Rate Limiting",
      "External System",
      "Queueing",
      "Snuba",
      "Retries Exhausted"
    ],
    "cluster_size": 10,
    "cluster_min_similarity": 0.934797987416585,
    "cluster_avg_similarity": 0.9563011575767963,
    "fixability_score": 0.2345627397298813
  },
  {
    "cluster_id": 196,
    "project_ids": [],
    "group_ids": [6674831561, 6809113966, 7013115455],
    "issue_titles": ["TypeError: 'NoneType' object is not iterable"],
    "title": "Email notification fails on null HTTP query string",
    "description": "The safe_urlencode() utility function lacks null input handling, causing email notifications to crash when processing events with null HTTP query strings during digest delivery.",
    "tags": [
      "Serialization",
      "Input Validation",
      "Email Notifications",
      "HTTP Interface",
      "Null Value Handling"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9542580648650998,
    "cluster_avg_similarity": 0.9671024096341837,
    "fixability_score": 0.7470424175262451
  },
  {
    "cluster_id": 255,
    "project_ids": [],
    "group_ids": [6675539406, 6702482376],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "Sentry API timeout from unregistered Snuba referrer",
    "description": "Django API endpoints are timing out after 30 seconds when making Snuba queries with dynamically generated referrers that are not registered in the Referrer enum, causing improper query routing and performance degradation.",
    "tags": ["API", "External System", "Configuration", "Timeout", "Sentry", "Snuba"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.96529966132323,
    "cluster_avg_similarity": 0.96529966132323,
    "fixability_score": 0.22120177745819092
  },
  {
    "cluster_id": 200,
    "project_ids": [],
    "group_ids": [
      6676747935, 6676748076, 6678805069, 6681907680, 6694640273, 6705876262, 6705876268,
      6705936414, 6709058648, 6711460490, 6712694956, 6741298159, 6757146742, 6778613639,
      6793535338, 6839369059, 6855409643, 6906635231, 6960976910, 6977939695, 7021926784
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.store.save_event",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.options.sync_options_control",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.rules.processing.delayed_processing",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.workflow_engine.tasks.trigger_action",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.process_buffer.process_incr",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.profiles.task.process_profile",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.tasks.relay.build_project_config",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by getsentry.tasks.run_spike_projection",
      "ProcessingDeadlineExceeded: execution deadline of 45 seconds exceeded by sentry.integrations.source_code_management.tasks.pr_comment_workflow",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.integrations.slack.tasks.send_activity_notifications_to_slack_threads",
      "ProcessingDeadlineExceeded: execution deadline of 120 seconds exceeded by sentry.issues.tasks.post_process.post_process_group",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.store.process_event",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.store.save_event_transaction",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.debug_files.tasks.refresh_artifact_bundles_in_use"
    ],
    "title": "Task timeouts during memcache socket operations",
    "description": "Multiple task types are exceeding their processing deadlines while attempting to connect to or communicate with memcache servers. The pymemcache client is experiencing socket connection failures and blocking operations that exceed configured task timeouts.",
    "tags": [
      "Caching",
      "Networking",
      "Resource Limits",
      "Memcached",
      "Connection Reset",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 21,
    "cluster_min_similarity": 0.9250366144330152,
    "cluster_avg_similarity": 0.9542196592632344,
    "fixability_score": 0.2211247980594635
  },
  {
    "cluster_id": 202,
    "project_ids": [],
    "group_ids": [6677440517, 6794413207, 6844267979],
    "issue_titles": ["NotificationClassNotSetException"],
    "title": "Notification class not registered in task worker",
    "description": "Task worker processes fail to send notifications because notification classes are not imported during worker initialization, preventing the @register() decorators from executing and registering the classes.",
    "tags": ["Configuration", "Queueing", "Multiprocessing", "Missing Registration"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9604949169816486,
    "cluster_avg_similarity": 0.9644519240250854,
    "fixability_score": 0.5921037793159485
  },
  {
    "cluster_id": 203,
    "project_ids": [],
    "group_ids": [6677784134, 6708941920, 7017527232],
    "issue_titles": ["ValueError: not enough values to unpack (expected 2, got 1)"],
    "title": "GitHub integration fails on malformed external issue keys",
    "description": "External issue records exist with malformed keys (e.g., '3870' instead of 'repo#issue_id'), causing the GitHub integration to fail when parsing issue URLs during group serialization.",
    "tags": ["External System", "Data Integrity", "API", "GitHub", "Input Validation"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.959934978901326,
    "cluster_avg_similarity": 0.9634591379710807,
    "fixability_score": 0.6791635155677795
  },
  {
    "cluster_id": 207,
    "project_ids": [],
    "group_ids": [6678435494, 6689961421, 6689961440, 6793345628, 6901433528, 6923750134],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='Cannot query apdex with a threshold parameter on the metrics dataset', code='parse_error')}",
      "SubscriptionError: Cannot query apdex with a threshold parameter on the metrics dataset",
      "ApiError: status=400 body={'detail': ErrorDetail(string='app_name is not a tag in the metrics dataset', code='parse_error')}"
    ],
    "title": "Apdex with threshold incompatible with metrics dataset",
    "description": "Alert rules using apdex(300) aggregate fail when queried against the metrics dataset during chart rendering, as the metrics dataset only supports unparameterized apdex functions.",
    "tags": [
      "Data Integrity",
      "Input Validation",
      "Configuration",
      "Incompatible Query",
      "Apdex",
      "Metrics Dataset"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9297636058836002,
    "cluster_avg_similarity": 0.9570255412594808,
    "fixability_score": 0.47052839398384094
  },
  {
    "cluster_id": 208,
    "project_ids": [],
    "group_ids": [
      6678435496, 6678435554, 6678537705, 6678537737, 6689961427, 6689961447, 6689961448,
      6746480180, 6746480185, 6746480186, 6746480190, 6775280372, 6775316673, 6775384336,
      6775486526, 6778300806, 6783126514, 6793345667, 6793375009, 6798177485, 6799524941,
      6805712233, 6811977041, 6825413890, 6825426177, 6828916841, 6830365844, 6849762058
    ],
    "issue_titles": [
      "SubscriptionError: http.url is not a tag in the metrics dataset",
      "SubscriptionError: sdk.name is not a tag in the metrics dataset",
      "SubscriptionError: se is not a tag in the metrics dataset",
      "SubscriptionError: user.display is not a tag in the metrics dataset",
      "SubscriptionError: url is not a tag in the metrics dataset",
      "SubscriptionError: transaction.duration is not a tag in the metrics dataset",
      "SubscriptionError: customerType is not a tag in the metrics dataset",
      "IncompatibleMetricsQuery: SITE_DOMAIN is not a tag in the metrics dataset"
    ],
    "title": "Metrics query builder rejecting valid tags during deletion",
    "description": "Alert subscription deletions fail because MetricsQueryBuilder.resolve_tag_key() validates tags against an incomplete DEFAULT_METRIC_TAGS set when the mep-use-default-tags feature flag is enabled, rejecting valid tags like user.display, http.url, and transaction.duration.",
    "tags": ["API", "Configuration", "Input Validation", "Incompatible Query", "Snuba"],
    "cluster_size": 28,
    "cluster_min_similarity": 0.9279692825957887,
    "cluster_avg_similarity": 0.9725628964234607,
    "fixability_score": 0.6731545329093933
  },
  {
    "cluster_id": 209,
    "project_ids": [],
    "group_ids": [6678435499, 6678637607, 6689961437, 6792301323],
    "issue_titles": [
      "SubscriptionError: Metric: c:custom/business_successfully_fetched@none could not be resolved",
      "SubscriptionError: Metric: c:custom/checkout.failed@none could not be resolved"
    ],
    "title": "Alert metric resolution fails in query builder",
    "description": "The AlertMetricsQueryBuilder is unable to resolve metric columns during query construction, raising IncompatibleMetricsQuery exceptions when processing Snuba subscriptions.",
    "tags": ["API", "Input Validation", "Metrics Resolution", "Query Builder"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.952295813001491,
    "cluster_avg_similarity": 0.9660842840834921,
    "fixability_score": 0.3462689518928528
  },
  {
    "cluster_id": 210,
    "project_ids": [],
    "group_ids": [6678435501, 6678570727, 6678570748, 6793537779],
    "issue_titles": [
      "SubscriptionError: release value com.example.vu.android@2.10.4+43 in filter not found",
      "SubscriptionError: release value application.monitoring.javascript@22.5.5 in filter not found",
      "SubscriptionError: release value application.monitoring.javascript@22.2.1 in filter not found"
    ],
    "title": "IncompatibleMetricsQuery: filter value not found",
    "description": "AlertMetricsQueryBuilder is failing during release filter conversion when a filter value cannot be found in the metrics dataset, causing task worker failures.",
    "tags": ["API", "Input Validation", "Metrics Query", "Incompatible Filter Value"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.962982319692262,
    "cluster_avg_similarity": 0.9685864623672503,
    "fixability_score": 0.5326046347618103
  },
  {
    "cluster_id": 211,
    "project_ids": [],
    "group_ids": [6678511610, 6725560157, 6791455908],
    "issue_titles": ["SentryAppSentryError: event_not_in_servicehook"],
    "title": "SentryApp webhook fails for unsubscribed issue events",
    "description": "Workflow notification tasks are being queued for Sentry Apps that haven't subscribed to specific issue events like issue.unresolved, causing validation failures when the webhook is sent.",
    "tags": ["API", "External System", "Configuration", "Event Not In Servicehook"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.964850641085982,
    "cluster_avg_similarity": 0.9690766779721353,
    "fixability_score": 0.44155406951904297
  },
  {
    "cluster_id": 212,
    "project_ids": [],
    "group_ids": [6678537740, 6678537779, 6678637575, 6678637623, 6797847220, 6798177476],
    "issue_titles": [
      "SubscriptionError: Environment: PRD was not found",
      "SubscriptionError: Environment: Production was not found"
    ],
    "title": "AlertMetricsQueryBuilder fails on unknown environment",
    "description": "The metrics query builder is raising IncompatibleMetricsQuery exceptions when attempting to resolve environment filter values that don't exist in the system.",
    "tags": [
      "Input Validation",
      "Configuration",
      "Metrics Query",
      "Environment Not Found"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9661516574513718,
    "cluster_avg_similarity": 0.9775642289571184,
    "fixability_score": 0.5191752314567566
  },
  {
    "cluster_id": 213,
    "project_ids": [],
    "group_ids": [6678537766, 6793468098, 6797823141],
    "issue_titles": [
      "SubscriptionError: Invalid query. Project(s) application-monitoring-springboot do not exist or are not actively selected."
    ],
    "title": "InvalidSearchQuery in project slug filter conversion",
    "description": "Search query processing fails when converting project slug filters, causing task worker exceptions during Snuba subscription creation and deletion operations.",
    "tags": [
      "Input Validation",
      "API",
      "Search Query Processing",
      "Invalid Search Query"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9550721553337389,
    "cluster_avg_similarity": 0.9636389743733886,
    "fixability_score": 0.32647064328193665
  },
  {
    "cluster_id": 214,
    "project_ids": [],
    "group_ids": [
      6678565556, 6684972392, 6721903869, 6784775663, 6788281996, 6796039249, 6800377295,
      6802471422, 6806805534, 6833681577, 6936628438, 6985072459, 6986437163
    ],
    "issue_titles": [
      "OperationalError: canceling statement due to statement timeout",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.auto_resolve_project_issues",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by getsentry.tasks.stats.sync_outdated",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.process_suspect_commits",
      "OperationalError: canceling statement due to user request"
    ],
    "title": "Task worker queries timeout with processing deadlines",
    "description": "Database queries in background tasks are being canceled due to processing deadlines being exceeded, affecting both task worker operations and API requests during query execution.",
    "tags": [
      "Database",
      "Resource Limits",
      "PostgreSQL",
      "Timeout",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 13,
    "cluster_min_similarity": 0.9256948254164665,
    "cluster_avg_similarity": 0.9521710767396186,
    "fixability_score": 0.24345864355564117
  },
  {
    "cluster_id": 215,
    "project_ids": [],
    "group_ids": [6678740537, 6691278425, 6733309572],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.tasks.statistical_detectors.detect_function_trends",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.tasks.statistical_detectors.detect_transaction_trends",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.statistical_detectors.detect_transaction_change_points"
    ],
    "title": "Statistical detector task exceeds processing deadline",
    "description": "The detect_transaction_change_points task lacks a processing_deadline_duration setting and defaults to 10 seconds, which is insufficient for complex Snuba queries that analyze 14 days of metrics data.",
    "tags": [
      "Configuration",
      "Resource Limits",
      "External System",
      "Processing Deadline Exceeded",
      "Snuba"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9417162182379052,
    "cluster_avg_similarity": 0.9531890530650676,
    "fixability_score": 0.21357190608978271
  },
  {
    "cluster_id": 216,
    "project_ids": [],
    "group_ids": [
      6678805052, 6680339419, 6694215313, 6709185938, 6709373545, 6717413273, 6725735955,
      6732089288, 6736424508, 6738193717, 6799141727, 6806749607, 6830730446, 7004758163
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by getsentry.tasks.quotas.recalculate_projected_spikes",
      "ProcessingDeadlineExceeded: execution deadline of 120 seconds exceeded by sentry.tasks.weekly_escalating_forecast.generate_forecasts_for_projects",
      "ProcessingDeadlineExceeded: execution deadline of 150 seconds exceeded by sentry.integrations.source_code_management.tasks.open_pr_comment_workflow",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.weekly_escalating_forecast.generate_and_save_missing_forecasts",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.autofix.trigger_autofix_from_issue_summary",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.dynamic_sampling.tasks.recalibrate_orgs",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.service_hooks.process_service_hook",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.user_report",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.check_auth",
      "ProcessingDeadlineExceeded: execution deadline of 600 seconds exceeded by sentry.tasks.summaries.weekly_reports.prepare_organization_report"
    ],
    "title": "Task worker timeouts querying Snuba during peak load",
    "description": "Multiple Sentry background tasks are timing out while making queries to Snuba, the analytics database. The tasks exceed their processing deadlines when Snuba queries take longer than expected due to query complexity, rate limiting, or system load.",
    "tags": [
      "External System",
      "Resource Limits",
      "Snuba",
      "Timeout",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 14,
    "cluster_min_similarity": 0.9173206374162798,
    "cluster_avg_similarity": 0.936089628692665,
    "fixability_score": 0.2271079123020172
  },
  {
    "cluster_id": 221,
    "project_ids": [],
    "group_ids": [6680310665, 6756304596, 6806414039],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.auto_enable_codecov.enable_for_org",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by getsentry.tasks.verify_github_subscriptions",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.process_commit_context"
    ],
    "title": "GitHub API requests timing out in task workers",
    "description": "Task workers are exceeding processing deadlines when making HTTP requests to GitHub API endpoints, causing socket read operations to hang and trigger timeout handlers.",
    "tags": [
      "External System",
      "Networking",
      "GitHub",
      "Timeout",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9578178126308419,
    "cluster_avg_similarity": 0.9633152555962671,
    "fixability_score": 0.21720871329307556
  },
  {
    "cluster_id": 228,
    "project_ids": [],
    "group_ids": [
      6684093720, 6731215162, 6792300053, 6794377386, 6796631026, 6806291306, 6869524816,
      6879888174, 6879888386, 6880968478, 6975814027, 7017568711
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by getsentry.integrations.slack.tasks.new_organization_notify",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.tasks.code_owners_auto_sync",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by getsentry.tasks.quotas.deactivate_db_spike",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.integrations.tasks.sync_status_outbound",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.workflow_engine.tasks.trigger_action",
      "HTTPError: 408 Client Error: Request Timeout for url: https://openrouter.ai/api/v1/models",
      "ProcessingDeadlineExceeded: execution deadline of 905 seconds exceeded by sentry.tasks.commits.fetch_commits",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.integrations.slack.tasks.send_activity_notifications_to_slack_threads",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.rules.processing.delayed_processing"
    ],
    "title": "Task worker timeouts on external HTTP calls",
    "description": "Multiple Sentry tasks are timing out due to external HTTP requests (Slack API, Jira API, internal silo communication) that exceed processing deadlines. The tasks make synchronous HTTP calls without appropriate timeout configuration, causing deadline exceeded errors when external services are slow or unavailable.",
    "tags": [
      "Networking",
      "External System",
      "Timeout",
      "Processing Deadline",
      "HTTP",
      "Slack",
      "Jira"
    ],
    "cluster_size": 12,
    "cluster_min_similarity": 0.9133987279109428,
    "cluster_avg_similarity": 0.9444537332526995,
    "fixability_score": 0.23997050523757935
  },
  {
    "cluster_id": 230,
    "project_ids": [],
    "group_ids": [6684541076, 6792744388, 6794669443],
    "issue_titles": [
      "UnableToAcquireLock: Unable to acquire <Lock: 'queue_comment_task:194480649'> due to error: Could not set key: 'l:queue_comment_task:194480649'",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'deploy-notify:94226451'>> within 9.937 seconds (98 attempts.)",
      "UnableToAcquireLock: Unable to acquire <Lock: 'create_invoices.subscription:2264513'> due to error: Could not set key: 'l:create_invoices.subscription:2264513'"
    ],
    "title": "Redis lock acquisition fails with RedisCluster",
    "description": "Lock acquisition fails because code checks 'is not True' but RedisCluster returns 'OK' string instead of True boolean, causing locks to fail even when Redis successfully sets the key.",
    "tags": ["Caching", "Concurrency", "Redis", "Type Mismatch"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9578071853102849,
    "cluster_avg_similarity": 0.967796258380456,
    "fixability_score": 0.23941974341869354
  },
  {
    "cluster_id": 234,
    "project_ids": [],
    "group_ids": [
      6685373553, 6793075224, 6812214998, 6812647562, 6984737196, 6992643012, 7005450537,
      7017257930
    ],
    "issue_titles": [
      "Subscription.DoesNotExist: Subscription matching query does not exist.",
      "Organization.DoesNotExist: Organization matching query does not exist."
    ],
    "title": "Subscription lookup fails for organization in task worker",
    "description": "Task worker processes are failing when attempting to retrieve Subscription records for organizations that exist but lack corresponding subscription data, likely due to data consistency issues or replica lag between primary and replica databases.",
    "tags": [
      "Database",
      "Data Integrity",
      "Task Processing",
      "Subscription",
      "Django ORM",
      "DoesNotExist"
    ],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9279275880769414,
    "cluster_avg_similarity": 0.9507147601263585,
    "fixability_score": 0.3818122446537018
  },
  {
    "cluster_id": 237,
    "project_ids": [],
    "group_ids": [
      6688775092, 6707917945, 6710958153, 6755570545, 6837751626, 6841632111, 6870267192,
      6959355998
    ],
    "issue_titles": ["DecodeError: Error parsing message"],
    "title": "Snuba RPC error response parsing failure",
    "description": "API calls to Snuba RPC are failing when the service returns non-200/202 HTTP responses with non-protobuf formatted error content, causing protobuf parsing errors in the error handling code.",
    "tags": ["API", "External System", "Serialization", "Snuba", "Parse Error"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9307478196286729,
    "cluster_avg_similarity": 0.959003507033109,
    "fixability_score": 0.42296895384788513
  },
  {
    "cluster_id": 239,
    "project_ids": [],
    "group_ids": [6689961415, 6748982357],
    "issue_titles": [
      "IncompatibleMetricsQuery: SITE_DOMAIN is not a tag in the metrics dataset",
      "SubscriptionError: se is not a tag in the metrics dataset"
    ],
    "title": "Metrics query builder rejects non-default tags",
    "description": "Alert subscription queries fail when organizations:mep-use-default-tags feature flag is enabled and queries contain tags not in the DEFAULT_METRIC_TAGS set, causing IncompatibleMetricsQuery exceptions during subscription processing.",
    "tags": ["API", "Configuration", "Metrics", "IncompatibleMetricsQuery", "Sentry"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9740524372178958,
    "cluster_avg_similarity": 0.9740524372178958,
    "fixability_score": 0.615878164768219
  },
  {
    "cluster_id": 243,
    "project_ids": [],
    "group_ids": [
      6692799304, 6693305837, 6707727283, 6707727284, 6726430752, 6726976190, 6738867332,
      6792401517, 6792749793, 6793260810, 6793367935, 6794254059, 6821521843, 6854937591,
      6856252842, 6872714672, 6873833726, 6878201777, 6879879123, 6886280330, 6964501754,
      6977220303, 7010399757, 7023469264
    ],
    "issue_titles": [
      "Project.DoesNotExist: Project matching query does not exist.",
      "NoRetriesRemainingError: sentry.tasks.code_owners_auto_sync has consumed all of its retries",
      "File.DoesNotExist: File matching query does not exist.",
      "ExternalIssue.DoesNotExist: ExternalIssue matching query does not exist.",
      "Release.DoesNotExist: Release matching query does not exist.",
      "Action.DoesNotExist: Action matching query does not exist.",
      "Group.DoesNotExist: Group matching query does not exist.",
      "Organization.DoesNotExist: Organization matching query does not exist.",
      "SentryAppSentryError: missing_installation",
      "Commit.DoesNotExist: Commit matching query does not exist.",
      "SubscriptionError: An unexpected database issue occurred while charging this invoice"
    ],
    "title": "Task worker failures from missing database records",
    "description": "Multiple task workers are encountering DoesNotExist exceptions when attempting to retrieve database objects that may have been deleted or merged after the tasks were queued, indicating race conditions between task execution and data lifecycle operations.",
    "tags": ["Database", "Concurrency", "Task Queue", "Django", "Data Integrity"],
    "cluster_size": 24,
    "cluster_min_similarity": 0.9091422122652629,
    "cluster_avg_similarity": 0.9473059332491184,
    "fixability_score": 0.5579190850257874
  },
  {
    "cluster_id": 245,
    "project_ids": [],
    "group_ids": [6693754863, 6721729431],
    "issue_titles": [
      "GitHubApiError: b'{\\r\\n  \"message\": \"Bad credentials\",\\r\\n  \"documentation_url\": \"https://docs.github.com/rest\",\\r\\n  \"status\": \"401\"\\r\\n}'"
    ],
    "title": "GitHub OAuth authentication API failures",
    "description": "GitHub API requests during OAuth authentication pipeline are failing, preventing users from completing login through GitHub provider.",
    "tags": ["External System", "Authentication", "API", "GitHub"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9742857012665916,
    "cluster_avg_similarity": 0.9742857012665916,
    "fixability_score": 0.26475274562835693
  },
  {
    "cluster_id": 247,
    "project_ids": [],
    "group_ids": [6696655082, 6696659077],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 300 seconds exceeded by sentry.hybridcloud.tasks.deliver_webhooks.drain_mailbox",
      "ProcessingDeadlineExceeded: execution deadline of 180 seconds exceeded by sentry.hybridcloud.tasks.deliver_webhooks.drain_mailbox_parallel"
    ],
    "title": "Task worker processing deadline exceeded",
    "description": "Task worker processes are hitting processing deadlines while executing webhook delivery tasks, causing them to be terminated mid-execution.",
    "tags": ["Queueing", "Resource Limits", "Timeout", "Task Worker", "Webhook Delivery"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9614756963122989,
    "cluster_avg_similarity": 0.9614756963122989,
    "fixability_score": 0.21548084914684296
  },
  {
    "cluster_id": 252,
    "project_ids": [],
    "group_ids": [6698006459, 6698197464, 6937547120],
    "issue_titles": [
      "Exception: HTTP 400 (admin_policy_enforced): Access to your account data is restricted by policies within your organization. Please contact the administrator of your organization for more information."
    ],
    "title": "OAuth2 identity refresh failing in auth check task",
    "description": "The check_auth_identities background task is consistently failing when attempting to refresh OAuth2 authentication identities, with exceptions being raised in the OAuth2 provider's refresh_identity method.",
    "tags": ["Authentication", "External System", "OAuth2", "Task Worker"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9878317286467284,
    "cluster_avg_similarity": 0.9890037249519438,
    "fixability_score": 0.2542259395122528
  },
  {
    "cluster_id": 254,
    "project_ids": [],
    "group_ids": [6698211440, 6907395781],
    "issue_titles": ["Exception: Same primary email address for multiple users"],
    "title": "Identity login fails: duplicate primary emails",
    "description": "OAuth identity linking fails when multiple users have the same primary email address due to case-sensitivity mismatch between database query and Python comparison.",
    "tags": ["Authentication", "Data Integrity", "OAuth", "Django", "Case Sensitivity"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9718863704295906,
    "cluster_avg_similarity": 0.9718863704295906,
    "fixability_score": 0.3956395387649536
  },
  {
    "cluster_id": 256,
    "project_ids": [],
    "group_ids": [
      6702972843, 6703015521, 6703065135, 6703445929, 6704011338, 6704838350, 6723603645
    ],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, API_TOKEN_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORGANIZATION_INTEGRATION_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORG_AUTH_TOKEN_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORGAUTHTOKEN_UPDATE_USED due to database error",
      "OutboxDatabaseError: Failed to process Outbox, AUDIT_LOG_EVENT due to database error",
      "OutboxDatabaseError: Failed to process Outbox, SUBSCRIPTION_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, USER_IP_EVENT due to database error"
    ],
    "title": "Outbox processing task failures in worker processes",
    "description": "Task workers processing outbox batches are encountering database execution errors during the drain_outbox_shards operations, causing exceptions to be captured and reported.",
    "tags": ["Queueing", "Database", "PostgreSQL", "Task Worker"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9449834736922347,
    "cluster_avg_similarity": 0.9617365393681305,
    "fixability_score": 0.22145818173885345
  },
  {
    "cluster_id": 258,
    "project_ids": [],
    "group_ids": [6703083452, 6704778470, 6750099076],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, AUTH_IDENTITY_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORGANIZATION_MEMBER_TEAM_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, PROVISION_ORGANIZATION due to database error"
    ],
    "title": "Django exception handling triggering database errors",
    "description": "Exceptions in Django request processing are causing secondary database errors during exception handling and logging, particularly in PostgreSQL operations within Sentry middleware stack.",
    "tags": ["API", "Database", "Django", "PostgreSQL", "Exception Handling"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9559509708544273,
    "cluster_avg_similarity": 0.960426392130827,
    "fixability_score": 0.22928771376609802
  },
  {
    "cluster_id": 267,
    "project_ids": [],
    "group_ids": [6705064103, 6754548453, 6758268352],
    "issue_titles": [
      "AttributeError: 'NoneType' object has no attribute 'id'",
      "AttributeError: 'NoneType' object has no attribute 'is_free_plan'"
    ],
    "title": "Seer feature handler crashes on missing subscription",
    "description": "SeerAddedFeatureHandler fails to check for null subscription before passing it to BillingHistory.get_current(), causing AttributeError when organization has no subscription record.",
    "tags": ["API", "Input Validation", "Billing", "Feature Flags", "Django"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.946851804764423,
    "cluster_avg_similarity": 0.9556116762666572,
    "fixability_score": 0.7244406342506409
  },
  {
    "cluster_id": 268,
    "project_ids": [],
    "group_ids": [6705077758, 6725840749, 6801175178, 6824532158, 6834481033, 7022579794],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification"
    ],
    "title": "Workflow notification task timeout from complex serialization",
    "description": "The workflow_notification task is timing out with its default 10-second deadline while serializing Group objects, which triggers multiple expensive database queries and RPC calls for assignee lookup, annotations, and statistics that exceed the processing deadline.",
    "tags": ["Queueing", "Serialization", "Database", "Timeout", "Django", "PostgreSQL"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.943151053978385,
    "cluster_avg_similarity": 0.9604593597525757,
    "fixability_score": 0.22412970662117004
  },
  {
    "cluster_id": 269,
    "project_ids": [],
    "group_ids": [
      6705269507, 6791911603, 6808438570, 6808438571, 6808438599, 6808438630, 6808438637,
      6899646185, 6899646239, 6927089687, 7021095593
    ],
    "issue_titles": ["OperationalError: server closed the connection unexpectedly"],
    "title": "Django PostgreSQL reconnection fails on unexpected closure",
    "description": "Database queries fail when PostgreSQL unexpectedly closes connections due to broken exception handling in the auto-reconnect cursor decorator, preventing proper retry attempts.",
    "tags": ["Database", "API", "PostgreSQL", "Django", "Connection Reset"],
    "cluster_size": 11,
    "cluster_min_similarity": 0.9327956967783599,
    "cluster_avg_similarity": 0.9576516554658113,
    "fixability_score": 0.24781382083892822
  },
  {
    "cluster_id": 270,
    "project_ids": [],
    "group_ids": [
      6705342006, 6726098209, 6732056221, 6736162416, 6795623703, 6815838947, 6959131688,
      6959753217, 7000729724, 7013648445
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"
    ],
    "title": "Sentry App webhook tasks timeout on slow external endpoints",
    "description": "Webhook delivery tasks are timing out due to inadequate processing deadlines when external webhook endpoints are slow or unreachable. Tasks perform extensive database operations and serialization before making HTTP requests, consuming most of their allocated time.",
    "tags": [
      "External System",
      "Resource Limits",
      "Networking",
      "Timeout",
      "Sentry Apps",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9403142803714782,
    "cluster_avg_similarity": 0.9593847216937428,
    "fixability_score": 0.21654054522514343
  },
  {
    "cluster_id": 272,
    "project_ids": [],
    "group_ids": [6706122053, 6982119219, 6996385989],
    "issue_titles": ["Project.DoesNotExist: Project matching query does not exist."],
    "title": "SDK crash detection project not found in database",
    "description": "SDK crash monitoring is configured to report detected crashes to project ID 4505469596663808, but this project doesn't exist in the current database/region, causing post-processing failures.",
    "tags": ["Configuration", "Database", "SDK Crash Detection", "Project Not Found"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9475854339541031,
    "cluster_avg_similarity": 0.9553299777370355,
    "fixability_score": 0.4024760127067566
  },
  {
    "cluster_id": 273,
    "project_ids": [],
    "group_ids": [6706758580, 6881015062, 6897372272],
    "issue_titles": [
      "Environment.DoesNotExist: Environment matching query does not exist."
    ],
    "title": "Environment lookup failure in post-processing",
    "description": "Post-processing pipeline fails when trying to link events to user reports because environment records with empty names don't exist in the database, while the lookup expects them to be present.",
    "tags": ["Database", "Data Integrity", "Django", "Does Not Exist"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9508653502711645,
    "cluster_avg_similarity": 0.9645864871350426,
    "fixability_score": 0.5401941537857056
  },
  {
    "cluster_id": 275,
    "project_ids": [],
    "group_ids": [6708210396, 6793812861, 6915293682],
    "issue_titles": ["SnubaRPCError: code: 500"],
    "title": "Snuba RPC calls failing across multiple endpoints",
    "description": "Multiple API endpoints are experiencing failures when making RPC calls to Snuba, affecting trace queries, attribute lookups, and event data retrieval.",
    "tags": ["External System", "API", "RPC", "Snuba", "SnubaRPCError"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9497410576827393,
    "cluster_avg_similarity": 0.9597001141354754,
    "fixability_score": 0.5533650517463684
  },
  {
    "cluster_id": 278,
    "project_ids": [],
    "group_ids": [6709793450, 6709846241, 6909441101],
    "issue_titles": [
      "ApiError: status=403 body=The user does not have access to the organization."
    ],
    "title": "Slack user access fails for org member updates",
    "description": "Slack integration webhook actions are failing with API errors when the linked Sentry user lacks proper organization membership or the identity mapping is inconsistent with current access permissions.",
    "tags": ["External System", "Authorization", "API", "Slack", "Identity Linking"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9492777604245028,
    "cluster_avg_similarity": 0.9595300812390394,
    "fixability_score": 0.2981165647506714
  },
  {
    "cluster_id": 280,
    "project_ids": [],
    "group_ids": [6711114721, 6746118296, 6990914635],
    "issue_titles": [
      "IntegrityError: update or delete on table \"sentry_monitorcheckin\" violates foreign key constraint \"sentry_monitorincide_resolving_checkin_id_9b2daf6a_fk_sentry_mo\" on table \"sentry_monitorincident\"",
      "IntegrityError: update or delete on table \"sentry_incident\" violates foreign key constraint \"incident_id_refs_id_74273623\" on table \"sentry_incidentseen\"",
      "IntegrityError: update or delete on table \"sentry_artifactbundle\" violates foreign key constraint \"sentry_artifactbundl_artifact_bundle_id_8279332c_fk_sentry_ar\" on table \"sentry_artifactbundleindex\""
    ],
    "title": "Database commit failures in multiprocess task workers",
    "description": "Multiple background task workers (deletion, artifact assembly, cleanup) are failing during database transaction commits, indicating potential database connectivity or transaction management issues.",
    "tags": ["Database", "Concurrency", "Multiprocessing", "Transaction Commit"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9562998632222675,
    "cluster_avg_similarity": 0.9578031692111132,
    "fixability_score": 0.38961106538772583
  },
  {
    "cluster_id": 281,
    "project_ids": [],
    "group_ids": [6711283984, 6889458880, 6889470540, 6921313902],
    "issue_titles": ["IntegrationConfigurationError: Identity not found."],
    "title": "GitLab integration identity missing during commit context",
    "description": "The GitLab integration's default_auth_id references an Identity record that no longer exists in the database, preventing commit context processing and stacktrace linking from functioning properly.",
    "tags": [
      "External System",
      "Authentication",
      "Data Integrity",
      "GitLab",
      "Identity DoesNotExist"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9523117416858041,
    "cluster_avg_similarity": 0.9612763532401519,
    "fixability_score": 0.45799070596694946
  },
  {
    "cluster_id": 282,
    "project_ids": [],
    "group_ids": [
      6711717462, 6712227035, 6745616372, 6753978566, 6784479206, 6799173375, 6800297377,
      6820754142, 6856077790
    ],
    "issue_titles": [
      "ApiInvalidRequestError: {\"message\": \"Invalid Form Body\", \"code\": 50035, \"errors\": {\"embeds\": {\"0\": {\"description\": {\"_errors\": [{\"code\": \"BASE_TYPE_MAX_LENGTH\", \"message\": \"Must be 4096 or fewer in length.\"}]}}}}}",
      "ApiInvalidRequestError: {\"message\": \"Invalid Form Body\", \"code\": 50035, \"errors\": {\"embeds\": {\"_errors\": [{\"code\": \"MAX_EMBED_SIZE_EXCEEDED\", \"message\": \"Embed size exceeds maximum size of 6000\"}]}}}"
    ],
    "title": "Discord API rejects oversized embed descriptions",
    "description": "Discord notifications fail when issue evidence (like SQL queries) exceeds Discord's 4096 character limit for embed descriptions. The message builder lacks validation to truncate long content before sending.",
    "tags": ["External System", "API", "Input Validation", "Discord", "HTTP Error"],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9508380131706711,
    "cluster_avg_similarity": 0.9668341522950092,
    "fixability_score": 0.24754901230335236
  },
  {
    "cluster_id": 288,
    "project_ids": [],
    "group_ids": [6713621492, 6714288680, 6725845377],
    "issue_titles": [
      "NoRetriesRemainingError: sentry.tasks.summaries.weekly_reports.prepare_organization_report has consumed all of its retries",
      "NoRetriesRemainingError: sentry.tasks.merge.merge_groups has consumed all of its retries",
      "NoRetriesRemainingError: sentry.tasks.weekly_escalating_forecast.generate_forecasts_for_projects has consumed all of its retries"
    ],
    "title": "Task worker database operation failures",
    "description": "Sentry task worker child processes are encountering database errors during SQL execution and transaction commits, leading to retry exceptions being captured.",
    "tags": ["Database", "Queueing", "PostgreSQL", "Django"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9532272469022233,
    "cluster_avg_similarity": 0.9621313796170655,
    "fixability_score": 0.22464601695537567
  },
  {
    "cluster_id": 291,
    "project_ids": [],
    "group_ids": [6714198279, 7009473332, 7009542281, 7010182441],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth state validation fails during callback processing",
    "description": "OAuth callback flow experiences state mismatch errors when validating stored state tokens, often triggered by user cancellation or session expiration during the authentication process.",
    "tags": [
      "Authentication",
      "External System",
      "API",
      "OAuth State Mismatch",
      "Redis",
      "Azure DevOps"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9415124946511538,
    "cluster_avg_similarity": 0.957821446934732,
    "fixability_score": 0.3263169527053833
  },
  {
    "cluster_id": 293,
    "project_ids": [],
    "group_ids": [6716981703, 6793866834, 6999322093],
    "issue_titles": [
      "ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}",
      "NoRetriesRemainingError: sentry.integrations.tasks.sync_status_outbound has consumed all of its retries"
    ],
    "title": "Jira sync fails on deleted/inaccessible issues",
    "description": "The sync_status_outbound task does not handle 404 errors from Jira API gracefully when linked issues are deleted or become inaccessible, causing unnecessary retries and failure alerts instead of halting gracefully.",
    "tags": ["External System", "API", "Jira", "HTTP 404", "Missing Error Handling"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9450306728378214,
    "cluster_avg_similarity": 0.952935831840569,
    "fixability_score": 0.2552914023399353
  },
  {
    "cluster_id": 295,
    "project_ids": [],
    "group_ids": [6717195662, 6719007322, 7001100798],
    "issue_titles": ["Exception: Seer API error: 503"],
    "title": "Seer API error in autofix issue summary generation",
    "description": "The Seer autofixability scoring endpoint is returning 503 errors when the autofixability model hasn't been loaded, causing autofix automation tasks to fail during issue summary generation.",
    "tags": ["External System", "API", "Seer", "HTTP 503"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9529153856027,
    "cluster_avg_similarity": 0.9651811618635654,
    "fixability_score": 0.23388956487178802
  },
  {
    "cluster_id": 297,
    "project_ids": [],
    "group_ids": [6718358964, 6726781123, 6783517032, 6803878240],
    "issue_titles": [
      "SentryAppSentryError: workflow_notification.missing_installation",
      "SentryAppSentryError: event_not_in_servicehook",
      "SentryAppSentryError: missing_servicehook"
    ],
    "title": "SentryApp webhook delivery fails with missing ServiceHook",
    "description": "Workflow notifications fail because ServiceHooks are missing for existing installations, likely due to race conditions during app configuration updates or cache invalidation issues in the hybrid cloud architecture.",
    "tags": [
      "External System",
      "Queueing",
      "Configuration",
      "Linear",
      "Missing ServiceHook",
      "Cache Invalidation"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9436115235658178,
    "cluster_avg_similarity": 0.9571790153704095,
    "fixability_score": 0.35650500655174255
  },
  {
    "cluster_id": 302,
    "project_ids": [],
    "group_ids": [6719866258, 6792141135],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='192.168.208.181', port=8080): Read timed out. (read timeout=5)"
    ],
    "title": "File deletion tasks fail on filestore read timeout",
    "description": "Task worker file deletion fails when filestore service takes more than 5 seconds to respond. HTTP client intentionally skips retries on timeout, but task retry policy doesn't cover ReadTimeoutError, causing permanent task failure.",
    "tags": ["External System", "Queueing", "HTTP", "Timeout", "File Storage"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9667202957692602,
    "cluster_avg_similarity": 0.9667202957692602,
    "fixability_score": 0.22106000781059265
  },
  {
    "cluster_id": 306,
    "project_ids": [],
    "group_ids": [6722086808, 6722086820],
    "issue_titles": ["TypeError: unhashable type: 'list'"],
    "title": "Device class filter validation failing in metrics query",
    "description": "Metrics performance queries are failing during device class filter conversion, likely due to an invalid or unmapped device class value being passed to the query builder.",
    "tags": ["Input Validation", "API", "Metrics Query", "Device Class Filter"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9735757948795428,
    "cluster_avg_similarity": 0.9735757948795428,
    "fixability_score": 0.7532607913017273
  },
  {
    "cluster_id": 309,
    "project_ids": [],
    "group_ids": [6723347604, 6736623029, 6794697352, 6979564718, 7014965062],
    "issue_titles": [
      "UnknownOption: 'feature.projects:triage-signals-v0'",
      "UnknownOption: 'overwatch.forward-webhooks.verbose'"
    ],
    "title": "Orphaned feature option sync fails after scope change",
    "description": "The sync_options task encounters a database option 'feature.projects:triage-signals-v0' that was moved from organization to project scope, but the old database record doesn't match any currently registered option in the options registry.",
    "tags": ["Configuration", "Database", "Queueing", "Stale Data", "Feature Flag"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.941770108339874,
    "cluster_avg_similarity": 0.9572068402651007,
    "fixability_score": 0.5242747068405151
  },
  {
    "cluster_id": 310,
    "project_ids": [],
    "group_ids": [6723571938, 6747547432, 6748936512, 6769375194, 6977999086],
    "issue_titles": [
      "ApiInvalidRequestError",
      "ApiError: {\"message\":\"404 Group Not Found\"}",
      "IntegrationError: Error Communicating with GitLab (HTTP 404): 404 Project Not Found",
      "IntegrationResourceNotFoundError"
    ],
    "title": "GitLab integration fails with 404 group not found",
    "description": "GitLab integrations are failing because the configured GitLab groups are no longer accessible via the GitLab API, likely due to deleted groups, revoked permissions, or expired OAuth tokens.",
    "tags": ["External System", "API", "GitLab", "HTTP 404", "Integration"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9238148501171215,
    "cluster_avg_similarity": 0.9504052581405382,
    "fixability_score": 0.36061206459999084
  },
  {
    "cluster_id": 313,
    "project_ids": [],
    "group_ids": [6724877345, 6724877767],
    "issue_titles": [
      "Subscription.DoesNotExist: Subscription matching query does not exist."
    ],
    "title": "Missing Subscription records for organizations",
    "description": "Organizations exist in the database but lack corresponding Subscription records, causing DoesNotExist errors when billing config endpoints try to retrieve subscription data. This indicates a data consistency issue in the provisioning process.",
    "tags": ["Database", "Data Integrity", "API", "Django", "Record Missing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9633133677635537,
    "cluster_avg_similarity": 0.9633133677635537,
    "fixability_score": 0.37084901332855225
  },
  {
    "cluster_id": 316,
    "project_ids": [],
    "group_ids": [6725335284, 6792752367],
    "issue_titles": ["JSONDecodeError: Expecting value: line 1 column 1 (char 0)"],
    "title": "Tempest task JSON parsing failures in worker processes",
    "description": "Multiple Tempest tasks (poll_tempest_crashes and fetch_latest_item_id) are failing during JSON deserialization of HTTP responses, indicating malformed or unexpected response content from the external service.",
    "tags": ["External System", "Deserialization", "Tempest", "JSON Parse Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9812122308989997,
    "cluster_avg_similarity": 0.9812122308989997,
    "fixability_score": 0.39892247319221497
  },
  {
    "cluster_id": 318,
    "project_ids": [],
    "group_ids": [
      6725487265, 6725487266, 6805674835, 6829771050, 6830365847, 6831587006, 6835044609,
      6835568510, 6835993624
    ],
    "issue_titles": [
      "InvalidSearchQuery: Choose a single environment to filter by release stage."
    ],
    "title": "Release stage filter requires single environment context",
    "description": "Subscription updates fail when queries contain release.stage filters because the old subscription query is resolved without environment context, but release stage filtering requires exactly one environment to be specified.",
    "tags": [
      "Input Validation",
      "Configuration",
      "Snuba",
      "Release Stage Filter",
      "Invalid Search Query"
    ],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9542071992317394,
    "cluster_avg_similarity": 0.97708012605583,
    "fixability_score": 0.49116599559783936
  },
  {
    "cluster_id": 322,
    "project_ids": [],
    "group_ids": [6727719216, 6792702711, 7004758065],
    "issue_titles": [
      "TimeoutException",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by getsentry.tasks.check_completed_spikes"
    ],
    "title": "Task timeout in project config generation",
    "description": "The build_project_config task is timing out during metric extraction due to N+1 database queries when accessing environment relationships without proper eager loading.",
    "tags": ["Queueing", "Database", "Timeout", "N+1 Query Pattern"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9513179794262868,
    "cluster_avg_similarity": 0.9609120953961748,
    "fixability_score": 0.21836937963962555
  },
  {
    "cluster_id": 324,
    "project_ids": [],
    "group_ids": [6728181594, 6792767328, 6800184233],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.scheduleMessage)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"
    ],
    "title": "Slack API failures in channel operations",
    "description": "Multiple Slack integration tasks are failing when calling Slack API methods including chat_scheduleMessage and chat_postMessage, resulting in SlackApiError exceptions during channel lookup and message posting operations.",
    "tags": ["External System", "API", "Slack", "SlackApiError"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9440984504587108,
    "cluster_avg_similarity": 0.9538524229024241,
    "fixability_score": 0.421697735786438
  },
  {
    "cluster_id": 325,
    "project_ids": [],
    "group_ids": [6728270121, 6805899738],
    "issue_titles": [
      "ObjectErrorUnknown: invalid MachO file",
      "ObjectErrorUnknown: invalid PE file"
    ],
    "title": "Debug file assembly failing in symbolic library",
    "description": "The assemble_dif task is failing during debug file processing when iterating through archive objects, with errors occurring in the symbolic Rust library's rustcall mechanism.",
    "tags": [
      "External System",
      "Serialization",
      "Debug File Processing",
      "Symbolic Library",
      "Rust Interop"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9616549906208202,
    "cluster_avg_similarity": 0.9616549906208202,
    "fixability_score": 0.3820214867591858
  },
  {
    "cluster_id": 328,
    "project_ids": [],
    "group_ids": [6728508401, 6800382248],
    "issue_titles": ["TypeError: 'NoneType' object cannot be interpreted as an integer"],
    "title": "Slack notification build fails in datetime processing",
    "description": "Slack notification message builder is encountering an error when processing APPROX_START_TIME context field using datetime.fromtimestamp(), preventing activity notifications from being sent to Slack.",
    "tags": ["External System", "Serialization", "Slack", "Datetime Conversion"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9849970855409761,
    "cluster_avg_similarity": 0.9849970855409761,
    "fixability_score": 0.7445865273475647
  },
  {
    "cluster_id": 329,
    "project_ids": [],
    "group_ids": [6729690402, 6839231608, 7009473564, 7010182387],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth pipeline fails due to unregistered vsts_login_new provider",
    "description": "The identity pipeline remaps 'vsts_login' to 'vsts_login_new' when vsts.social-auth-migration is enabled, but no provider is registered with that key in the identity manager, causing NotRegistered exceptions during OAuth callbacks.",
    "tags": [
      "Authentication",
      "Configuration",
      "OAuth",
      "Provider Registration",
      "Azure DevOps"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9596994861074405,
    "cluster_avg_similarity": 0.967542985400526,
    "fixability_score": 0.29786843061447144
  },
  {
    "cluster_id": 333,
    "project_ids": [],
    "group_ids": [6731076286, 6798184191],
    "issue_titles": ["OrganizationIntegrationNotFound: missing org_integration"],
    "title": "Missing org_integration in integration comment tasks",
    "description": "Integration comment creation and update tasks are failing because the required organization integration is not found when attempting to sync comments with external systems.",
    "tags": ["External System", "Configuration", "Integration", "Missing Configuration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.979021933847531,
    "cluster_avg_similarity": 0.979021933847531,
    "fixability_score": 0.3500274121761322
  },
  {
    "cluster_id": 334,
    "project_ids": [],
    "group_ids": [6731105009, 6832091408],
    "issue_titles": [
      "NoRetriesRemainingError: sentry.integrations.tasks.create_comment has consumed all of its retries",
      "NoRetriesRemainingError: sentry.tasks.integrations.update_comment has consumed all of its retries"
    ],
    "title": "Integration tasks fail with missing org_integration",
    "description": "Comment synchronization tasks for external integrations are failing because the org_integration property is unexpectedly missing, causing OrganizationIntegrationNotFound exceptions in both create and update comment workflows.",
    "tags": ["External System", "Configuration", "Integration", "Missing Dependency"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9762443000482821,
    "cluster_avg_similarity": 0.9762443000482821,
    "fixability_score": 0.2599080502986908
  },
  {
    "cluster_id": 337,
    "project_ids": [],
    "group_ids": [6734072355, 6910195059],
    "issue_titles": [
      "ApiInvalidRequestError: {\"error\":{\"code\":\"BadSyntax\",\"message\":\"Bad format of conversation ID\"}}"
    ],
    "title": "MS Teams API endpoint error during channel validation",
    "description": "MS Teams integration using incorrect Bot Framework API endpoint /v3/teams/{teamId}/conversations for channel listing, causing 'Bad format of conversation ID' errors when validating alert rule channels.",
    "tags": ["External System", "API", "Configuration", "Microsoft Teams", "Bad Request"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9654564374826723,
    "cluster_avg_similarity": 0.9654564374826723,
    "fixability_score": 0.3101234436035156
  },
  {
    "cluster_id": 340,
    "project_ids": [],
    "group_ids": [6735138068, 6808813115, 6977602427, 7017777330],
    "issue_titles": [
      "TypeError: Interface.__init__() got multiple values for argument 'self'"
    ],
    "title": "Context deserialization fails with reserved 'self' key",
    "description": "Event contexts containing a 'self' key (captured from local variables) cause Interface deserialization to fail when unpacked as keyword arguments, since 'self' conflicts with Python's reserved method parameter.",
    "tags": [
      "Serialization",
      "API",
      "Input Validation",
      "Sentry",
      "Deserialization",
      "Reserved Keywords"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.951502124640015,
    "cluster_avg_similarity": 0.9636330066257708,
    "fixability_score": 0.7340851426124573
  },
  {
    "cluster_id": 343,
    "project_ids": [],
    "group_ids": [6735587400, 6736085739],
    "issue_titles": [
      "TypeError: '<' not supported between instances of 'str' and 'float'"
    ],
    "title": "Trace sorting fails on mixed float/string comparison",
    "description": "The child_sort_key function returns incompatible data types when sorting trace events, causing comparison failures between float timestamps and string values during trace serialization.",
    "tags": ["API", "Serialization", "Data Integrity", "Type Mismatch", "Django"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9718583868143614,
    "cluster_avg_similarity": 0.9718583868143614,
    "fixability_score": 0.7483933568000793
  },
  {
    "cluster_id": 344,
    "project_ids": [],
    "group_ids": [6735896989, 6735897451, 7013483347],
    "issue_titles": [
      "ApiInvalidRequestError: {\"error\":\"invalid_grant\",\"error_description\":\"AADSTS54005: OAuth2 Authorization code was already redeemed, please retry with a new valid code or use an existing refresh token. Trace ID: 5af9116d-699a-4157-a673-04d3bda22000 Correlation ID: 8ccb9236-2c0c-...",
      "ApiInvalidRequestError: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid grant: authorization code is invalid\"}",
      "PipelineError: Failed to retrieve token from the upstream service."
    ],
    "title": "OAuth authorization code reused causing token exchange failure",
    "description": "OAuth authorization codes are being submitted multiple times due to double-clicks, browser retries, or network issues, causing providers to reject subsequent attempts since authorization codes are single-use tokens.",
    "tags": [
      "Authentication",
      "External System",
      "API",
      "OAuth",
      "Idempotency",
      "Race Condition"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9542770000351463,
    "cluster_avg_similarity": 0.9665070307152982,
    "fixability_score": 0.3319315016269684
  },
  {
    "cluster_id": 347,
    "project_ids": [],
    "group_ids": [
      6737480353, 6741533525, 6741533662, 6746194384, 6803878965, 6808644708, 6816571342,
      6915035744, 6918080031, 7013950941
    ],
    "issue_titles": ["Group.DoesNotExist: Group matching query does not exist."],
    "title": "Group lookup fails during trace serialization",
    "description": "Trace serialization attempts to fetch Group objects that don't exist in the database, causing DoesNotExist exceptions. This occurs when Snuba contains stale references to deleted or merged Groups, or when there's data inconsistency between Snuba and the PostgreSQL Group table.",
    "tags": ["Database", "Data Integrity", "API", "Django", "Does Not Exist"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9463815241275152,
    "cluster_avg_similarity": 0.9587850038770772,
    "fixability_score": 0.5825334787368774
  },
  {
    "cluster_id": 353,
    "project_ids": [],
    "group_ids": [6744811987, 6795845689],
    "issue_titles": ["DeleteAborted: delete_groups.no_groups_found"],
    "title": "Group deletion task aborted - no groups found",
    "description": "The delete_groups_for_project task is raising DeleteAborted exceptions when no groups are found to delete for a project, indicating potential timing issues or redundant deletion attempts.",
    "tags": ["Data Integrity", "Queueing", "Task Processing", "Delete Aborted"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9805811559334056,
    "cluster_avg_similarity": 0.9805811559334056,
    "fixability_score": 0.44205442070961
  },
  {
    "cluster_id": 365,
    "project_ids": [],
    "group_ids": [6751424943, 6794331610],
    "issue_titles": [
      "ApiError: {\"message\":\"Request body is not processable. Please check the errors.\",\"errors\":{\"message\":\"Message can not be empty.\"},\"took\":0.0,\"requestId\":\"d05f1c43-ca96-4bd6-97d1-e80c183d7a67\"}",
      "ApiError: {\"message\":\"Request body is not processable. Please check the errors.\",\"errors\":{\"message\":\"Message can not be empty.\"},\"took\":0.001,\"requestId\":\"b1d67713-333f-4f2a-8cea-504d6552e69d\"}"
    ],
    "title": "OpsGenie spike protection deactivation fails with 422",
    "description": "Spike protection deactivation incorrectly tries to create a new alert with only an identifier field, causing OpsGenie to reject it due to missing required message field. The code should be using the alert closure endpoint instead.",
    "tags": ["External System", "API", "Notifications", "OpsGenie", "Validation Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9596356240149151,
    "cluster_avg_similarity": 0.9596356240149151,
    "fixability_score": 0.3677184283733368
  },
  {
    "cluster_id": 372,
    "project_ids": [],
    "group_ids": [6755570178, 6837752393, 6837759595, 6870110084, 6870132866, 6945973028],
    "issue_titles": ["DecodeError: Error parsing message"],
    "title": "Snuba RPC query timeout causing protobuf parse failures",
    "description": "Snuba queries are timing out at the gateway level, returning 504 responses with HTML/text bodies instead of protobuf error messages. The RPC error handling unconditionally tries to parse all non-200 responses as protobuf, causing DecodeError when infrastructure-level timeouts occur.",
    "tags": [
      "External System",
      "API",
      "Timeout",
      "Serialization",
      "Snuba",
      "Gateway Timeout"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9463440373574701,
    "cluster_avg_similarity": 0.9679271161933684,
    "fixability_score": 0.3513004183769226
  },
  {
    "cluster_id": 378,
    "project_ids": [],
    "group_ids": [6761349855, 6909721758, 6925947524],
    "issue_titles": [
      "ValueError: AlertRuleWorkflow not found when querying for AlertRuleWorkflow",
      "IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist."
    ],
    "title": "Django model DoesNotExist from incorrect field queries",
    "description": "Workflow engine tasks are failing when querying database models using non-existent field names, causing DoesNotExist exceptions during alert notification processing.",
    "tags": [
      "Database",
      "Data Integrity",
      "Django",
      "Schema Migration",
      "Workflow Engine"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9474171953056146,
    "cluster_avg_similarity": 0.9604136125408349,
    "fixability_score": 0.5559651851654053
  },
  {
    "cluster_id": 379,
    "project_ids": [],
    "group_ids": [6761355450, 6792433102, 6792433167, 6921408379, 7005214134, 7016144644],
    "issue_titles": [
      "IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist.",
      "ValueError: IncidentGroupOpenPeriod does not exist"
    ],
    "title": "IncidentGroupOpenPeriod lookup fails in metric alert notifications",
    "description": "Metric alert notification serializers are failing because expected IncidentGroupOpenPeriod relationships don't exist, likely due to failed relationship creation during incident ingestion or missing AlertRuleDetector mappings.",
    "tags": ["Database", "Data Integrity", "Serialization", "Django", "DoesNotExist"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9393306849803982,
    "cluster_avg_similarity": 0.9618581932200907,
    "fixability_score": 0.5797672867774963
  },
  {
    "cluster_id": 382,
    "project_ids": [],
    "group_ids": [6764738347, 6933211953],
    "issue_titles": ["KeyError: 'project.slug'"],
    "title": "Missing project slug in top events query results",
    "description": "Event stats queries fail when 'project.id' is requested in groupby but the table query results don't include the expected 'project' or 'project.slug' fields needed for condition building.",
    "tags": ["API", "Data Integrity", "Snuba", "Key Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9639217886940854,
    "cluster_avg_similarity": 0.9639217886940854,
    "fixability_score": 0.7217955589294434
  },
  {
    "cluster_id": 384,
    "project_ids": [],
    "group_ids": [
      6767165452, 6804302134, 6807858695, 6810480082, 6810480105, 6905784597, 6905891548,
      6931309529, 6931314341, 6961334799
    ],
    "issue_titles": ["SnubaRPCError: code: 408"],
    "title": "Snuba RPC query failures across trace endpoints",
    "description": "Multiple trace-related API endpoints are experiencing failures when making RPC calls to Snuba for querying logs, spans, and profile data.",
    "tags": ["External System", "API", "Snuba", "RPC"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9551905580402412,
    "cluster_avg_similarity": 0.9693962961256294,
    "fixability_score": 0.5102595090866089
  },
  {
    "cluster_id": 385,
    "project_ids": [],
    "group_ids": [6767363302, 6810274582, 6883514139, 6902485333],
    "issue_titles": [
      "MissingSchema: Invalid URL '': No scheme supplied. Perhaps you meant https://?",
      "MissingSchema: Invalid URL '/sentry/issues?installationId=22974884-2be4-4c8b-a640-cc2826dd2205': No scheme supplied. Perhaps you meant https:///sentry/issues?installationId=22974884-2be4-4c8b-a640-cc2826dd2205?"
    ],
    "title": "Sentry App webhook requests fail with empty URL",
    "description": "Internal Sentry Apps with empty webhook URLs cause MissingSchema errors when attempting to make external requests for dynamic form field choices. The URL building logic fails to validate the webhook URL before constructing HTTP requests.",
    "tags": ["API", "Input Validation", "External System", "Missing Schema"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9492186241954925,
    "cluster_avg_similarity": 0.9583225251867048,
    "fixability_score": 0.5743309855461121
  },
  {
    "cluster_id": 391,
    "project_ids": [],
    "group_ids": [6775118951, 6843697728],
    "issue_titles": [
      "ValueError: Expected 1 sentry app installation for action type: sentry_app, target_identifier: f7321fdc-be18-4d4a-bc85-925091bc847d, but got 0"
    ],
    "title": "Sentry app notification action fails target validation",
    "description": "Workflow engine notification actions for Sentry apps are failing during target identifier validation, causing ValueError exceptions in the issue alert handler.",
    "tags": ["Configuration", "Input Validation", "Workflow Engine", "Sentry App"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9657477045899078,
    "cluster_avg_similarity": 0.9657477045899078,
    "fixability_score": 0.46990665793418884
  },
  {
    "cluster_id": 398,
    "project_ids": [],
    "group_ids": [6779047596, 6792254811, 6793990277, 6866950211],
    "issue_titles": [
      "RefreshError: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 503,\\n    \"message\": \"Authentication backend unavailable.\",\\n    \"status\": \"UNAVAILABLE\"\\n  }\\n}\\n')",
      "RefreshError: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 503,\\n    \"message\": \"The service is currently unavailable.\",\\n    \"status\": \"UNAVAILABLE\"\\n  }\\n}\\n')"
    ],
    "title": "GCP token refresh fails during profile symbolication",
    "description": "Profile symbolication is failing when attempting to refresh Google Cloud Platform IAM credentials for symbol source access. The GCP IAM credentials service is returning 503 errors, causing the entire symbolication process to fail without retry logic.",
    "tags": [
      "Authentication",
      "External System",
      "Google Cloud Platform",
      "Refresh Error"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9733558756136954,
    "cluster_avg_similarity": 0.9781278926155351,
    "fixability_score": 0.2503323554992676
  },
  {
    "cluster_id": 405,
    "project_ids": [],
    "group_ids": [6784499232, 6784699410],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "Django API query timeout with complex JOINs",
    "description": "Database queries are timing out due to complex JOIN operations with DISTINCT clauses across multiple tables, causing PostgreSQL to cancel statements that exceed execution time limits.",
    "tags": ["Database", "API", "PostgreSQL", "Django", "Timeout", "Query Optimization"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9558935975646562,
    "cluster_avg_similarity": 0.9558935975646562,
    "fixability_score": 0.23938700556755066
  },
  {
    "cluster_id": 406,
    "project_ids": [],
    "group_ids": [
      6784563319, 6784718347, 6784777741, 6784828208, 6785097543, 6785098003, 6785111362,
      6785768117, 6840464452, 6897808544, 7025006724
    ],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "Database error in Sentry API exception handling",
    "description": "Multiple API requests are encountering database execution failures during exception processing, affecting the error capture and reporting flow in Sentry's Django middleware stack.",
    "tags": ["Database", "API", "Django", "PostgreSQL"],
    "cluster_size": 11,
    "cluster_min_similarity": 0.9728269236837757,
    "cluster_avg_similarity": 0.9823656196403177,
    "fixability_score": 0.24173124134540558
  },
  {
    "cluster_id": 419,
    "project_ids": [],
    "group_ids": [
      6784662423, 6785409246, 6785601248, 6785669238, 6792578888, 6797684953, 6800471686,
      6945426256, 7024279245, 7024282576, 7024282626
    ],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "PostgreSQL database errors in worker processes",
    "description": "Multiple worker processes (task workers and cleanup processes) are encountering database execution failures when running PostgreSQL queries through Django ORM.",
    "tags": ["Database", "Concurrency", "PostgreSQL", "Django ORM"],
    "cluster_size": 11,
    "cluster_min_similarity": 0.9336991610280053,
    "cluster_avg_similarity": 0.9625091875578484,
    "fixability_score": 0.2576742470264435
  },
  {
    "cluster_id": 410,
    "project_ids": [],
    "group_ids": [
      6784663532, 6784729082, 6784998767, 6788870342, 6794486651, 6804434396, 6944693617
    ],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "Sentry API database query timeout in Django backend",
    "description": "Multiple identical stack traces showing Django API request processing failures ending in PostgreSQL database query execution timeouts. The errors occur during exception handling within Sentry's API base middleware, indicating database connection or query performance issues.",
    "tags": ["Database", "API", "Django", "PostgreSQL", "Timeout"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9341237489169602,
    "cluster_avg_similarity": 0.9504111570543569,
    "fixability_score": 0.2480674386024475
  },
  {
    "cluster_id": 412,
    "project_ids": [],
    "group_ids": [
      6784705408, 6785665754, 6786817754, 6794696235, 6796792406, 6796792420, 6805604414,
      6805650979, 6806782105, 6808442238, 6813362692, 6842020804, 6883978502, 6937471892
    ],
    "issue_titles": ["OperationalError: server closed the connection unexpectedly"],
    "title": "PostgreSQL connection closed in worker processes",
    "description": "Database connections to PostgreSQL replicas are being unexpectedly closed by the server during query execution in multiprocessing taskworker child processes, particularly affecting billing usage calculations.",
    "tags": [
      "Database",
      "Concurrency",
      "PostgreSQL",
      "Connection Reset",
      "Multiprocessing"
    ],
    "cluster_size": 14,
    "cluster_min_similarity": 0.9513825262113867,
    "cluster_avg_similarity": 0.9749972995628229,
    "fixability_score": 0.26726025342941284
  },
  {
    "cluster_id": 416,
    "project_ids": [],
    "group_ids": [6785082756, 6797141280, 6952385461, 6995480460, 7004710759],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"sentry_commit_repository_id_key_7f948336_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_commit_repository_id_2d25b4d8949fca93_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_preprodartifactsi_organization_id_head_siz_ee2086a2_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_projectcodeowners_repository_project_path__1864c01a_uniq\"",
      "IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist."
    ],
    "title": "Django model DoesNotExist in get_or_create patterns",
    "description": "Multiple stacktraces showing Django ORM DoesNotExist exceptions occurring during database operations, primarily in multiprocessing worker contexts and API request handling.",
    "tags": ["Database", "Concurrency", "Django", "DoesNotExist"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9474383641962135,
    "cluster_avg_similarity": 0.957229227964626,
    "fixability_score": 0.4386817216873169
  },
  {
    "cluster_id": 420,
    "project_ids": [],
    "group_ids": [
      6785427948, 6792940177, 6814735066, 6816419490, 6859339382, 7006618171, 7010064041
    ],
    "issue_titles": ["OperationalError: deadlock detected"],
    "title": "Database deadlock in billing metric history updates",
    "description": "Concurrent subscription processing tasks are attempting to update the same billing metric history records without proper lock ordering, resulting in circular wait conditions and PostgreSQL deadlocks.",
    "tags": ["Database", "Concurrency", "PostgreSQL", "Deadlock"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9228993051633619,
    "cluster_avg_similarity": 0.9511801277287427,
    "fixability_score": 0.2697044014930725
  },
  {
    "cluster_id": 423,
    "project_ids": [],
    "group_ids": [
      6788281975, 6802471392, 6802471399, 6802471409, 6820659301, 6878624669, 7015573042
    ],
    "issue_titles": ["OperationalError: canceling statement due to statement timeout"],
    "title": "PostgreSQL timeout in organization events query",
    "description": "Database queries are timing out when processing organization events API requests, particularly when filtering by 'release:latest' which triggers expensive window function operations across multiple projects and releases.",
    "tags": ["Database", "API", "PostgreSQL", "Timeout", "Query Performance"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9526143030564103,
    "cluster_avg_similarity": 0.9650486561522665,
    "fixability_score": 0.23191478848457336
  },
  {
    "cluster_id": 426,
    "project_ids": [],
    "group_ids": [6789123542, 6817439707, 6819272625],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"auth_user_username_key\""
    ],
    "title": "Race condition in user registration form validation",
    "description": "Concurrent registration requests bypass form uniqueness validation and fail at database level due to lack of atomicity between username existence check and user creation.",
    "tags": [
      "Concurrency",
      "Database",
      "Input Validation",
      "Django",
      "Race Condition",
      "Constraint Violation"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9518925490205588,
    "cluster_avg_similarity": 0.9655801020252929,
    "fixability_score": 0.4182223379611969
  },
  {
    "cluster_id": 427,
    "project_ids": [],
    "group_ids": [6789304594, 6791435859],
    "issue_titles": ["TypeError: Object of type datetime is not JSON serializable"],
    "title": "JSON serialization failure in PostgreSQL adapter",
    "description": "Django's PostgreSQL backend is attempting to serialize an object to JSON that contains non-serializable data types, causing the JSON encoder to raise a TypeError.",
    "tags": ["Database", "Serialization", "PostgreSQL", "Django", "JSON", "Type Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9651167734941558,
    "cluster_avg_similarity": 0.9651167734941558,
    "fixability_score": 0.6548852324485779
  },
  {
    "cluster_id": 436,
    "project_ids": [],
    "group_ids": [6792831416, 6953959701, 6978590335],
    "issue_titles": [
      "BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentry-replays/o?uploadType=multipart: <!DOCTYPE html>",
      "BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: <!DOCTYPE html>",
      "BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles-de/o?uploadType=multipart: <!DOCTYPE html>"
    ],
    "title": "GCS upload fails - BadGateway not in retry list",
    "description": "Profile and replay uploads to Google Cloud Storage are failing on HTTP 502 errors because BadGateway exceptions are not included in GCS_RETRYABLE_ERRORS, preventing proper retry handling.",
    "tags": [
      "External System",
      "Configuration",
      "Google Cloud Storage",
      "BadGateway",
      "Retries Exhausted"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9645721720371905,
    "cluster_avg_similarity": 0.9684868085397574,
    "fixability_score": 0.2422308772802353
  },
  {
    "cluster_id": 438,
    "project_ids": [],
    "group_ids": [6792981820, 6805727503, 6808030692],
    "issue_titles": [
      "UnsupportedResponseType: text/html; charset=utf-8",
      "ValueError: Not a valid response type: <!DOCTYPE html>"
    ],
    "title": "GitLab API returning HTML instead of JSON for blame requests",
    "description": "GitLab blame API endpoints are returning HTML content with HTTP 200 status instead of expected JSON, causing response parsing to fail when processing commit context for static files.",
    "tags": ["External System", "API", "Serialization", "GitLab", "Content Negotiation"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9569008471252504,
    "cluster_avg_similarity": 0.9673737399112657,
    "fixability_score": 0.5304184556007385
  },
  {
    "cluster_id": 439,
    "project_ids": [],
    "group_ids": [6793049827, 7020778160],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"accounts_customer_organization_id_key\"",
      "IntegrityError: duplicate key value violates unique constraint \"accounts_thirdpartyaccount_type_4e5db8d20cf5f9f5_uniq\""
    ],
    "title": "Organization provisioning failures in outbox processing",
    "description": "Worker processes are encountering errors during organization provisioning tasks while draining hybrid cloud outbox shards, affecting partner account setup and subscription processing.",
    "tags": ["Queueing", "Database", "Provisioning", "Outbox Processing", "PostgreSQL"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9636280743948088,
    "cluster_avg_similarity": 0.9636280743948088,
    "fixability_score": 0.3178182542324066
  },
  {
    "cluster_id": 443,
    "project_ids": [],
    "group_ids": [6793315486, 6794582189],
    "issue_titles": [
      "ApiRetryError: SafeHTTPConnectionPool(host='sentry-rpc-prod-control.us.sentry.internal', port=8999): Max retries exceeded with url: /api/0/internal/integration-proxy/ (Caused by ResponseError('too many 503 error responses'))",
      "ApiRetryError: SafeHTTPConnectionPool(host='10.2.0.67', port=8999): Max retries exceeded with url: /extensions/jira/issue/CX-5750/?xdm_e=https%3A%2F%2Fdomainpartners.atlassian.net&xdm_c=channel-sentry.io.jira__sentry-issues-glance-4387276911594019194&cp=&xdm_deprecate..."
    ],
    "title": "Region silo consistently returning 503 errors",
    "description": "The region silo at 10.2.0.67:8999 persistently returned 503 Service Unavailable errors across 5+ retry attempts, causing the cross-silo request to fail after exhausting all configured retries.",
    "tags": [
      "External System",
      "Networking",
      "HTTP",
      "Retries Exhausted",
      "Upstream Unavailable"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9627953649299642,
    "cluster_avg_similarity": 0.9627953649299642,
    "fixability_score": 0.22279562056064606
  },
  {
    "cluster_id": 447,
    "project_ids": [],
    "group_ids": [6793457428, 6793463459],
    "issue_titles": ["UnsupportedResponseType: text/html; charset=utf-8"],
    "title": "JSON decode failure in SCM integration response parsing",
    "description": "Source code management integration workflow is failing to parse JSON responses, likely due to malformed or unexpected response format from external SCM provider.",
    "tags": [
      "External System",
      "Deserialization",
      "Source Code Management",
      "JSON Parse Error"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9736562302240847,
    "cluster_avg_similarity": 0.9736562302240847,
    "fixability_score": 0.48955321311950684
  },
  {
    "cluster_id": 452,
    "project_ids": [],
    "group_ids": [6793574006, 6806179302, 6910371222, 6912403409, 6915144776, 6941962034],
    "issue_titles": [
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Active' that is not in the list of supported values",
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Ready for testing' that is not in the list of supported values",
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Resolved' that is not in the list of supported values"
    ],
    "title": "Azure DevOps integration status sync fails with invalid state",
    "description": "Outbound status synchronization to Azure DevOps fails when attempting to set work item status to 'Active', which is not valid for the specific work item type or project configuration.",
    "tags": ["External System", "API", "Azure DevOps", "Input Validation"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9586693670339586,
    "cluster_avg_similarity": 0.9677368544450718,
    "fixability_score": 0.32819005846977234
  },
  {
    "cluster_id": 455,
    "project_ids": [],
    "group_ids": [6794134248, 7013444713],
    "issue_titles": [
      "IntegrityError: insert or update on table \"sentry_groupemailthread\" violates foreign key constraint \"sentry_groupemailthr_group_id_4b988db2_fk_sentry_gr\""
    ],
    "title": "Race condition in activity notifications for deleted groups",
    "description": "Activity notifications are being processed after the associated group has been deleted, causing foreign key violations when creating GroupEmailThread records for non-existent groups.",
    "tags": [
      "Database",
      "Concurrency",
      "Django",
      "Foreign Key Violation",
      "Race Condition"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9697106492875206,
    "cluster_avg_similarity": 0.9697106492875206,
    "fixability_score": 0.5041176080703735
  },
  {
    "cluster_id": 456,
    "project_ids": [],
    "group_ids": [6794396340, 6840907073, 6919117994, 6919117996, 6960360939],
    "issue_titles": [
      "ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}",
      "RetryError"
    ],
    "title": "Jira ticket creation fails on immediate retrieval",
    "description": "Integration creates Jira tickets successfully but fails when immediately retrieving the created issue due to Jira's eventual consistency and service availability issues.",
    "tags": ["External System", "API", "Jira", "Upstream Unavailable"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9436496598862641,
    "cluster_avg_similarity": 0.9605355275011608,
    "fixability_score": 0.2690275013446808
  },
  {
    "cluster_id": 459,
    "project_ids": [],
    "group_ids": [6794677909, 6812355657, 6871496144, 6926399632, 7002396288],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e..."
    ],
    "title": "Snuba rate limit exceeded during Slack notifications",
    "description": "Multiple concurrent Slack workflow notifications are independently querying Snuba for user counts, causing 19+ concurrent queries to exceed the configured limit of 18 queries for the organization.",
    "tags": ["Rate Limiting", "External System", "Concurrency", "Snuba", "Slack"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9439662098830917,
    "cluster_avg_similarity": 0.9556778296409865,
    "fixability_score": 0.2333497405052185
  },
  {
    "cluster_id": 461,
    "project_ids": [],
    "group_ids": [6794694732, 7001687475, 7003153053],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"
    ],
    "title": "Webhook task timeout from 5s deadline vs 10s RPC call",
    "description": "Send resource change webhook task has a 5-second processing deadline but requires RPC calls with 10-second timeouts, plus database queries for cache versioning that exceed the task deadline.",
    "tags": [
      "Configuration",
      "Resource Limits",
      "Database",
      "RPC Timeout",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9484712465075182,
    "cluster_avg_similarity": 0.9556454117323913,
    "fixability_score": 0.21491511166095734
  },
  {
    "cluster_id": 463,
    "project_ids": [],
    "group_ids": [6795024031, 6799758222],
    "issue_titles": ["AttributeError: 'NoneType' object has no attribute 'get'"],
    "title": "Activity notification crashes on null data field",
    "description": "The EscalatingActivityNotification assumes activity.data is always a dictionary, but manage_issue_states() can create activities with data=None when called without an event parameter, causing AttributeError when trying to call .get() on None.",
    "tags": ["Data Integrity", "Input Validation", "Null Reference Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9562511874226006,
    "cluster_avg_similarity": 0.9562511874226006,
    "fixability_score": 0.7546771764755249
  },
  {
    "cluster_id": 464,
    "project_ids": [],
    "group_ids": [6795291238, 6797692325],
    "issue_titles": [
      "AvataxException: StringLengthError: \"Field 'postalCode' has an invalid length.\"",
      "AvataxException: GetTaxError: 'Address not geocoded.'"
    ],
    "title": "Avatax integration failing during invoice processing",
    "description": "Sales tax calculation through Avatax is failing during subscription invoice generation, preventing billing operations from completing successfully.",
    "tags": ["External System", "API", "Avatax", "Invoice Processing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.957764385205827,
    "cluster_avg_similarity": 0.957764385205827,
    "fixability_score": 0.36770328879356384
  },
  {
    "cluster_id": 467,
    "project_ids": [],
    "group_ids": [6795517445, 6805787930],
    "issue_titles": [
      "SentryAppSentryError: missing_installation",
      "SentryAppSentryError: workflow_notification.missing_installation"
    ],
    "title": "Sentry App webhook fails on deleted installation",
    "description": "Webhook tasks fail when trying to send notifications to Sentry App installations that were deleted between task queueing and execution, due to stale cache or race condition in installation deletion flow.",
    "tags": ["External System", "Caching", "Concurrency", "Sentry Apps", "Stale Cache"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9666145489306287,
    "cluster_avg_similarity": 0.9666145489306287,
    "fixability_score": 0.2652767598628998
  },
  {
    "cluster_id": 470,
    "project_ids": [],
    "group_ids": [6796577351, 7008868960],
    "issue_titles": [
      "TooManyRequests: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: {"
    ],
    "title": "GCS profile uploads fail on 429 rate limiting",
    "description": "Profile processing tasks are failing when Google Cloud Storage returns 429 rate limit errors because TooManyRequests is not included in the retryable errors list, causing immediate task failure instead of retry with backoff.",
    "tags": [
      "External System",
      "Rate Limiting",
      "Google Cloud Storage",
      "Retries Exhausted"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9689228793613505,
    "cluster_avg_similarity": 0.9689228793613505,
    "fixability_score": 0.2212076336145401
  },
  {
    "cluster_id": 471,
    "project_ids": [],
    "group_ids": [6796618506, 6796639341],
    "issue_titles": ["TimeoutError: The read operation timed out"],
    "title": "Slack API timeout during link unfurl processing",
    "description": "Synchronous unfurl processing with database queries and API calls takes too long, causing SSL socket timeouts when sending unfurl responses back to Slack's API.",
    "tags": ["External System", "API", "Slack", "SSL", "Timeout"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9584220453219895,
    "cluster_avg_similarity": 0.9584220453219895,
    "fixability_score": 0.22749066352844238
  },
  {
    "cluster_id": 486,
    "project_ids": [],
    "group_ids": [6799839824, 6799839828],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, PROJECT_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, TEAM_UPDATE due to database error"
    ],
    "title": "Task deadline timeout interrupting outbox drain queries",
    "description": "Deletion tasks with 20-minute processing deadlines are being interrupted by SIGALRM while executing outbox drain operations in Django's on_commit hooks, causing PostgreSQL to cancel queries mid-execution.",
    "tags": ["Database", "Queueing", "PostgreSQL", "Timeout", "Query Canceled"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.971872537889704,
    "cluster_avg_similarity": 0.971872537889704,
    "fixability_score": 0.24229644238948822
  },
  {
    "cluster_id": 496,
    "project_ids": [],
    "group_ids": [6802448656, 7004754177],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, ORGANIZATION_MEMBER_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, TEAM_UPDATE due to database error"
    ],
    "title": "PostgreSQL query cancellation during outbox processing",
    "description": "Database queries are being canceled during outbox message processing due to statement timeouts or explicit cancellations, but the error handling only accounts for lock unavailability, causing unhandled QueryCanceled exceptions to propagate.",
    "tags": ["Database", "API", "PostgreSQL", "Query Canceled", "Timeout"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9722256896518031,
    "cluster_avg_similarity": 0.9722256896518031,
    "fixability_score": 0.22095352411270142
  },
  {
    "cluster_id": 502,
    "project_ids": [],
    "group_ids": [
      6803811392, 6868855969, 6868960886, 6868997925, 6869043403, 6873245716, 6907385791,
      6919915425
    ],
    "issue_titles": [
      "ApiForbiddenError: {",
      "ApiForbiddenError: <html>",
      "ApiForbiddenError: Forbidden",
      "ApiForbiddenError: {\"message\":\"403 Forbidden - Your account has been blocked.\"}"
    ],
    "title": "GitLab API returns 403 for integration requests",
    "description": "GitLab integration requests are failing with 403 Forbidden errors, likely due to expired or invalidated access tokens that are not being properly refreshed in the region silo architecture.",
    "tags": ["External System", "API", "Authentication", "GitLab", "HTTP 403"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9198613656988771,
    "cluster_avg_similarity": 0.9502382844889358,
    "fixability_score": 0.33435842394828796
  },
  {
    "cluster_id": 505,
    "project_ids": [],
    "group_ids": [6804142084, 6804142086, 6804142087, 6804142097, 6804142103, 6804142109],
    "issue_titles": ["OperationalError: canceling statement due to lock timeout"],
    "title": "PostgreSQL replica lock timeout from webhook processing",
    "description": "Task workers reading webhook payloads from read replica are experiencing lock timeouts due to concurrent write operations being replicated from the primary database, creating lock contention during replication lag.",
    "tags": ["Database", "Concurrency", "PostgreSQL", "Queueing", "Lock Timeout"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9497012914489866,
    "cluster_avg_similarity": 0.9668824965851563,
    "fixability_score": 0.23591963946819305
  },
  {
    "cluster_id": 510,
    "project_ids": [],
    "group_ids": [6805577213, 6823712862],
    "issue_titles": ["ApiError: <html>", "ApiError: <!DOCTYPE html>"],
    "title": "HTTP error in commit context integration requests",
    "description": "Commit context processing tasks are failing when making HTTP requests to source code management integrations, resulting in HTTPError exceptions during blame retrieval.",
    "tags": ["External System", "API", "HTTP", "Commit Context Integration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9748932635781583,
    "cluster_avg_similarity": 0.9748932635781583,
    "fixability_score": 0.2746500074863434
  },
  {
    "cluster_id": 512,
    "project_ids": [],
    "group_ids": [6805654159, 6844093237, 6906725648, 6909774457, 6909992567],
    "issue_titles": [
      "ApiError",
      "ApiError: {\"detail\":\"Internal Error\",\"errorId\":null}",
      "ApiError: <html>",
      "ApiError: GitLab is not responding"
    ],
    "title": "HTTP errors in source code management integration tasks",
    "description": "Multiple source code management integration tasks are failing with HTTP errors when making requests to external SCM providers during commit context processing and PR comment workflows.",
    "tags": ["External System", "API", "Source Code Management", "HTTP Error"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9558315379948155,
    "cluster_avg_similarity": 0.9714195188809809,
    "fixability_score": 0.27624619007110596
  },
  {
    "cluster_id": 517,
    "project_ids": [],
    "group_ids": [6806699088, 6885263122, 6886540659, 6886638352, 6889931382],
    "issue_titles": [
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'number', 'integer', 'kilobyte', 'nanosecond', 'minute', 'gibibyte', 'gigabyte', 'second', 'microsecond', 'currency', 'week', 'byte', 'megabyte', 'exaby...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'gibibyte', 'day', 'pebibyte', 'second', 'currency', 'megabyte', 'hour', 'microsecond', 'week', 'millisecond', 'mebibyte', 'kilobyte', 'petabyte', 'tebi...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'terabyte', 'integer', 'mebibyte', 'microsecond', 'duration', 'byte', 'minute', 'gibibyte', 'second', 'kilobyte', 'gigabyte', 'number', 'currency', 'peb...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'hour', 'mebibyte', 'microsecond', 'duration', 'percentage', 'millisecond', 'kilobyte', 'nanosecond', 'exabyte', 'gibibyte', 'megabyte', 'minute', 'tera...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'byte', 'bit', 'nanosecond', 'gibibyte', 'exbibyte', 'microsecond', 'percentage', 'tebibyte', 'pebibyte', 'number', 'kibibyte', 'kilobyte', 'minute', 'w..."
    ],
    "title": "EAP search resolver raising InvalidSearchQuery",
    "description": "The search resolver in the EAP (Events Analytics Platform) is consistently failing to resolve functions during timeseries query processing, raising InvalidSearchQuery exceptions in worker processes.",
    "tags": ["Input Validation", "API", "Search Resolver", "Invalid Search Query"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.958018521833547,
    "cluster_avg_similarity": 0.9689197428017404,
    "fixability_score": 0.7538249492645264
  },
  {
    "cluster_id": 523,
    "project_ids": [],
    "group_ids": [
      6808849248, 6808890367, 6809744703, 6810090277, 6835141589, 6835733702, 6941958832,
      6941958857, 7006981998
    ],
    "issue_titles": [
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'megabyte', 'percentage', 'integer', 'duration', 'hour', 'byte', 'pebibyte', 'nanosecond', 'minute', 'millisecond', 'kibibyte', 'week', ...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'number', 'gibibyte', 'microsecond', 'percentage', 'second', 'terabyte', 'kibibyte', 'kilobyte', 'megabyte', 'week', 'day', 'tebibyte', ...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'petabyte', 'microsecond', 'gibibyte', 'percentage', 'mebibyte', 'nanosecond', 'second', 'gigabyte', 'bit', 'megabyte', 'millisecond', '...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'exbibyte', 'number', 'gigabyte', 'exabyte', 'bit', 'nanosecond', 'minute', 'percentage', 'millisecond', 'duration', 'integer', 'kibibyt...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'kibibyte', 'gigabyte', 'microsecond', 'pebibyte', 'petabyte', 'currency', 'bit', 'percentage', 'exbibyte', 'exabyte', 'mebibyte', 'byte...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'percentage', 'bit', 'pebibyte', 'megabyte', 'exabyte', 'minute', 'week', 'second', 'exbibyte', 'nanosecond', 'day', 'kilobyte', 'durati...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'currency', 'bit', 'hour', 'number', 'integer', 'kibibyte', 'millisecond', 'kilobyte', 'percentage', 'day', 'microsecond', 'exabyte', 'w...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'millisecond', 'second', 'microsecond', 'day', 'exbibyte', 'gibibyte', 'integer', 'currency', 'bit', 'pebibyte', 'gigabyte', 'exabyte', ...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'mebibyte', 'gibibyte', 'megabyte', 'hour', 'gigabyte', 'week', 'duration', 'petabyte', 'bit', 'currency', 'day', 'number', 'integer', '..."
    ],
    "title": "EAP subscription fails: transaction.duration not defined",
    "description": "Snuba subscription creation fails because transaction.duration is not defined in EAP Spans attribute definitions, causing it to be treated as a string tag instead of a duration field when resolving p95() aggregates.",
    "tags": ["API", "Input Validation", "Schema Migration", "Invalid Search Query"],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9196061181667036,
    "cluster_avg_similarity": 0.9602468803064611,
    "fixability_score": 0.7491958141326904
  },
  {
    "cluster_id": 526,
    "project_ids": [],
    "group_ids": [6810188157, 6949465777],
    "issue_titles": [
      "IntegrationError: There was an error creating a comment on the Jira issue.",
      "ApiError: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/issues/comments#update-an-issue-comment\",\"status\":\"404\"}"
    ],
    "title": "GitHub/Jira integration fails on stale external IDs",
    "description": "Background tasks fail when attempting to update comments on GitHub PRs or Jira issues that have been deleted or are no longer accessible, but still exist in Sentry's database with stale external IDs.",
    "tags": ["External System", "API", "Data Integrity", "HTTP 404", "GitHub", "Jira"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9562923385054667,
    "cluster_avg_similarity": 0.9562923385054667,
    "fixability_score": 0.2831040918827057
  },
  {
    "cluster_id": 527,
    "project_ids": [],
    "group_ids": [6810382376, 6815823965],
    "issue_titles": ["MarketoError: Max rate limit '100' exceeded with in '20' secs"],
    "title": "Marketo API rate limit from excessive token requests",
    "description": "The MarketoClient retrieves a new OAuth token for every lead submission instead of caching tokens, causing double the API requests and exceeding Marketo's 100 requests per 20 seconds limit during high signup activity.",
    "tags": ["External System", "API", "Rate Limiting", "Marketo", "Authentication"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9736143525915263,
    "cluster_avg_similarity": 0.9736143525915263,
    "fixability_score": 0.22125354409217834
  },
  {
    "cluster_id": 537,
    "project_ids": [],
    "group_ids": [6830486238, 6851769528],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 52 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer..."
    ],
    "title": "Digest delivery exceeds Snuba concurrent query limit",
    "description": "Multiple digest delivery tasks executing concurrently make simultaneous TSDB queries with the same referrer, causing the cumulative query count to exceed Snuba's 50-query limit and trigger rate limiting protection.",
    "tags": ["Rate Limiting", "Concurrency", "External System", "Snuba", "TSDB"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9796443947507012,
    "cluster_avg_similarity": 0.9796443947507012,
    "fixability_score": 0.22811326384544373
  },
  {
    "cluster_id": 543,
    "project_ids": [],
    "group_ids": [6835170630, 6901424352],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c482443d7f0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7eae34134dd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
    ],
    "title": "Snooze validation fails due to Snuba connectivity issues",
    "description": "Post-processing pipeline fails when validating group snoozes because unconditional Snuba queries for user counts cannot connect to snuba-api service. All events for groups with active user-count snoozes are affected.",
    "tags": [
      "Networking",
      "External System",
      "Connection Reset",
      "Snuba",
      "Post Processing"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9685393869184717,
    "cluster_avg_similarity": 0.9685393869184717,
    "fixability_score": 0.22332972288131714
  },
  {
    "cluster_id": 544,
    "project_ids": [],
    "group_ids": [6837427063, 7015717611, 7016003682, 7016549063],
    "issue_titles": ["AssertionError"],
    "title": "Slack notification fails for legacy rules without workflow_id",
    "description": "Legacy alert rules created before workflow-engine-ui feature lack workflow_id in action data, causing assertion failures when building Slack notification messages.",
    "tags": [
      "External System",
      "Data Integrity",
      "Configuration",
      "Slack",
      "Assertion Error"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9686324506857211,
    "cluster_avg_similarity": 0.9748177909611112,
    "fixability_score": 0.7363777756690979
  },
  {
    "cluster_id": 547,
    "project_ids": [],
    "group_ids": [6839574206, 6911693548, 6919281209, 6928177029, 6969398531],
    "issue_titles": ["ApiError", "RetryError"],
    "title": "Integration notification HTTP errors in workflow actions",
    "description": "Workflow engine actions for MS Teams and Discord integrations are failing with HTTP errors when attempting to send notifications, causing task worker processes to encounter exceptions during integration client requests.",
    "tags": ["External System", "Networking", "Integration", "HTTP Error"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.95118951184161,
    "cluster_avg_similarity": 0.9632386253810417,
    "fixability_score": 0.2623176574707031
  },
  {
    "cluster_id": 551,
    "project_ids": [],
    "group_ids": [6841916270, 6848738202, 7024907832, 7024908187],
    "issue_titles": ["Cancelled: CANCELLED"],
    "title": "BigTable operations failing in nodestore backend",
    "description": "Event and transaction data storage is failing when attempting to save or retrieve data from Google Cloud Bigtable through the nodestore service. The failures occur during both write operations (mutate_rows) and read operations (read_rows) with gRPC errors being raised after retry attempts are exhausted.",
    "tags": [
      "External System",
      "Database",
      "Google Cloud Bigtable",
      "gRPC",
      "Retries Exhausted"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9479627218830252,
    "cluster_avg_similarity": 0.9640016557164514,
    "fixability_score": 0.2753864526748657
  },
  {
    "cluster_id": 552,
    "project_ids": [],
    "group_ids": [6842075781, 6843137165, 6885478841, 6889167490, 7016295924],
    "issue_titles": [
      "OperationalError: canceling statement due to user request",
      "OperationalError: server closed the connection unexpectedly"
    ],
    "title": "PostgreSQL connection failure in multiprocessing workers",
    "description": "Child processes spawned via multiprocessing inherit stale database connections from parent processes, causing connection errors when workers attempt to execute queries. The auto-reconnect mechanism fails to properly restore connections in the multiprocessing context.",
    "tags": ["Database", "Multiprocessing", "PostgreSQL", "Connection Reset"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9541333880608776,
    "cluster_avg_similarity": 0.9653905469395079,
    "fixability_score": 0.27051517367362976
  },
  {
    "cluster_id": 560,
    "project_ids": [],
    "group_ids": [6849955923, 6999733631],
    "issue_titles": [
      "TypeError: '<' not supported between instances of 'TrendBundle' and 'TrendBundle'"
    ],
    "title": "Statistical detector heap comparison fails on TrendBundle",
    "description": "The limit_regressions_by_project method fails when using heapq.heappush with TrendBundle objects that have identical scores, as TrendBundle dataclass lacks comparison operators needed for heap tie-breaking.",
    "tags": [
      "Statistical Detectors",
      "Data Structures",
      "TrendBundle",
      "Heap Comparison"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9604637414624307,
    "cluster_avg_similarity": 0.9604637414624307,
    "fixability_score": 0.6300655007362366
  },
  {
    "cluster_id": 561,
    "project_ids": [],
    "group_ids": [
      6851323218, 6851485112, 6852696200, 6853692146, 6853692147, 7021618233, 7022366196,
      7022895146
    ],
    "issue_titles": [
      "SubscriptionError: \"is:\" queries are not supported in this search."
    ],
    "title": "EAP search rejects unsupported 'is:' query syntax",
    "description": "Search queries containing 'is:' filters are being processed in EAP search contexts where this syntax is not supported, causing InvalidSearchQuery exceptions during Snuba subscription creation.",
    "tags": ["Input Validation", "API", "Search Query", "Unsupported Syntax"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9774528693430155,
    "cluster_avg_similarity": 0.9828471795919403,
    "fixability_score": 0.49657678604125977
  },
  {
    "cluster_id": 562,
    "project_ids": [],
    "group_ids": [6852547059, 7015139120],
    "issue_titles": [
      "QueryExecutionError: Connection reset by peer (10.0.0.1:9010)",
      "QueryExecutionError: DB::Exception: Function tuple requires at least one argument.: While processing tuple(): While processing ((project_id AS _snuba_project_id) IN tuple(4506041650577408)) AND ((deleted = 0) AND ((timestamp AS _snuba_timestamp) >= toDateTime('2025-08-15T07..."
    ],
    "title": "ClickHouse tuple() function called with empty group_id filter",
    "description": "Empty group_ids list from issue.id search filter creates invalid ClickHouse query 'group_id IN tuple()' which requires at least one argument. Occurs when searched issue IDs cannot be found in the system.",
    "tags": ["Database", "Input Validation", "ClickHouse", "Query Execution Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9751301933107679,
    "cluster_avg_similarity": 0.9751301933107679,
    "fixability_score": 0.7189873456954956
  },
  {
    "cluster_id": 563,
    "project_ids": [],
    "group_ids": [6853256343, 6939002409],
    "issue_titles": ["IdentityNotValid"],
    "title": "OAuth2 token refresh failing with HTTP error",
    "description": "Worker processes are encountering HTTP errors when attempting to refresh OAuth2 tokens, causing failures in the identity service.",
    "tags": ["Authentication", "External System", "OAuth2", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9794335408486344,
    "cluster_avg_similarity": 0.9794335408486344,
    "fixability_score": 0.2655694782733917
  },
  {
    "cluster_id": 572,
    "project_ids": [],
    "group_ids": [6867730427, 6870329363, 6874853205, 6876037064],
    "issue_titles": [
      "SSLError: HTTPSConnectionPool(host='api.codecov.io', port=443): Max retries exceeded with url: /webhooks/sentry (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_DECODE_ERROR] tlsv1 alert decode error (_ssl.c:1018)')))",
      "SSLError: HTTPSConnectionPool(host='api.codecov.io', port=443): Max retries exceeded with url: /webhooks/sentry (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1018)')))",
      "ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
    ],
    "title": "SSL handshake failure to Codecov API",
    "description": "Webhook delivery to Codecov is failing during SSL handshake with UNEXPECTED_EOF_WHILE_READING when processing multiple webhooks in succession. The CodecovApiClient creates new request instances for each webhook without maintaining persistent sessions, causing connection reuse issues with urllib3's connection pool.",
    "tags": ["Networking", "External System", "TLS", "Codecov", "TLS Handshake Failure"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9510868743792132,
    "cluster_avg_similarity": 0.9599793912258097,
    "fixability_score": 0.27404654026031494
  },
  {
    "cluster_id": 577,
    "project_ids": [],
    "group_ids": [6868873699, 6870676475],
    "issue_titles": ["ApiForbiddenError: {"],
    "title": "GitHub API rate limit returns 403 causing task failures",
    "description": "GitHub's secondary rate limit returns 403 Forbidden status instead of 429, but Sentry code only catches ApiRateLimitedError (429). The ApiForbiddenError is treated as an unhandled exception instead of graceful rate-limit handling.",
    "tags": ["External System", "API", "Rate Limiting", "GitHub", "Exception Handling"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9693062975348111,
    "cluster_avg_similarity": 0.9693062975348111,
    "fixability_score": 0.24742397665977478
  },
  {
    "cluster_id": 578,
    "project_ids": [],
    "group_ids": [6868949389, 7016793496],
    "issue_titles": [
      "ApiForbiddenError: {\"error\":{\"code\":\"ConversationBlockedByUser\",\"message\":\"User blocked the conversation with the bot.\"}}"
    ],
    "title": "MS Teams notification fails when user blocks bot",
    "description": "Microsoft Teams notification delivery failed with 403 Forbidden when the user has blocked the bot. The error handling doesn't distinguish between permanent blocking errors and transient failures.",
    "tags": ["External System", "API", "Microsoft Teams", "User Blocked Bot", "HTTP 403"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9653092942481873,
    "cluster_avg_similarity": 0.9653092942481873,
    "fixability_score": 0.23038354516029358
  },
  {
    "cluster_id": 581,
    "project_ids": [],
    "group_ids": [6869438488, 6997235255],
    "issue_titles": ["ValueError: Sentry app config must contain name and value keys"],
    "title": "SentryApp config validation rejects list values",
    "description": "The SentryAppFormConfigDataBlob validation only accepts string, None, or int values but Sentry app integrations legitimately use list values for multi-select fields like assignee_ids and labels.",
    "tags": [
      "Input Validation",
      "Configuration",
      "Workflow Engine",
      "SentryApp Integration"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9765949622202527,
    "cluster_avg_similarity": 0.9765949622202527,
    "fixability_score": 0.7765365839004517
  },
  {
    "cluster_id": 582,
    "project_ids": [],
    "group_ids": [6869664533, 6882646160],
    "issue_titles": [
      "ApiForbiddenError: {\"message\":\"Resource not accessible by integration\",\"documentation_url\":\"https://docs.github.com/rest/issues/comments#create-an-issue-comment\",\"status\":\"403\"}"
    ],
    "title": "GitHub API rejecting PR comments with Copilot actions",
    "description": "GitHub's issue comment API is returning 403 Forbidden errors when Sentry tries to create PR comments that include unsupported 'actions' fields for Copilot chat features.",
    "tags": ["External System", "API", "GitHub", "Integration", "Unsupported Field"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9740369071212882,
    "cluster_avg_similarity": 0.9740369071212882,
    "fixability_score": 0.30864909291267395
  },
  {
    "cluster_id": 602,
    "project_ids": [],
    "group_ids": [6881702098, 7017436380],
    "issue_titles": ["ApiError"],
    "title": "Stacktrace link fails on Bitbucket API server errors",
    "description": "The stacktrace link resolution endpoint does not gracefully handle 5xx server errors from Bitbucket API, causing the entire request to fail instead of falling back gracefully like it does for 404/400 errors.",
    "tags": ["External System", "API", "Bitbucket", "Upstream Unavailable"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9640661204144882,
    "cluster_avg_similarity": 0.9640661204144882,
    "fixability_score": 0.24186253547668457
  },
  {
    "cluster_id": 609,
    "project_ids": [],
    "group_ids": [6884134834, 7021892243],
    "issue_titles": [
      "ApiInvalidRequestError: {\"error\":\"You do not have access to repository codyde/sentryvibe, or the repository does not exist. If you believe you should have access, ensure the Cursor GitHub App is installed: https://cursor.com/api/auth/connect-github?auth_id=google-oauth2%7Cuser...",
      "IntegrationError: Error Communicating with GitHub (HTTP 404): If this repository exists, ensure that your installation has permission to access this repository (https://github.com/settings/installations)."
    ],
    "title": "HTTP errors from external integration requests",
    "description": "Integration lifecycle management and coding agents are encountering HTTP errors when making requests to external services, causing failures in group integration updates and autofix operations.",
    "tags": ["External System", "API", "HTTP Error", "Integrations"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9609766686885708,
    "cluster_avg_similarity": 0.9609766686885708,
    "fixability_score": 0.29533272981643677
  },
  {
    "cluster_id": 613,
    "project_ids": [],
    "group_ids": [6886744447, 6930058038, 6942141693],
    "issue_titles": [
      "HTTPError: 400 Client Error: Bad Request for url: https://api.codecov.io/sentry/internal/account/link/",
      "HTTPError: 400 Client Error: Bad Request for url: https://api.codecov.io/sentry/internal/account/unlink/"
    ],
    "title": "Codecov API HTTP errors in GitHub integration tasks",
    "description": "Task workers are encountering HTTP errors when making requests to Codecov API for account linking and unlinking operations in the GitHub integration.",
    "tags": ["External System", "API", "Codecov", "HTTP Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9741676668280258,
    "cluster_avg_similarity": 0.9773657526422829,
    "fixability_score": 0.25273630023002625
  },
  {
    "cluster_id": 614,
    "project_ids": [],
    "group_ids": [6886762810, 6952679837],
    "issue_titles": ["SnubaRPCError: code: 500"],
    "title": "Snuba trace query failing due to allocation policies",
    "description": "Autofix tasks fail when fetching trace data from Snuba, receiving HTTP 500 errors due to routing strategy allocation policies rejecting the query. This prevents issue summary generation for autofix functionality.",
    "tags": [
      "External System",
      "Queueing",
      "Resource Limits",
      "Snuba",
      "Allocation Policy Rejected"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9670636403036711,
    "cluster_avg_similarity": 0.9670636403036711,
    "fixability_score": 0.24172386527061462
  },
  {
    "cluster_id": 615,
    "project_ids": [],
    "group_ids": [6887443647, 6965339062],
    "issue_titles": [
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 404): VS800075: The project with id 'vstfs:///Classification/TeamProject/488a4ec0-daef-43f9-afcf-3e9eb0e32e9e' does not exist, or you do not have permission to access it.",
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 404): VS402323: Work item type Microsoft.VSTS.WorkItemTypes.Bug does not exist in project a1b727ac-c1d6-4c10-bbfb-b10fa5d7c6b0 or you do not have permission to access it."
    ],
    "title": "Azure DevOps ticket creation fails with stale project ID",
    "description": "Workflow engine attempting to create Azure DevOps work items using outdated project ID that no longer exists or is inaccessible, resulting in HTTP 404 from the API.",
    "tags": ["External System", "Configuration", "Azure DevOps", "Project Not Found"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9566422374123136,
    "cluster_avg_similarity": 0.9566422374123136,
    "fixability_score": 0.4713897109031677
  },
  {
    "cluster_id": 623,
    "project_ids": [],
    "group_ids": [6895696128, 6901434403, 6901439154, 6901440023, 6901566724, 6920892966],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='account.id is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='transaction.duration is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='http.url is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='app_name is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='url is not a tag in the metrics dataset', code='parse_error')}"
    ],
    "title": "API client errors in metric alert chart generation",
    "description": "Metric alert notifications are failing when attempting to generate chart data due to API client errors during internal service calls for timeseries data and issue periods.",
    "tags": ["API", "External System", "Metric Alerts", "Chart Generation", "ApiError"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9602595525922317,
    "cluster_avg_similarity": 0.9699696340555525,
    "fixability_score": 0.5468818545341492
  },
  {
    "cluster_id": 624,
    "project_ids": [],
    "group_ids": [6895752021, 6989208923],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.unfurl)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"
    ],
    "title": "Slack API blocks parameter malformed causing 500 errors",
    "description": "The chat_postMessage and chat_unfurl methods are receiving JSON-encoded strings for the blocks parameter instead of Python lists, causing Slack API to return HTTP 500 with non-JSON responses that the SDK cannot parse.",
    "tags": ["External System", "API", "Slack", "Serialization", "Upstream Unavailable"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9629412307535176,
    "cluster_avg_similarity": 0.9629412307535176,
    "fixability_score": 0.2914651334285736
  },
  {
    "cluster_id": 628,
    "project_ids": [],
    "group_ids": [6897002430, 6897503671, 6950824391, 7015335147],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='url is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='transaction.duration is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='Cannot query apdex with a threshold parameter on the metrics dataset', code='parse_error')}"
    ],
    "title": "Metric alert chart fails for apdex with threshold",
    "description": "Chart building for email notifications fails when metric alerts use apdex() with threshold parameters due to incorrect dataset mapping from generic_metrics to metrics dataset.",
    "tags": [
      "API",
      "Configuration",
      "Metric Alert Chart",
      "Dataset Mapping",
      "Apdex Function"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9515853729616868,
    "cluster_avg_similarity": 0.9604354434007649,
    "fixability_score": 0.42561858892440796
  },
  {
    "cluster_id": 633,
    "project_ids": [],
    "group_ids": [6900302081, 7016155264],
    "issue_titles": [
      "ApiForbiddenError: {\"message\":\"Resource not accessible by integration\",\"documentation_url\":\"https://docs.github.com/rest/checks/runs#create-a-check-run\",\"status\":\"403\"}",
      "IntegrationConfigurationError: GitHub App lacks permissions to create check runs. Please ensure the app has the required permissions and that the organization has accepted any updated permissions."
    ],
    "title": "GitHub App lacks permission for check runs API",
    "description": "Task fails when GitHub App integration doesn't have the required 'checks' permission to create status checks on repositories, resulting in 403 'Resource not accessible by integration' errors.",
    "tags": ["External System", "Authorization", "API", "GitHub", "Permission Denied"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9797084910938852,
    "cluster_avg_similarity": 0.9797084910938852,
    "fixability_score": 0.30775052309036255
  },
  {
    "cluster_id": 636,
    "project_ids": [],
    "group_ids": [6901116707, 6905848242],
    "issue_titles": ["SnubaRPCError: code: 400"],
    "title": "Snuba RPC queries failing in API endpoints",
    "description": "Multiple API endpoints for events and timeseries data are encountering RPC errors when querying Snuba, causing failures in data retrieval operations.",
    "tags": ["External System", "API", "RPC", "Snuba RPC Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9586759681298569,
    "cluster_avg_similarity": 0.9586759681298569,
    "fixability_score": 0.21576960384845734
  },
  {
    "cluster_id": 639,
    "project_ids": [],
    "group_ids": [6902559572, 6903628022, 6921588332],
    "issue_titles": [
      "ConnectionError: SafeHTTPSConnectionPool(host='your-endpoint.example.com', port=443): Max retries exceeded with url: /sentry (Caused by NewConnectionError('<sentry.net.http.SafeHTTPSConnection object at 0x79aa78464e10>: Failed to establish a new connection: [Errno -2] N...",
      "ConnectionError: SafeHTTPSConnectionPool(host='your-endpoint.example.com', port=443): Max retries exceeded with url: /sentry (Caused by NewConnectionError('<sentry.net.http.SafeHTTPSConnection object at 0x7ce5f08f6990>: Failed to establish a new connection: [Errno -2] N...",
      "ConnectionError: SafeHTTPSConnectionPool(host='your-endpoint.example.com', port=443): Max retries exceeded with url: /sentry (Caused by NewConnectionError('<sentry.net.http.SafeHTTPSConnection object at 0x7b81102f0b90>: Failed to establish a new connection: [Errno -2] N..."
    ],
    "title": "DNS resolution failure in task worker HTTP connections",
    "description": "Task workers are failing to resolve hostnames when attempting HTTP connections for service hooks, indicating either unreachable endpoints or DNS configuration issues in the production environment.",
    "tags": ["Networking", "DNS Resolution Failure", "Task Worker", "External System"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9549272902741676,
    "cluster_avg_similarity": 0.9610156169125549,
    "fixability_score": 0.27608156204223633
  },
  {
    "cluster_id": 640,
    "project_ids": [],
    "group_ids": [6904636886, 7002716279],
    "issue_titles": ["TypeError: unhashable type: 'list'"],
    "title": "Alert rule validation fails on failure_count function",
    "description": "The failure_count() function has a malformed aggregate structure that returns a nested list instead of a column name, causing unhashable type errors during alert rule validation.",
    "tags": ["Input Validation", "API", "Configuration", "Alert Rules", "Serialization"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9713217272691979,
    "cluster_avg_similarity": 0.9713217272691979,
    "fixability_score": 0.7571432590484619
  },
  {
    "cluster_id": 644,
    "project_ids": [],
    "group_ids": [6906447794, 6906447795],
    "issue_titles": [
      "SnubaRPCError: code: 500",
      "ExportError: Internal error. Please try again."
    ],
    "title": "Snuba RPC memory limit errors not handled properly",
    "description": "Data export tasks fail permanently when ClickHouse memory limits are exceeded via RPC calls because SnubaRPCError doesn't parse error codes to convert them to recoverable exception types like QueryMemoryLimitExceeded.",
    "tags": [
      "External System",
      "API",
      "Data Export",
      "Snuba",
      "ClickHouse",
      "Memory Limit"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9635674530483519,
    "cluster_avg_similarity": 0.9635674530483519,
    "fixability_score": 0.2360754907131195
  },
  {
    "cluster_id": 648,
    "project_ids": [],
    "group_ids": [6907167829, 6907577877],
    "issue_titles": ["ApiError"],
    "title": "SCM integration HTTP errors during file checks",
    "description": "Source code management integration failing when checking file existence in repositories, causing HTTP errors during stacktrace link generation.",
    "tags": ["External System", "API", "SCM Integration", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.971176188084385,
    "cluster_avg_similarity": 0.971176188084385,
    "fixability_score": 0.22858305275440216
  },
  {
    "cluster_id": 654,
    "project_ids": [],
    "group_ids": [6910285456, 7019766963],
    "issue_titles": [
      "ApiForbiddenError: {\"errorMessages\":[\"You do not have the permission to see the specified issue.\"],\"errors\":{}}",
      "ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}"
    ],
    "title": "HTTP error in integration status sync task",
    "description": "The sync_status_outbound task is failing when making HTTP requests to external integration services, causing task worker processes to encounter HTTPError exceptions.",
    "tags": ["External System", "API", "Queueing", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9635396569556726,
    "cluster_avg_similarity": 0.9635396569556726,
    "fixability_score": 0.24824972450733185
  },
  {
    "cluster_id": 659,
    "project_ids": [],
    "group_ids": [6913163064, 6916986999],
    "issue_titles": ["TypeError: Recursion limit reached"],
    "title": "Issue summary generation fails in Slack notifications",
    "description": "Errors occur when generating AI issue summaries for Slack notifications and alerts, causing failures in the Seer service integration during JSON serialization.",
    "tags": ["External System", "API", "Serialization", "Slack", "Seer"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9571508350160413,
    "cluster_avg_similarity": 0.9571508350160413,
    "fixability_score": 0.39199793338775635
  },
  {
    "cluster_id": 664,
    "project_ids": [],
    "group_ids": [6918205435, 6918983553],
    "issue_titles": ["IntegrationConfigurationError: Identity not found."],
    "title": "GitLab integration missing default identity",
    "description": "GitLab integration tasks are failing because no default identity is configured, causing Identity.DoesNotExist exceptions during commit context processing and PR comment workflows.",
    "tags": [
      "External System",
      "Configuration",
      "Authentication",
      "GitLab",
      "Identity DoesNotExist"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9696784988670252,
    "cluster_avg_similarity": 0.9696784988670252,
    "fixability_score": 0.4419569969177246
  },
  {
    "cluster_id": 665,
    "project_ids": [],
    "group_ids": [
      6919087350, 6919934306, 6925518308, 6929156762, 6950833988, 7006684726, 7016370589
    ],
    "issue_titles": [
      "IntegrationConfigurationError: {\"errorMessages\":[\"Action 101 is invalid\"],\"errors\":{}}",
      "IntegrationConfigurationError: {\"errorMessages\":[\"You should define estimation (Story Points).\"],\"errors\":{}}",
      "IntegrationConfigurationError: {\"errorMessages\":[\"An Assignee is required for all finished work.\"],\"errors\":{}}",
      "IntegrationConfigurationError: {\"errorMessages\":[\"Please update story points (enter 0 if 0)\",\"Kindly enter the Dev Owner\"],\"errors\":{}}",
      "ApiInvalidRequestError: {\"errorMessages\":[],\"errors\":{\"resolution\":\"Resolution is required.\"}}"
    ],
    "title": "Jira outbound sync fails on required transition fields",
    "description": "Jira integration sync_status_outbound task fails when attempting to transition issues that require mandatory custom fields, as the code only sends transition ID without checking for or providing required field values.",
    "tags": ["External System", "API", "Jira", "Input Validation", "HTTP Error"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9279325440775746,
    "cluster_avg_similarity": 0.9565099803213929,
    "fixability_score": 0.3226957619190216
  },
  {
    "cluster_id": 668,
    "project_ids": [],
    "group_ids": [6919653814, 6919888464],
    "issue_titles": [
      "IntegrationFormError: {'parent': ['Could not find issue by id or key.']}"
    ],
    "title": "HTTP error in post-processing rule execution",
    "description": "Task workers are encountering HTTP errors when executing post-processing rules, causing failures in the event processing pipeline during external integration requests.",
    "tags": ["API", "External System", "Networking", "HTTP Error", "Post Processing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9623417852275236,
    "cluster_avg_similarity": 0.9623417852275236,
    "fixability_score": 0.2970100939273834
  },
  {
    "cluster_id": 669,
    "project_ids": [],
    "group_ids": [6919862658, 6931488484],
    "issue_titles": [
      "IntegrationConfigurationError: {\"errorMessages\":[\"Fields Resolution and Regression must not be empty when closing this issue.\"],\"errors\":{}}"
    ],
    "title": "Integration sync task HTTP errors in worker processes",
    "description": "Outbound status synchronization tasks are failing with HTTP errors when communicating with external integration endpoints, causing worker process failures.",
    "tags": ["External System", "API", "HTTP Error", "Task Worker"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9778209206821844,
    "cluster_avg_similarity": 0.9778209206821844,
    "fixability_score": 0.3974294662475586
  },
  {
    "cluster_id": 677,
    "project_ids": [],
    "group_ids": [6925797657, 6929027521],
    "issue_titles": [
      "IntegrationFormError: {'customfield_20407': [\"Option id 'null' is not valid\"], 'customfield_17016': [\"Option id 'n/a' is not valid\"]}",
      "IntegrationFormError: {'customfield_10111': ['Customer name is required.']}"
    ],
    "title": "Jira custom field transformer missing for team fields",
    "description": "Team field values are being passed through without proper transformation due to missing handler in the type transformer mappings, causing Jira API to receive incorrectly formatted values.",
    "tags": ["External System", "API", "Serialization", "Jira", "Missing Configuration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9555390067125455,
    "cluster_avg_similarity": 0.9555390067125455,
    "fixability_score": 0.4321736693382263
  },
  {
    "cluster_id": 679,
    "project_ids": [],
    "group_ids": [6926147545, 6927273924, 6940106787, 6942355371],
    "issue_titles": [
      "ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"
    ],
    "title": "Metric alert chart generation API error",
    "description": "Chart generation for metric alerts is failing during notification delivery to Slack and Discord integrations due to API errors when fetching metric issue data.",
    "tags": [
      "API",
      "External System",
      "Sentry Client",
      "Metric Alerts",
      "Chart Generation"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9752024954829852,
    "cluster_avg_similarity": 0.9808331825351707,
    "fixability_score": 0.6216449737548828
  },
  {
    "cluster_id": 682,
    "project_ids": [],
    "group_ids": [6928230758, 7019625543],
    "issue_titles": [
      "ValidationError: Invalid config: Additional properties are not allowed ('options' was unexpected)"
    ],
    "title": "Workflow action config validation fails during rule update",
    "description": "Project rule updates are failing when the workflow engine attempts to validate action configurations during the issue alert migration process.",
    "tags": ["Input Validation", "Configuration", "API", "Validation Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9645175116695943,
    "cluster_avg_similarity": 0.9645175116695943,
    "fixability_score": 0.43605831265449524
  },
  {
    "cluster_id": 690,
    "project_ids": [],
    "group_ids": [6935964384, 6941517821, 6952042219],
    "issue_titles": ["IntegrationResourceNotFoundError"],
    "title": "HTTP error in notification action workflow execution",
    "description": "Workflow engine notification actions are failing due to HTTP errors when making requests to external integrations during issue alert processing.",
    "tags": ["Networking", "External System", "HTTP Error", "Workflow Engine"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9712677253229265,
    "cluster_avg_similarity": 0.9749467511832677,
    "fixability_score": 0.2909908592700958
  },
  {
    "cluster_id": 704,
    "project_ids": [],
    "group_ids": [6948437711, 7023348768],
    "issue_titles": ["KeyError: 'statusCode'"],
    "title": "Replay log summarization fails on missing statusCode",
    "description": "The replay summary generation is attempting to access a 'statusCode' field in the payload data that doesn't exist, causing KeyError exceptions during log message processing.",
    "tags": ["Data Integrity", "Input Validation", "Replay Features", "Missing Field"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9882410102548541,
    "cluster_avg_similarity": 0.9882410102548541,
    "fixability_score": 0.5679507851600647
  },
  {
    "cluster_id": 710,
    "project_ids": [],
    "group_ids": [6952625620, 7017047800],
    "issue_titles": [
      "ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"
    ],
    "title": "Discord alert fails due to AlertRuleDetector lookup failure",
    "description": "Discord metric alert notifications are failing because the AlertRuleDetector lookup table is missing records linking detector IDs to alert rule IDs, causing API calls to use non-existent alert rule identifiers.",
    "tags": ["External System", "Data Integrity", "API", "Discord", "Missing Record"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9779340683442379,
    "cluster_avg_similarity": 0.9779340683442379,
    "fixability_score": 0.3264564871788025
  },
  {
    "cluster_id": 724,
    "project_ids": [],
    "group_ids": [6963221055, 6963221131, 7003895835, 7003973300, 7004048071],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "Django database connection fails in async context",
    "description": "Python 3.13.1 implicitly creates an asyncio event loop during Django initialization, triggering Django's async safety checks that prevent synchronous ORM operations from accessing the database properly.",
    "tags": [
      "Database",
      "Concurrency",
      "Configuration",
      "Django",
      "PostgreSQL",
      "Async Context",
      "Connection Refused"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.946786523231882,
    "cluster_avg_similarity": 0.9588782668853044,
    "fixability_score": 0.2689988911151886
  },
  {
    "cluster_id": 727,
    "project_ids": [],
    "group_ids": [6963223239, 6984577065],
    "issue_titles": [
      "OutboxFlushError: Could not flush shard category=33 (ORG_AUTH_TOKEN_UPDATE)",
      "OutboxFlushError: Could not flush shard category=0 (USER_UPDATE)"
    ],
    "title": "Hybrid cloud RPC calls failing with invalid service request",
    "description": "Cross-silo RPC calls for async replication tasks are failing during outbox processing, preventing proper data synchronization between silos.",
    "tags": [
      "API",
      "External System",
      "Queueing",
      "Invalid Service Request",
      "Cross-Silo RPC"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9754639883569379,
    "cluster_avg_similarity": 0.9754639883569379,
    "fixability_score": 0.2499106526374817
  },
  {
    "cluster_id": 728,
    "project_ids": [],
    "group_ids": [6964281150, 6964311697],
    "issue_titles": [
      "Cannot disable BillingSeatAssignment that doesn't exist",
      "Cannot remove BillingSeatAssignment that doesn't exist"
    ],
    "title": "BillingSeatAssignment missing when disabling detector",
    "description": "Uptime detectors are being disabled but no corresponding BillingSeatAssignment exists in the current billing history, likely due to race conditions between detector lifecycle and seat assignment or detectors that never had seats assigned.",
    "tags": [
      "Database",
      "Data Integrity",
      "Uptime Monitoring",
      "Billing",
      "Race Condition"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9558707717095408,
    "cluster_avg_similarity": 0.9558707717095408,
    "fixability_score": 0.32972970604896545
  },
  {
    "cluster_id": 730,
    "project_ids": [],
    "group_ids": [6964872308, 6964872452],
    "issue_titles": ["AssertionError"],
    "title": "Authentication pipeline fails with assertion error",
    "description": "OAuth2 authentication pipeline encounters assertion failure during membership handling, indicating missing or invalid RPC organization membership object.",
    "tags": ["Authentication", "API", "Django", "Assertion Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9614794793786624,
    "cluster_avg_similarity": 0.9614794793786624,
    "fixability_score": 0.5063017010688782
  },
  {
    "cluster_id": 740,
    "project_ids": [],
    "group_ids": [6969557733, 7003688259],
    "issue_titles": [
      "ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"
    ],
    "title": "Slack metric alert chart API returns 404 for incidents",
    "description": "The incidents API call fails with HTTP 404 when building metric alert charts for Slack notifications due to an invalid project parameter value of -1 being passed to the endpoint.",
    "tags": ["API", "External System", "Slack", "Invalid Project Parameter"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.963195007991872,
    "cluster_avg_similarity": 0.963195007991872,
    "fixability_score": 0.3070140480995178
  },
  {
    "cluster_id": 748,
    "project_ids": [],
    "group_ids": [6977926819, 6977946585, 6977955995, 6977974407, 6977978467],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=15)"
    ],
    "title": "Seer spam detection timeouts in feedback ingestion",
    "description": "HTTP read timeouts occur when calling the seer-web-summarization service for spam detection during feedback processing. The 15-second timeout is exceeded due to service unavailability or connection pool exhaustion.",
    "tags": ["External System", "Networking", "Timeout", "Seer", "Connection Pool"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9395092729403528,
    "cluster_avg_similarity": 0.9612860590407333,
    "fixability_score": 0.21301668882369995
  },
  {
    "cluster_id": 754,
    "project_ids": [],
    "group_ids": [6981975256, 7003895622],
    "issue_titles": [
      "OperationalError: canceling statement due to user request",
      "OperationalError: server closed the connection unexpectedly"
    ],
    "title": "PostgreSQL connection timeout during bulk deletion",
    "description": "Long-running group deletion operations with sequential child table deletions cause idle transaction timeouts, closing database connections before completion.",
    "tags": [
      "Database",
      "Bulk Operations",
      "PostgreSQL",
      "Idle Timeout",
      "Connection Reset"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9579837100043009,
    "cluster_avg_similarity": 0.9579837100043009,
    "fixability_score": 0.24722494184970856
  },
  {
    "cluster_id": 757,
    "project_ids": [],
    "group_ids": [6983217934, 7011813887, 7014444250],
    "issue_titles": [
      "SubscriptionError: ReservedBudgetHistory not updated while recomputing reserved budget spend"
    ],
    "title": "Billing usage lock timeout during reserved budget recompute",
    "description": "Usage buffer flush fails when reserved budget lock acquisition times out after 5 seconds, returning None instead of handling lock contention gracefully in billing metric history writes.",
    "tags": ["Concurrency", "Database", "Queueing", "Lock Timeout", "Subscription Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9771891900188616,
    "cluster_avg_similarity": 0.9778962400712086,
    "fixability_score": 0.33975714445114136
  },
  {
    "cluster_id": 766,
    "project_ids": [],
    "group_ids": [6993631144, 6998456211, 7013410795],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "Task worker database queries cancelled by processing deadline",
    "description": "Task worker processes are timing out after 90-120 seconds and SIGALRM signal interruption causes PostgreSQL to cancel in-progress queries with 'user request' cancellation errors.",
    "tags": ["Database", "Concurrency", "PostgreSQL", "Timeout", "Task Processing"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9581772386636567,
    "cluster_avg_similarity": 0.9618625349720581,
    "fixability_score": 0.22459477186203003
  },
  {
    "cluster_id": 773,
    "project_ids": [],
    "group_ids": [6997376086, 7001416547],
    "issue_titles": ["RuntimeError: dictionary changed size during iteration"],
    "title": "Kafka producer metrics race condition in threaded consumers",
    "description": "Multiple threads are concurrently modifying the ConfluentProducer metrics dictionary while another thread iterates over it, causing a dictionary size change error during metrics flushing in the arroyo Kafka backend.",
    "tags": ["Concurrency", "Messaging", "Kafka", "Dictionary Changed Size"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9805045732111671,
    "cluster_avg_similarity": 0.9805045732111671,
    "fixability_score": 0.3792543411254883
  },
  {
    "cluster_id": 774,
    "project_ids": [],
    "group_ids": [6997913297, 7006941576],
    "issue_titles": ["ValueError: could not convert string to float: '2500ms'"],
    "title": "EAP resolver fails parsing apdex function with time units",
    "description": "The resolve_function method attempts to convert time unit arguments like '2500ms' directly to float without parsing the unit suffix, causing conversion failures when processing apdex queries.",
    "tags": [
      "API",
      "Input Validation",
      "Serialization",
      "Function Parsing",
      "Time Unit Conversion"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9554662061815267,
    "cluster_avg_similarity": 0.9554662061815267,
    "fixability_score": 0.7269492745399475
  },
  {
    "cluster_id": 781,
    "project_ids": [],
    "group_ids": [
      7001535740, 7001664563, 7006047416, 7006860226, 7017657981, 7018354736, 7018502802,
      7018908040, 7020888902, 7022660992
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2"
    ],
    "title": "Webhook task timeout during DNS resolution",
    "description": "Webhook tasks are hitting their 5-second processing deadline while attempting to resolve DNS names for external webhook URLs or during Redis cluster initialization.",
    "tags": [
      "Networking",
      "Queueing",
      "DNS Resolution Failure",
      "Processing Deadline Exceeded",
      "Redis"
    ],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9388805392764084,
    "cluster_avg_similarity": 0.9622444024181456,
    "fixability_score": 0.22269436717033386
  },
  {
    "cluster_id": 784,
    "project_ids": [],
    "group_ids": [7001742428, 7016470239],
    "issue_titles": [
      "DataCondition.MultipleObjectsReturned: get() returned more than one DataCondition -- it returned 2!"
    ],
    "title": "Workflow engine serializer fails with multiple triggers",
    "description": "The WorkflowEngineActionSerializer fails when an action is linked to multiple alert rule triggers (e.g., warning + critical), causing a MultipleObjectsReturned exception during detector trigger lookup.",
    "tags": ["API", "Serialization", "Database", "Constraint Violation"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9703445743307757,
    "cluster_avg_similarity": 0.9703445743307757,
    "fixability_score": 0.49355918169021606
  },
  {
    "cluster_id": 788,
    "project_ids": [],
    "group_ids": [7003973301, 7003973322],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "Database replica unavailable during job feature checks",
    "description": "Dashboard migration job fails to connect to usage_replica database when checking feature flags for each organization, causing repeated connection failures on port 6432.",
    "tags": [
      "Database",
      "External System",
      "Configuration",
      "PostgreSQL",
      "Connection Reset"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9742549861922472,
    "cluster_avg_similarity": 0.9742549861922472,
    "fixability_score": 0.2657999098300934
  },
  {
    "cluster_id": 794,
    "project_ids": [],
    "group_ids": [7004867863, 7004867957, 7004868003, 7009542153],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth2 state validation fails due to Redis session expiry",
    "description": "VSTS OAuth login fails when Redis session state expires between OAuth request and callback validation, causing state mismatch errors even with valid OAuth parameters.",
    "tags": [
      "Authentication",
      "External System",
      "Caching",
      "Redis",
      "OAuth2",
      "Session Timeout"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9774144947995997,
    "cluster_avg_similarity": 0.9785109717217194,
    "fixability_score": 0.3142615556716919
  },
  {
    "cluster_id": 797,
    "project_ids": [],
    "group_ids": [7005906779, 7009541944, 7009542437, 7009542446],
    "issue_titles": [
      "PipelineError: An error occurred while validating your request.",
      "PipelineError: Could not fetch a request token from Jira. oauth_problem=signature_invalid&oauth_signature=b4KmyGsvE%2BDBLileyEA48xK4QN%2FrQtWYT%2B3pm6llboWGqTVp083Q4lu6V2%2FhkvmCh4yc9fFi2IRcFz2k9x4QHfxHMB1NGQCu2MS5vAJJaDcN9wwrDpHdO1tg5nlybVUyGycWdc1FFqs..."
    ],
    "title": "OAuth callback state mismatch in GitHub login flow",
    "description": "The OAuth state parameter stored during login initiation doesn't match the state returned in the callback, likely due to Redis session race conditions or timing issues during the GitHub authentication flow.",
    "tags": [
      "Authentication",
      "External System",
      "OAuth",
      "GitHub",
      "Redis",
      "State Mismatch"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9604823260806331,
    "cluster_avg_similarity": 0.9713895614893403,
    "fixability_score": 0.3073435425758362
  },
  {
    "cluster_id": 801,
    "project_ids": [],
    "group_ids": [7006590963, 7006591206],
    "issue_titles": [
      "RetryError",
      "OperationalError: canceling statement due to user request"
    ],
    "title": "Group deletion task timeout during hash cleanup",
    "description": "Large groups with ~640k hash records exceed the 10-minute task processing deadline, causing SIGALRM timeout interruption during database queries and resulting in query cancellation errors.",
    "tags": ["Database", "Queueing", "PostgreSQL", "Timeout", "Resource Limits"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9765695391881317,
    "cluster_avg_similarity": 0.9765695391881317,
    "fixability_score": 0.2468487024307251
  },
  {
    "cluster_id": 802,
    "project_ids": [],
    "group_ids": [7006655649, 7006713356],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "PostgreSQL query canceled in GroupHashMetadata deletion",
    "description": "Deletion tasks are timing out due to full table scans on unindexed seer_matched_grouphash_id column, causing PostgreSQL to cancel long-running queries during GroupHashMetadata batch processing.",
    "tags": ["Database", "Performance", "PostgreSQL", "Query Canceled", "Missing Index"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9676588070703817,
    "cluster_avg_similarity": 0.9676588070703817,
    "fixability_score": 0.23561565577983856
  },
  {
    "cluster_id": 818,
    "project_ids": [],
    "group_ids": [7012239361, 7014169330],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='Your interval and date range would create too many results. Use a larger interval, or a smaller date range.', code='parse_error')}"
    ],
    "title": "Sessions API limit exceeded in crash-free alert charts",
    "description": "Crash-free metric alerts use the alert's 60-minute evaluation window as the query interval when fetching sessions data for chart visualization, but this interval applied to the expanded incident date range (up to 59 days) exceeds the sessions API's 1,000 data point limit.",
    "tags": [
      "API",
      "External System",
      "Rate Limiting",
      "Slack Integration",
      "Sessions API"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9644343957271756,
    "cluster_avg_similarity": 0.9644343957271756,
    "fixability_score": 0.22260630130767822
  },
  {
    "cluster_id": 824,
    "project_ids": [],
    "group_ids": [7013927465, 7015997339],
    "issue_titles": [
      "JSONDecodeError: unexpected end of data: line 1 column 65793 (char 65792)",
      "JSONDecodeError: unexpected end of data: line 1 column 53630 (char 53629)"
    ],
    "title": "VSTS/GitLab webhook request body truncation during parsing",
    "description": "Multiple reads of Django's request.body in webhook middleware causes WSGI input stream truncation at ~50KB limit, resulting in incomplete JSON payloads during webhook processing.",
    "tags": [
      "API",
      "Serialization",
      "Django",
      "Request Body Truncation",
      "JSON Parse Error"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9714526313411375,
    "cluster_avg_similarity": 0.9714526313411375,
    "fixability_score": 0.37155047059059143
  },
  {
    "cluster_id": 848,
    "project_ids": [],
    "group_ids": [7021539800, 7021540234],
    "issue_titles": ["SubscriptionIntegrityError: Cannot start plan trial"],
    "title": "Database execution failure in Sentry API endpoint",
    "description": "API requests are failing during database query execution, with errors occurring in the PostgreSQL database layer after passing through Django middleware stack.",
    "tags": ["Database", "API", "PostgreSQL", "Django"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9839674006358616,
    "cluster_avg_similarity": 0.9839674006358616,
    "fixability_score": 0.26744261384010315
  },
  {
    "cluster_id": 849,
    "project_ids": [],
    "group_ids": [7021744342, 7024563637],
    "issue_titles": [
      "HTTPError: 500 Server Error: Internal Server Error for url: https://api.github.com/user",
      "HTTPError: 500 Server Error: Internal Server Error for url: https://api.github.com/user/emails"
    ],
    "title": "GitHub API requests failing in identity pipeline",
    "description": "HTTP errors are occurring when fetching user information and email data from GitHub's API during the identity provider authentication flow.",
    "tags": ["Authentication", "External System", "API", "GitHub", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9692740595246337,
    "cluster_avg_similarity": 0.9692740595246337,
    "fixability_score": 0.25345584750175476
  },
  {
    "cluster_id": 857,
    "project_ids": [],
    "group_ids": [7024062529, 7024066882],
    "issue_titles": [
      "HTTPError: 503 Server Error: Service Unavailable for url: https://app.vssps.visualstudio.com/_apis/profile/profiles/me?api-version=1.0"
    ],
    "title": "VSTS integration setup fails on user info retrieval",
    "description": "HTTP error occurs when fetching user information from VSTS API during integration pipeline completion, causing the integration setup to fail.",
    "tags": ["External System", "API", "VSTS", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9915396988019348,
    "cluster_avg_similarity": 0.9915396988019348,
    "fixability_score": 0.23719733953475952
  },
  {
    "cluster_id": 859,
    "project_ids": [],
    "group_ids": [7024265714, 7024599475],
    "issue_titles": [
      "ApiRateLimitedError",
      "ApiRateLimitedError: <!doctype html><html lang=\"en\"><head><title>HTTP Status 429  Too Many Requests</title><style type=\"text/css\">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 ..."
    ],
    "title": "HTTP errors from external integration requests",
    "description": "Integration endpoints are receiving HTTP error responses from external services, causing request failures in group integration details and MS Teams webhook handlers.",
    "tags": ["External System", "API", "HTTP Error", "Integration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9652758079564077,
    "cluster_avg_similarity": 0.9652758079564077,
    "fixability_score": 0.22053806483745575
  }
]
