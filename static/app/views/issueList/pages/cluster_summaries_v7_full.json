[
  {
    "cluster_id": 1,
    "project_ids": [],
    "group_ids": [2963494257, 6713621492, 6714288680, 6725845377, 6738867332],
    "issue_titles": [
      "NoRetriesRemainingError: sentry.tasks.summaries.weekly_reports.prepare_organization_report has consumed all of its retries",
      "NoRetriesRemainingError: sentry.tasks.merge.merge_groups has consumed all of its retries",
      "NoRetriesRemainingError: sentry.tasks.code_owners_auto_sync has consumed all of its retries",
      "NoRetriesRemainingError: sentry.tasks.weekly_escalating_forecast.generate_forecasts_for_projects has consumed all of its retries"
    ],
    "title": "Task worker database failures with retry errors",
    "description": "Multiple task worker child processes are experiencing database-related failures during SQL execution and transaction commits, triggering retry error handling in the worker.",
    "tags": ["Database", "Queueing", "Concurrency", "PostgreSQL", "Task Worker"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9420168261015817,
    "cluster_avg_similarity": 0.9525897779891906
  },
  {
    "cluster_id": 13,
    "project_ids": [],
    "group_ids": [4918185292, 6690381475],
    "issue_titles": [
      "ChunkedEncodingError: ('Connection broken: IncompleteRead(1182900 bytes read, 2049249 more expected)', IncompleteRead(1182900 bytes read, 2049249 more expected))",
      "ProtocolError: ('Connection broken: IncompleteRead(507 bytes read, 3589 more expected)', IncompleteRead(507 bytes read, 3589 more expected))"
    ],
    "title": "HTTP incomplete read during request processing",
    "description": "Integration proxy endpoints are encountering incomplete HTTP responses, causing IncompleteRead exceptions during request/response handling in urllib3.",
    "tags": ["Networking", "API", "HTTP", "Incomplete Read"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9706110811185935,
    "cluster_avg_similarity": 0.9706110811185935
  },
  {
    "cluster_id": 22,
    "project_ids": [],
    "group_ids": [5288360693, 6532776250, 6536481522, 6921207226, 7007152009],
    "issue_titles": [
      "KafkaException: KafkaError{code=_DESTROY,val=-197,str=\"Failed to get committed offsets: Local: Broker handle destroyed\"}",
      "RetryException",
      "KafkaException: KafkaError{code=_DESTROY,val=-197,str=\"Commit failed: Local: Broker handle destroyed\"}"
    ],
    "title": "Kafka offset commit fails during consumer shutdown",
    "description": "Kafka consumer attempts to commit offsets after broker handle is destroyed during partition revocation, causing cascading failures in the StreamProcessor shutdown sequence.",
    "tags": [
      "Messaging",
      "Concurrency",
      "Kafka",
      "Partition Revocation",
      "Broker Handle Destroyed"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9498432401323968,
    "cluster_avg_similarity": 0.9707096582865802
  },
  {
    "cluster_id": 23,
    "project_ids": [],
    "group_ids": [5314268382, 6793049827, 7020778160],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"sentry_organizationslugreservation_slug_key\"",
      "IntegrityError: duplicate key value violates unique constraint \"accounts_customer_organization_id_key\"",
      "IntegrityError: duplicate key value violates unique constraint \"accounts_thirdpartyaccount_type_4e5db8d20cf5f9f5_uniq\""
    ],
    "title": "Database errors during organization provisioning",
    "description": "Multiple database execution failures occurring in organization provisioning workflows, affecting both API endpoints and background task processing for outbox message handling.",
    "tags": ["Database", "API", "PostgreSQL", "Organization Provisioning"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9444419819282877,
    "cluster_avg_similarity": 0.9563408287276488
  },
  {
    "cluster_id": 25,
    "project_ids": [],
    "group_ids": [5353134098, 5647494211, 6632448159],
    "issue_titles": [
      "ApiHostError: Unable to reach host: sentry-rpc-prod-control.us.sentry.internal:8999"
    ],
    "title": "HTTP socket timeout in source code integration APIs",
    "description": "Multiple source code management integration endpoints (stacktrace links, GitLab issues, PR workflows) are experiencing socket-level timeouts when communicating with external services.",
    "tags": [
      "Networking",
      "External System",
      "HTTP",
      "Timeout",
      "Source Code Integration"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9443690447390589,
    "cluster_avg_similarity": 0.9541172454223193
  },
  {
    "cluster_id": 27,
    "project_ids": [],
    "group_ids": [
      5360930327, 6656608692, 6672091732, 6675765299, 6676253500, 6712110232, 6713214240,
      6713727616, 6713929018, 6735586674, 6782675336, 6789207839, 6792288439, 6792667881,
      6792968209, 6838185572, 6838418923, 6840698708, 6843751363, 6977945398
    ],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='192.168.208.181', port=8080): Read timed out. (read timeout=5)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-gpu-web-group-seer', port=80): Read timed out. (read timeout=0.6)",
      "ReadTimeout: SafeHTTPSConnectionPool(host='applications.zoom.us', port=443): Read timed out. (read timeout=5)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=15)",
      "ReadTimeoutError: HTTPConnectionPool(host='filestore-default.ilb.de.sentry.internal.', port=8080): Read timed out. (read timeout=5)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=30)",
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=5)"
    ],
    "title": "HTTP connection failure in exception handling",
    "description": "Socket read operations are failing during HTTP requests made by the exception handling system, likely when attempting to report errors to external monitoring services.",
    "tags": ["Networking", "External System", "Socket Read Failure", "urllib3"],
    "cluster_size": 20,
    "cluster_min_similarity": 0.9256448118371005,
    "cluster_avg_similarity": 0.9543989644432693
  },
  {
    "cluster_id": 29,
    "project_ids": [],
    "group_ids": [5484889676, 7014219981],
    "issue_titles": [
      "Failed to send a request to Slack API server: The read operation timed out",
      "TimeoutError: The read operation timed out"
    ],
    "title": "Slack channel validation timeout during alert rule update",
    "description": "TimeoutError from Slack API conversations.info call is not caught during alert rule trigger action validation, causing unhandled exceptions during rule updates.",
    "tags": ["External System", "API", "Input Validation", "Slack", "Timeout"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9686293788549863,
    "cluster_avg_similarity": 0.9686293788549863
  },
  {
    "cluster_id": 30,
    "project_ids": [],
    "group_ids": [5501687898, 7015274408],
    "issue_titles": [
      "Failed to decode Slack API response: Received a response in a non-JSON format: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"><html><head><title>503 Service Unavailable</title>...",
      "Failed to decode Slack API response: Received a response in a non-JSON format: <!DOCTYPE html><html lang=\"en\"><head><meta charset=\"utf-8\"><title>Server Error | Slack</title><meta ..."
    ],
    "title": "Slack SDK JSON decode failure on 503 HTML response",
    "description": "Slack API returns HTML error page instead of JSON during 503 Service Unavailable, causing Slack SDK to fail JSON parsing. Missing exception handler for SlackRequestError in Sentry's notification code.",
    "tags": [
      "External System",
      "API",
      "Serialization",
      "Slack",
      "Upstream Unavailable",
      "JSON Decode Error"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9593659456334338,
    "cluster_avg_similarity": 0.9593659456334338
  },
  {
    "cluster_id": 32,
    "project_ids": [],
    "group_ids": [5539974675, 6659813316, 6719756970],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.scheduleMessage)"
    ],
    "title": "Slack bot lacks access to private channels for verification",
    "description": "The Slack integration fails to verify channel existence because the bot hasn't been invited to the private channel, causing chat.scheduleMessage to return channel_not_found even when the channel exists.",
    "tags": ["External System", "Authorization", "Slack", "Channel Not Found"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9424847894333181,
    "cluster_avg_similarity": 0.9596440713522458
  },
  {
    "cluster_id": 34,
    "project_ids": [],
    "group_ids": [5589731605, 6679304413, 6962588483],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/conversations.info)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.scheduleMessage)"
    ],
    "title": "Slack channel validation fails with channel_not_found",
    "description": "Channel validation attempts to verify channel existence using chat.scheduleMessage API but passes bare channel names that Slack API doesn't recognize, causing legitimate channel_not_found errors to be logged.",
    "tags": ["External System", "API", "Input Validation", "Slack", "Channel Not Found"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9545598505572751,
    "cluster_avg_similarity": 0.9596854382956135
  },
  {
    "cluster_id": 35,
    "project_ids": [],
    "group_ids": [5637871250, 6140617362, 6840520387, 6879549779, 6928118656],
    "issue_titles": [
      "ApiTimeoutError: Timed out attempting to reach host: discord.com",
      "ApiTimeoutError: Timed out attempting to reach host: smba.trafficmanager.net",
      "ApiTimeoutError: Timed out attempting to reach host: api.github.com"
    ],
    "title": "SSL read failures in integration alert notifications",
    "description": "SSL socket read operations are failing during HTTP requests when sending Discord and MS Teams integration notifications, causing alert delivery failures.",
    "tags": [
      "Networking",
      "External System",
      "TLS",
      "Discord",
      "MS Teams",
      "SSL Read Failure"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9376896178776655,
    "cluster_avg_similarity": 0.9527435323216391
  },
  {
    "cluster_id": 36,
    "project_ids": [],
    "group_ids": [5718609218, 6617701043, 6882555644],
    "issue_titles": [
      "ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "ApiConnectionResetError: Connection reset by peer"
    ],
    "title": "HTTP connection failures in Django middleware chain",
    "description": "Multiple HTTP requests are failing due to remote disconnections and socket errors during middleware processing, affecting both external integrations and internal API endpoints.",
    "tags": ["Networking", "API", "Django", "Connection Reset", "Remote Disconnected"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9494418839095545,
    "cluster_avg_similarity": 0.9614536960815735
  },
  {
    "cluster_id": 38,
    "project_ids": [],
    "group_ids": [5859179121, 6805580390],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/users.list)"
    ],
    "title": "Slack API error during user/channel lookup",
    "description": "Sentry's Slack integration is encountering API errors while attempting to retrieve user lists or validate channel information for alert rule configuration.",
    "tags": ["External System", "API", "Slack", "Integration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9707202752502426,
    "cluster_avg_similarity": 0.9707202752502426
  },
  {
    "cluster_id": 41,
    "project_ids": [],
    "group_ids": [6032121515, 6789342363, 7001894808, 7015274361],
    "issue_titles": [
      "Unrecognized Slack API message. Api Message: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"
    ],
    "title": "Slack API 503 error creates unrecognized error issues",
    "description": "Slack API returned 503 Service Unavailable with HTML content instead of JSON during a service outage. Sentry's error categorization doesn't recognize this as a temporary service issue, incorrectly treating it as an unrecognized failure requiring investigation rather than a transient external service problem.",
    "tags": [
      "External System",
      "API",
      "Slack",
      "Upstream Unavailable",
      "Unrecognized Error"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9592935178391885,
    "cluster_avg_similarity": 0.9676291264908067
  },
  {
    "cluster_id": 45,
    "project_ids": [],
    "group_ids": [6194067510, 6909613563],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Unknown function notHandled: While processing (notHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-11-12T02:25:50', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-11-13T02:26:50', 'Universal')) AND ((project...",
      "QueryExecutionError: DB::Exception: Unknown function isHandled: While processing (isHandled() = 1) AND ((finish_ts AS _snuba_finish_ts) >= toDateTime('2025-11-05T18:34:25', 'Universal')) AND (_snuba_finish_ts < toDateTime('2025-11-06T18:35:25', 'Universal')) AND ((project_i..."
    ],
    "title": "ClickHouse query fails with unknown notHandled function",
    "description": "The error.unhandled filter is being applied to the Transactions dataset, which generates an invalid notHandled() function call that doesn't exist in ClickHouse's transactions table schema.",
    "tags": ["Database", "API", "ClickHouse", "Query Execution Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9672824902577543,
    "cluster_avg_similarity": 0.9672824902577543
  },
  {
    "cluster_id": 47,
    "project_ids": [],
    "group_ids": [6268620370, 6268915744],
    "issue_titles": ["AssertionError"],
    "title": "Empty queue access in batch processing reset",
    "description": "The batch builder reset operation is attempting to pop from an empty input blocks queue, indicating a state management issue in the multiprocessing strategy.",
    "tags": ["Queueing", "Concurrency", "Arroyo", "Empty Queue"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9762623886710843,
    "cluster_avg_similarity": 0.9762623886710843
  },
  {
    "cluster_id": 48,
    "project_ids": [],
    "group_ids": [6269047879, 6270530195],
    "issue_titles": [
      "KafkaException: KafkaError{code=UNKNOWN_MEMBER_ID,val=25,str=\"Commit failed: Broker: Unknown member\"}"
    ],
    "title": "Message processing failures in Arroyo threads",
    "description": "Arroyo processing strategy is rejecting messages during thread-based execution and encountering issues with batch builder state management in multiprocessing mode.",
    "tags": ["Queueing", "Concurrency", "Arroyo", "Message Rejected"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9719095856185408,
    "cluster_avg_similarity": 0.9719095856185408
  },
  {
    "cluster_id": 50,
    "project_ids": [],
    "group_ids": [6291262905, 6304574025, 6929873784],
    "issue_titles": ["AssertionError"],
    "title": "Arroyo consumer race condition during rebalance",
    "description": "MessageRejected backpressure handling conflicts with Kafka partition rebalancing, causing paused consumer state to be cleared unexpectedly and violating polling assertions.",
    "tags": ["Messaging", "Concurrency", "Kafka", "Message Rejected", "Rebalancing"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9649193588973538,
    "cluster_avg_similarity": 0.9677784486792013
  },
  {
    "cluster_id": 51,
    "project_ids": [],
    "group_ids": [
      6351856285, 6656458218, 6673102888, 6680920814, 6711459865, 6725380019, 6725380254,
      6782612724, 6782644221, 6782679792, 6791854756, 6792450203, 6792515143, 6792656623,
      6792751930, 6792752935, 6793135079, 6793537536, 6793840201, 6793968470, 6793972067,
      6794198699, 6794201570, 6794593243, 6794593334, 6794670713, 6795050820, 6795050852,
      6795338666, 6798275869, 6800373349, 6802615426, 6802615434, 6841165034, 6842708313,
      6842780823, 6844034163, 6875382663, 6926399661
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 22 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 18 exceeds limit of 16', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'f...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 26 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 17 exceeds limit of 16', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 52 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'r...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 21 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 121 exceeds limit of 120', 'policy': 'referrer_guard_rail_policy', 'ref..."
    ],
    "title": "Snuba query rate limit exceeded across multiple APIs",
    "description": "Various Sentry API endpoints and background tasks are hitting rate limits when querying Snuba for tag data, release information, and user statistics, causing failures in issue listing, serialization, and notification delivery.",
    "tags": ["External System", "Rate Limiting", "API", "Snuba", "Query Throttled"],
    "cluster_size": 39,
    "cluster_min_similarity": 0.9451203793624962,
    "cluster_avg_similarity": 0.9669186880229976
  },
  {
    "cluster_id": 52,
    "project_ids": [],
    "group_ids": [6360138421, 6972399590],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "Django PostgreSQL connection failure during request",
    "description": "Django application is failing to establish new PostgreSQL database connections during request processing, causing requests to fail at the database connection level.",
    "tags": ["Database", "Networking", "Django", "PostgreSQL", "Connection Reset"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9911891163289722,
    "cluster_avg_similarity": 0.9911891163289722
  },
  {
    "cluster_id": 55,
    "project_ids": [],
    "group_ids": [6457005475, 6613139051, 6673722701, 6923332877],
    "issue_titles": [
      "UnqualifiedQueryError: Validation failed for entity search_issues: missing required conditions for project_id",
      "UnqualifiedQueryError: Validation failed for entity events: missing required conditions for project_id"
    ],
    "title": "Snuba queries missing project_id condition",
    "description": "Group statistics queries fail when project_id inference returns empty results, causing unqualified queries to be sent to Snuba without required project_id conditions.",
    "tags": ["Database", "Input Validation", "Snuba", "UnqualifiedQueryError"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9458605813783012,
    "cluster_avg_similarity": 0.9598476709598488
  },
  {
    "cluster_id": 63,
    "project_ids": [],
    "group_ids": [
      6587578534, 6707919991, 6837754302, 6870304852, 6887933835, 7004545656, 7015879866
    ],
    "issue_titles": [
      "DecodeError: Error parsing message",
      "RetryException: Could not successfully execute <function query_trace.<locals>.<lambda> at 0x7d325d324540> within 273.113 seconds (7 attempts.)"
    ],
    "title": "Snuba RPC protobuf parsing fails on 503 responses",
    "description": "When Snuba returns HTTP 503 Service Unavailable, the error handler tries to parse the response body as a protobuf ErrorProto message, but 503 responses typically contain HTML or plain text instead of valid protobuf data.",
    "tags": [
      "API",
      "External System",
      "Serialization",
      "Snuba",
      "Protobuf",
      "Upstream Unavailable"
    ],
    "cluster_size": 7,
    "cluster_min_similarity": 0.945865040214269,
    "cluster_avg_similarity": 0.9704579234292839
  },
  {
    "cluster_id": 64,
    "project_ids": [],
    "group_ids": [
      6591547772, 6712676758, 6712677141, 6729521164, 6775572003, 6794932421, 6995214673,
      6995215168, 6999513150, 6999514210
    ],
    "issue_titles": [
      "InvalidSearchQuery: Invalid value '['CHRONICLER-10J']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['HW-ADMIN-JS-ANX']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['AWAY-RESORTS-WEBSITE-QJ']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['NODE-UI-2KF']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['ORCA-HZ', 'ORCA-HN', 'ORCA-M6', 'ORCA-2QG', 'ORCA-6N1', 'ORCA-43D', 'ORCA-831']' for 'issue:' filter"
    ],
    "title": "Query subscription fails on non-existent group reference",
    "description": "Alert subscription queries are failing because they reference groups that no longer exist in the database, causing InvalidSearchQuery errors during query processing and subscription creation.",
    "tags": [
      "Database",
      "Data Integrity",
      "Queueing",
      "Query Validation",
      "Stale Reference"
    ],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9320715070797296,
    "cluster_avg_similarity": 0.9584978121517278
  },
  {
    "cluster_id": 65,
    "project_ids": [],
    "group_ids": [6591559046, 6895752021, 6989208923, 7004807773],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.unfurl)"
    ],
    "title": "Slack API failures from malformed block parameters",
    "description": "The Slack SDK is receiving malformed block or attachment parameters, causing the Slack API to return 503 errors with HTML responses instead of valid JSON, resulting in SlackApiError exceptions across multiple notification types.",
    "tags": [
      "External System",
      "API",
      "Slack",
      "Serialization",
      "Parameter Type Mismatch"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9295664666874326,
    "cluster_avg_similarity": 0.9518740620696577
  },
  {
    "cluster_id": 66,
    "project_ids": [],
    "group_ids": [6592963812, 6879549535],
    "issue_titles": [
      "ApiUnauthorized: {\"message\":\"401 Unauthorized\"}",
      "IdentityNotValid"
    ],
    "title": "GitLab OAuth token refresh fails with missing client_secret",
    "description": "OAuth token refresh requests to GitLab are failing because the identity data is missing the required client_secret parameter, causing GitLab to return 400 Bad Request and resulting in IdentityNotValid exceptions.",
    "tags": [
      "Authentication",
      "External System",
      "OAuth",
      "GitLab",
      "Missing Configuration"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9724184236031654,
    "cluster_avg_similarity": 0.9724184236031654
  },
  {
    "cluster_id": 68,
    "project_ids": [],
    "group_ids": [6603108012, 6619447005, 6619447007, 6783693043, 6933429386, 6933429606],
    "issue_titles": [
      "QueryOutsideRetentionError: Invalid date range. Please try a more recent date range."
    ],
    "title": "Query outside retention period in event retrieval",
    "description": "API endpoints for event details, replay events, and project events are failing because queries reference date ranges that fall outside the configured data retention period.",
    "tags": ["API", "Data Integrity", "Configuration", "Query Outside Retention"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9625022455149055,
    "cluster_avg_similarity": 0.970550711077729
  },
  {
    "cluster_id": 69,
    "project_ids": [],
    "group_ids": [6603108054, 6614051917],
    "issue_titles": [
      "JSONDecodeError: Input is a zero-length, empty document: line 1 column 1 (char 0)"
    ],
    "title": "MS Teams parser fails parsing request body JSON",
    "description": "The MS Teams integration middleware is encountering errors when attempting to parse incoming request body data using orjson.loads(), indicating malformed or invalid JSON payloads from MS Teams webhooks.",
    "tags": ["API", "External System", "Serialization", "MS Teams", "JSON Parse Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9861253340217798,
    "cluster_avg_similarity": 0.9861253340217798
  },
  {
    "cluster_id": 70,
    "project_ids": [],
    "group_ids": [6603110796, 6614836650],
    "issue_titles": ["KeyError: 'HTTP_X_GITLAB_TOKEN'"],
    "title": "GitLab webhook missing required token header",
    "description": "GitLab webhook processing fails when HTTP_X_GITLAB_TOKEN header is missing from requests, often due to misconfigured webhooks being sent to wrong endpoints.",
    "tags": ["API", "External System", "Input Validation", "GitLab", "Missing Header"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9632559951922605,
    "cluster_avg_similarity": 0.9632559951922605
  },
  {
    "cluster_id": 71,
    "project_ids": [],
    "group_ids": [6603111918, 6603259068],
    "issue_titles": ["AtlassianConnectValidationError: Signature is invalid"],
    "title": "Jira JWT signature verification fails due to secret mismatch",
    "description": "JWT tokens from Jira integrations are failing signature verification because the shared secret used by Jira to sign tokens no longer matches the secret stored in Sentry's integration metadata, likely due to secret rotation or reinstallation.",
    "tags": [
      "External System",
      "Authentication",
      "Security",
      "Jira",
      "JWT",
      "Signature Verification Failed"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9625565983582225,
    "cluster_avg_similarity": 0.9625565983582225
  },
  {
    "cluster_id": 72,
    "project_ids": [],
    "group_ids": [6603118689, 6623441649],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Unknown function failure_rate: While processing `tags.value`[indexOf(`tags.key`, 'product')] AS `_snuba_tags[product]`, `_snuba_tags[product]`, failure_rate() AS _snuba_failure_rate. Stack trace:",
      "QueryExecutionError: DB::Exception: Unknown function failure_rate: While processing toDateTime(intDiv(toUInt32(timestamp), 1800) * 1800, 'Universal') AS _snuba_time, count() / (1800 / 60) AS _snuba_epm, failure_rate() AS _snuba_failure_rate. Stack trace:"
    ],
    "title": "ClickHouse aggregation error in events stats query",
    "description": "ORDER BY clause contains auto-generated column alias that's not in GROUP BY when sorting by project with aggregations present. The orderby resolver fails to properly match project columns between SELECT and ORDER BY clauses.",
    "tags": ["Database", "API", "ClickHouse", "Query Execution Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9618477734606081,
    "cluster_avg_similarity": 0.9618477734606081
  },
  {
    "cluster_id": 73,
    "project_ids": [],
    "group_ids": [6603120581, 6613705158],
    "issue_titles": ["ValueError: too many values to unpack (expected 3)"],
    "title": "GitLab webhook token parsing fails with custom ports",
    "description": "GitLab webhook integration fails when parsing tokens from instances using non-standard ports, as the hostname contains colons that break the expected 3-part token format during unpacking.",
    "tags": ["External System", "Input Validation", "GitLab", "Token Parsing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9818045182488181,
    "cluster_avg_similarity": 0.9818045182488181
  },
  {
    "cluster_id": 75,
    "project_ids": [],
    "group_ids": [
      6603148180, 6619019742, 6655096517, 6678435574, 6710520999, 6725352853, 6725386635,
      6725421827, 6725487246, 6725487270, 6725517699, 6725580573, 6725580735, 6725665272,
      6725691300, 6775316556, 6793011113, 6796173506, 7020208264
    ],
    "issue_titles": [
      "SubscriptionError: Invalid value '['CLOUD-FUNCTIONS-B90']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['V2XHUB-1C']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['PROD-SONGWHIP-LOOKUP-2Z']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['WXG-TABLET-CLIENT-559']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['SILVERFIN-2QJC']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['TRACKER-V2-106']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['FRONTEND-Q09']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['NEW-ORDER-SERVICE-EY']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['OCTOPUS-WINDY-BACKEND-2S']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['NODE-UI-2KF']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['SAFE365-77']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['HW-ADMIN-JS-ANX']' for 'issue:' filter",
      "SubscriptionError: Invalid value '['CHATFORM-1G5']' for 'issue:' filter",
      "InvalidSearchQuery: Invalid value '['SENTRY-14ZS']' for 'issue:' filter"
    ],
    "title": "Group lookup fails in incident processing pipeline",
    "description": "Query subscription processing and task workers are encountering Group.DoesNotExist exceptions when attempting to resolve qualified short IDs during aggregation value calculations and issue filtering.",
    "tags": ["Database", "Data Integrity", "Messaging", "Group Not Found"],
    "cluster_size": 19,
    "cluster_min_similarity": 0.933657170249304,
    "cluster_avg_similarity": 0.9651025907658841
  },
  {
    "cluster_id": 76,
    "project_ids": [],
    "group_ids": [6603150949, 6710256302],
    "issue_titles": ["Http404", "KeyError: 'id'"],
    "title": "GitLab webhook missing project field in payload",
    "description": "GitLab webhook payloads are missing the required 'project' object, likely due to a custom webhook template configuration that excludes standard fields. The webhook handler attempts to access event['project']['id'] and raises Http404 when the field is absent.",
    "tags": ["External System", "Input Validation", "GitLab", "Configuration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9638499663992728,
    "cluster_avg_similarity": 0.9638499663992728
  },
  {
    "cluster_id": 77,
    "project_ids": [],
    "group_ids": [6603213168, 6779128558, 6793457428, 6793463459, 6876203029],
    "issue_titles": [
      "UnsupportedResponseType: text/html; charset=utf-8",
      "SentryAppSentryError: Something went wrong while preparing to get Select FormField options"
    ],
    "title": "JSON parsing fails on HTML response from external apps",
    "description": "External Sentry app services are returning HTML responses (likely redirects or error pages) instead of expected JSON, causing JSON decode errors when parsing responses.",
    "tags": [
      "API",
      "External System",
      "Serialization",
      "Input Validation",
      "JSON Decode Error"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9170572047306744,
    "cluster_avg_similarity": 0.9496496209168507
  },
  {
    "cluster_id": 81,
    "project_ids": [],
    "group_ids": [6612807396, 6620320653, 6620861558],
    "issue_titles": [
      "AtlassianConnectValidationError: No integration found",
      "AtlassianConnectValidationError: No token parameter"
    ],
    "title": "Jira integration missing JWT token for issue requests",
    "description": "Atlassian Connect validation is failing because requests to Jira issue endpoints are arriving without required JWT tokens, likely due to descriptor configuration mismatch after recent endpoint changes.",
    "tags": ["External System", "Authentication", "API", "Jira", "Missing Token"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.958185206191107,
    "cluster_avg_similarity": 0.9670913421613937
  },
  {
    "cluster_id": 83,
    "project_ids": [],
    "group_ids": [6612844028, 6738967728, 7001617188],
    "issue_titles": [
      "UnqualifiedQueryError: Validation failed for entity search_issues: Tag keys (flags_key) not resolved",
      "UnqualifiedQueryError: Validation failed for entity events: Tag keys (tags[mac catalyst app.name]) not resolved",
      "UnqualifiedQueryError: Validation failed for entity events: Tag keys (span_op_breakdowns) not resolved"
    ],
    "title": "Snuba queries fail on invalid tag keys with spaces",
    "description": "Tag keys containing spaces (e.g., 'mac catalyst app.name') are causing Snuba validation failures when querying for empty value counts, as Snuba's validator rejects tag keys with spaces as invalid.",
    "tags": ["External System", "Input Validation", "Snuba", "UnqualifiedQueryError"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9461061194213879,
    "cluster_avg_similarity": 0.9495578499492577
  },
  {
    "cluster_id": 88,
    "project_ids": [],
    "group_ids": [6613380217, 6615866385],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Cannot convert string 2025-11-13T11:26:46+00:00 to type DateTime: While processing ((environment AS _snuba_environment) = 'production') AND ((deleted = 0) AND has(_tags_hash_map, cityHash64('application=vendor-details-microfrontend')) AND...",
      "QueryExecutionError: DB::Exception: Cannot convert string 2025-11-11T00:00:00+00:00 to type DateTime: While processing (ifNull(release AS `_snuba_tags[sentry:release]`, '') IN ['com.nvent.olarm@2.0.15+16', 'com.olarm.olarm1@2.0.15+215016']) AND ((deleted = 0) AND (match(mes..."
    ],
    "title": "ClickHouse DateTime conversion fails in top events queries",
    "description": "ISO timestamp strings with timezone info from first query cannot be converted to DateTime type when used in second query filters, causing type mismatch in ClickHouse IN clauses.",
    "tags": ["Database", "Serialization", "ClickHouse", "Type Conversion", "Timezone"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9725463452773428,
    "cluster_avg_similarity": 0.9725463452773428
  },
  {
    "cluster_id": 89,
    "project_ids": [],
    "group_ids": [
      6613478263, 6632499142, 6635924641, 6638414449, 6643954095, 6643954097, 6644183937,
      6666638254, 6677148219, 6683783925, 6705191185, 6712727430, 6718939887, 6726293844,
      6734120945, 6745748294, 6750567715, 6792404555, 6792414712, 6792444893, 6792468106,
      6793495579, 6793982063, 6799535383, 6803845693, 6837753300, 6852720532, 6913434304,
      6915894504, 6920188494, 6921690153, 6921691201, 7016793655
    ],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "Snuba API read timeout during tag value queries",
    "description": "Sentry API requests to Snuba are timing out after 30 seconds when querying tag values across large date ranges. The queries sample 1M rows from events table but lack time range optimizations present in similar tag key queries.",
    "tags": ["Networking", "External System", "API", "Timeout", "Snuba"],
    "cluster_size": 33,
    "cluster_min_similarity": 0.9125569383248453,
    "cluster_avg_similarity": 0.9389028405311234
  },
  {
    "cluster_id": 91,
    "project_ids": [],
    "group_ids": [6613725819, 6616569843],
    "issue_titles": [
      "TypeError: unsupported operand types(s) or combination of types: 'NoneType' and 'str'"
    ],
    "title": "Heroku webhook signature validation fails with missing header",
    "description": "The Heroku release webhook handler attempts to validate signatures using hmac.compare_digest() with a None value when the Heroku-Webhook-Hmac-SHA256 header is missing, causing a TypeError.",
    "tags": [
      "External System",
      "Input Validation",
      "Heroku",
      "HMAC Validation",
      "Missing Header"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9650186402959627,
    "cluster_avg_similarity": 0.9650186402959627
  },
  {
    "cluster_id": 93,
    "project_ids": [],
    "group_ids": [6613950202, 6802477486],
    "issue_titles": ["File.DoesNotExist: File matching query does not exist."],
    "title": "ArtifactBundle post-delete tries to access cascade-deleted File",
    "description": "Django's cascade deletion removes the associated File before the post_delete signal handler executes, causing the handler to fail when trying to access the already-deleted File object.",
    "tags": ["Database", "Django ORM", "Cascade Delete", "Signal Handler", "Foreign Key"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9805802627958158,
    "cluster_avg_similarity": 0.9805802627958158
  },
  {
    "cluster_id": 96,
    "project_ids": [],
    "group_ids": [6614447366, 6615279706, 6696880291, 6734430244, 6841202580],
    "issue_titles": [
      "Repository.MultipleObjectsReturned: get() returned more than one Repository -- it returned 2!",
      "Detector.MultipleObjectsReturned: get() returned more than one Detector -- it returned 2!"
    ],
    "title": "Repository queries return multiple results due to duplicate names",
    "description": "Django ORM queries for repositories by organization and name are failing because multiple repositories exist with identical names but different providers or external IDs, violating the assumption of unique naming within organizations.",
    "tags": ["Database", "Data Integrity", "Django", "Constraint Violation"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9384366688313096,
    "cluster_avg_similarity": 0.9560434063155038
  },
  {
    "cluster_id": 102,
    "project_ids": [],
    "group_ids": [
      6614888006, 6621958996, 6635086777, 6662361418, 6705259139, 6794396371, 6794586191,
      6795338662, 6818156825, 6819785886, 6834638487, 6841165038, 6871496142, 6876832922,
      7015017604
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 2 exceeds limit of 1', 'overrides': {'organization_id__3741...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'p...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'r...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 32 exceeds limit of 30', 'overrides': {}, 'storage_key': 's..."
    ],
    "title": "Snuba concurrent query limit exceeded in bulk requests",
    "description": "Multiple API requests are simultaneously executing bulk Snuba queries with ThreadPoolExecutor, causing the total concurrent queries to exceed Snuba's allocation limit of 18-30 queries.",
    "tags": [
      "External System",
      "Rate Limiting",
      "Concurrency",
      "Snuba",
      "ThreadPoolExecutor"
    ],
    "cluster_size": 15,
    "cluster_min_similarity": 0.9296562416204643,
    "cluster_avg_similarity": 0.9501426079508253
  },
  {
    "cluster_id": 104,
    "project_ids": [],
    "group_ids": [
      6615110650, 6616181936, 6661409875, 6669329931, 6682950309, 6790267383, 6806357457,
      6809137192, 6906089410
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'BytesScannedRejectingPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'project_id 1447267 is over the bytes scanned limit of 150000000000 for referrer repla...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'BytesScannedRejectingPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'project_id 1223698 is over the bytes scanned limit of 320000000000 for referrer tagst...",
      "RateLimitExceeded: Query scanned more than the allocated amount of bytes"
    ],
    "title": "Snuba rate limits missing quota metadata fields",
    "description": "Multiple API endpoints are hitting Snuba's byte scan limits, but the 429 responses lack expected quota_allowance.summary fields, causing generic rate limit exceptions instead of detailed quota information.",
    "tags": ["Rate Limiting", "API", "External System", "Snuba", "Missing Data"],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9341989343201766,
    "cluster_avg_similarity": 0.9564028461495767
  },
  {
    "cluster_id": 108,
    "project_ids": [],
    "group_ids": [
      6615425161, 6623877702, 6643958561, 6643995916, 6659145162, 6659148952, 6659357448,
      6659410713, 6665586681, 6667514267, 6668404896, 6671504118, 6671993133, 6672965611,
      6672965735, 6672965958, 6672965965, 6672966357, 6673109215, 6673723150, 6673723306,
      6673723357, 6673723510, 6673723648, 6674926170, 6675538863, 6675539264, 6675905688,
      6678002632, 6684476185, 6689654993, 6693844110, 6696789901, 6696789902, 6696789961,
      6712322036, 6712727452, 6713083961, 6713162944, 6717462080, 6719467316, 6725404370,
      6725796943, 6726293839, 6752741393, 6783281778, 6786887317, 6788631847, 6792415595,
      6792442851, 6792468318, 6792469745, 6792948230, 6793495560, 6797461617, 6802818968,
      6803204984, 6803205131, 6806423177, 6812736658, 6857524795, 6867024086, 6905209336,
      6912902777, 6912905641, 6936293528
    ],
    "issue_titles": [
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)",
      "ReadTimeoutError: HTTPConnectionPool(host='vroom', port=80): Read timed out. (read timeout=15)",
      "IncompatibleMetricsQuery: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "HTTP read timeout from Snuba despite fast query",
    "description": "Network latency between Sentry and Snuba is causing HTTP response read timeouts after 30 seconds, even though Snuba completes queries in under 200ms. The timeout occurs during response body transmission rather than query execution.",
    "tags": ["Networking", "External System", "Snuba", "Read Timeout"],
    "cluster_size": 66,
    "cluster_min_similarity": 0.946500981055877,
    "cluster_avg_similarity": 0.9724636212669673
  },
  {
    "cluster_id": 113,
    "project_ids": [],
    "group_ids": [
      6616732997, 6689732332, 6689732381, 6689739785, 6689740124, 6689740141, 6689740219,
      6689740250, 6729690539, 7009473391, 7009473493, 7009473512, 7009473538, 7009542016,
      7009542028, 7009542098, 7009542126, 7009542138, 7009542195, 7009542237, 7009542277,
      7009542292, 7009542298, 7009542335, 7009542564, 7009583196, 7009583236, 7009583249,
      7009583280, 7009583300, 7009583345, 7009583353, 7009583386, 7009583400, 7009583402,
      7009583425, 7009583426, 7009583682
    ],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth callback error parameter injection vulnerability",
    "description": "Untrusted OAuth error parameters from callbacks are being passed directly to error messages and logs without sanitization, allowing attackers to inject malicious content through crafted OAuth redirect URLs.",
    "tags": [
      "Security",
      "Input Validation",
      "OAuth",
      "Authentication",
      "Injection Attack"
    ],
    "cluster_size": 38,
    "cluster_min_similarity": 0.9346882734520711,
    "cluster_avg_similarity": 0.9670373233673735
  },
  {
    "cluster_id": 118,
    "project_ids": [],
    "group_ids": [6618594859, 6701639376],
    "issue_titles": ["AssertionError"],
    "title": "SAML/OAuth SSO invite acceptance assertion failure",
    "description": "SSO login pipelines fail during invite acceptance due to a race condition where users are detected as already having organization membership after an invite was approved, causing accept_invite() to return None when a non-null value is expected.",
    "tags": [
      "Authentication",
      "Concurrency",
      "Django",
      "SAML",
      "OAuth",
      "Assertion Error"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9681982815668188,
    "cluster_avg_similarity": 0.9681982815668188
  },
  {
    "cluster_id": 119,
    "project_ids": [],
    "group_ids": [6618877476, 6724510724, 6867384936],
    "issue_titles": ["SnubaRPCError: code: 408"],
    "title": "Snuba RPC timeout on high accuracy spans queries",
    "description": "Large time range queries with HIGHEST_ACCURACY sampling mode are exceeding Snuba's timeout limits when scanning spans data. Snuba recommends using normal mode or shorter time periods for such queries.",
    "tags": ["External System", "Queueing", "Snuba", "Timeout", "High Accuracy Query"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9671115234278839,
    "cluster_avg_similarity": 0.9725889347395588
  },
  {
    "cluster_id": 122,
    "project_ids": [],
    "group_ids": [
      6623445231, 6657556453, 6668503039, 6675806277, 6675836331, 6676162528, 6697003643,
      6708645866, 6748168934, 6789103509, 6792160715, 6794699090, 6794929605, 6796441705,
      6796492717, 6839216422, 6849461741, 6849461805, 6849737978, 6901424380, 6933245202,
      7016209597
    ],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79d3e182fad0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c9ff4192e70>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ca060457650>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d95dd89e570>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /search_issues/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7afa2849f2f0>: Failed to establish a new connection: [Errno 111] Connection r...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2eb862dd90>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a4808648290>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /rpc/EndpointTraceItemTable/v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7affa0660ef0>: Failed to establish a new connection: [Errno 111] C...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /generic_metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6e75195130>: Failed to establish a new connection: [Errno 111] Connection...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe822bb9fd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /metrics/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c078876a7b0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f86901d8710>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /discover/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d17014b7d10>: Failed to establish a new connection: [Errno 111] Connection refuse...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b18d0697650>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /rpc/EndpointTimeSeries/v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x781aec51e8d0>: Failed to establish a new connection: [Errno 111] Conne...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7afff45fd5b0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e97ac708170>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d427c60f770>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /discover/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a2ebbb68290>: Failed to establish a new connection: [Errno 111] Connection refuse...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7adfa4656b10>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bbebef671d0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaRPCError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /rpc/EndpointTraceItemTable/v1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7affc03eac30>: Failed to establish a new connection: [Errno 111] C..."
    ],
    "title": "Snuba service connection refused during notifications",
    "description": "Multiple components including Slack notifications and task workers are failing to connect to the Snuba service at snuba-api:80, causing API endpoints and background processing to fail when attempting to query replay counts and analytics data.",
    "tags": ["Networking", "External System", "Connection Reset", "Snuba", "Sentry"],
    "cluster_size": 22,
    "cluster_min_similarity": 0.9342703866879823,
    "cluster_avg_similarity": 0.9643167669580098
  },
  {
    "cluster_id": 126,
    "project_ids": [],
    "group_ids": [6624154292, 6672775391],
    "issue_titles": ["IndexError: list index out of range"],
    "title": "Redis rate limit retrieval failures in Sentry",
    "description": "Rate limiting operations are failing when attempting to retrieve current limit values from Redis, affecting both monitor check-in processing and HTTP request rate limiting.",
    "tags": ["Rate Limiting", "Caching", "Redis", "External System"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9571044581153365,
    "cluster_avg_similarity": 0.9571044581153365
  },
  {
    "cluster_id": 128,
    "project_ids": [],
    "group_ids": [6626382005, 6703117189, 6736513829, 6776183779, 6781511608, 6866171483],
    "issue_titles": [
      "NotImplementedError: Haven't handled all the search expressions yet"
    ],
    "title": "EAP search resolver fails on unsupported query syntax",
    "description": "The Event Analytics Platform search query resolver encounters unsupported search expressions and raises NotImplementedError instead of proper validation errors, causing API endpoints to fail when processing certain query patterns.",
    "tags": ["API", "Input Validation", "Search Query", "NotImplementedError"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9322374792855541,
    "cluster_avg_similarity": 0.9529975946162358
  },
  {
    "cluster_id": 129,
    "project_ids": [],
    "group_ids": [6626412431, 6686967912, 6925768396],
    "issue_titles": [
      "QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_4' in table 'events': While processing events._snuba_gen_4 AS _snuba_gen_4: While processing SELECT count() AS _snuba_count FROM (SELECT match(message AS `_snuba_events.message`, '(?i).*Timeout\\\\ error...",
      "QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_2' in table 'events': While processing events._snuba_gen_2 AS _snuba_gen_2: While processing SELECT events.`_snuba_issue.id` AS `_snuba_issue.id`, count() AS _snuba_count, uniq(events.`_snuba_events.ta...",
      "QueryMissingColumn: DB::Exception: There's no column 'events._snuba_gen_2' in table 'events': While processing events._snuba_gen_2 AS _snuba_gen_2: While processing SELECT events.`_snuba_events.time` AS `_snuba_events.time`, count() AS _snuba_count FROM (SELECT toStartOfHo..."
    ],
    "title": "ClickHouse query fails on missing generated column in subquery",
    "description": "Snuba's subquery generator creates temporary column aliases for OR conditions spanning multiple entities but fails to add them to the correct subquery SELECT clauses, causing ClickHouse to error on missing columns like '_snuba_gen_3'.",
    "tags": ["Database", "Query Generation", "ClickHouse", "Subquery Processing"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9619455460537615,
    "cluster_avg_similarity": 0.9670337892742588
  },
  {
    "cluster_id": 132,
    "project_ids": [],
    "group_ids": [
      6640678616, 7022187299, 7022187303, 7022535390, 7022535393, 7022535577, 7022535673,
      7022535746, 7022535775, 7022535783, 7022535804, 7022550551, 7022550558
    ],
    "issue_titles": [
      "QueryExecutionError: Connection reset by peer (10.0.0.1:9016)",
      "QueryExecutionError: Connection reset by peer (10.0.0.1:9010)"
    ],
    "title": "ClickHouse query execution failures across multiple endpoints",
    "description": "Multiple Sentry API endpoints and background tasks are experiencing query execution failures when communicating with ClickHouse, affecting event stats, replay data, and group serialization operations.",
    "tags": ["Database", "External System", "ClickHouse", "Query Execution Error"],
    "cluster_size": 13,
    "cluster_min_similarity": 0.9555585166689037,
    "cluster_avg_similarity": 0.970709729782545
  },
  {
    "cluster_id": 137,
    "project_ids": [],
    "group_ids": [
      6646074554, 6674161870, 6683949171, 6684066154, 6713625898, 6719906014, 6792744042,
      6802764539, 6948382488, 6977020312
    ],
    "issue_titles": [
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:1224075:Member:1868843:ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:1224075:Member:1868843:ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509042865209424:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509042865209424:IssueOwners::ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4504849057775616:Member:2745935:ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4504849057775616:Member:2745935:ActiveMembers'",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509276312961104:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509276312961104:IssueOwners::ActiveMembers'",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'spend_allocations.record_consumption:4507644678569984.1'>> within 4.978 seconds (48 attempts.)",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'subscription:uptime_monitor:cff5cebc9ce84b368ca9a7d41bc15487'>> within 9.909 seconds (70 attempts.)",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509729370603600:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509729370603600:IssueOwners::ActiveMembers'",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'deploy-notify:95172894'>> within 9.966 seconds (71 attempts.)",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'usagebuffer.usage_flush_lock:4508357979930704'>> within 4.931 seconds (52 attempts.)",
      "UnableToAcquireLock: Unable to acquire <Lock: 'd:t:mail:p:4509810457313280:IssueOwners::ActiveMembers'> due to error: Could not set key: 'l:d:t:mail:p:4509810457313280:IssueOwners::ActiveMembers'"
    ],
    "title": "Redis lock acquisition fails in task worker processes",
    "description": "Multiple worker processes are unable to acquire Redis locks for the same key, causing task failures after retry exhaustion. This appears to be related to concurrent processing attempting to acquire locks with different UUIDs for the same resource.",
    "tags": ["Concurrency", "Caching", "Redis", "Lock Contention", "Multiprocessing"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9185392110802302,
    "cluster_avg_similarity": 0.9485954678309442
  },
  {
    "cluster_id": 149,
    "project_ids": [],
    "group_ids": [6656669874, 6795629190, 7024656370],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/conversations.info)"
    ],
    "title": "Slack API error during channel validation for alerts",
    "description": "Alert rule configuration is failing when validating Slack channel IDs through the Slack API conversations.info endpoint, causing task worker failures.",
    "tags": ["External System", "API", "Slack", "Input Validation"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9731966515138232,
    "cluster_avg_similarity": 0.9775293442635998
  },
  {
    "cluster_id": 151,
    "project_ids": [],
    "group_ids": [6656884667, 7005277326, 7005538229, 7024562390, 7024592603],
    "issue_titles": [
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-1-2:9000. DB::Exception: Memory limit (total) exceeded: would use 56.66 GiB (attempt to allocate chunk of 0 bytes), maximum: 56.53 GiB. OvercommitTracker decision: Memory overcommit has freed not enough...",
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-1-6:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.43 GiB (attempt to allocate chunk of 1073741824 bytes), maximum: 9.31 GiB.: While executing ReplacingSorted. Stack trace:",
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-2-5:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.32 GiB (attempt to allocate chunk of 4728496 bytes), maximum: 9.31 GiB.: (while reading column tags.value): (while reading from pa...",
      "QueryMemoryLimitExceeded: DB::Exception: Memory limit (for query) exceeded: would use 9.31 GiB (attempt to allocate chunk of 4316704 bytes), maximum: 9.31 GiB.: (avg_value_size_hint = 0, avg_chars_size = 1, limit = 93491): while receiving packet from snuba-errors-tiger-mz-2-3:90...",
      "QueryMemoryLimitExceeded: DB::Exception: Received from snuba-errors-tiger-mz-2-5:9000. DB::Exception: Memory limit (for query) exceeded: would use 9.67 GiB (attempt to allocate chunk of 1073740336 bytes), maximum: 9.31 GiB.. Stack trace:"
    ],
    "title": "ClickHouse memory limit exceeded during Snuba queries",
    "description": "Sentry API endpoints are running inefficient ClickHouse queries that consume excessive memory. The queries use memory-intensive aggregations like argMax() on boolean expressions or count empty values across many tag keys simultaneously, causing ClickHouse to exceed its memory limits during query execution.",
    "tags": ["Database", "API", "ClickHouse", "Memory", "Query Execution Error"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9430620444205482,
    "cluster_avg_similarity": 0.9578113745498109
  },
  {
    "cluster_id": 152,
    "project_ids": [],
    "group_ids": [6657194570, 6881767222, 6912693767],
    "issue_titles": [
      "KeyError: <ExternalProviders.DISCORD: 140>",
      "KeyError: <ExternalProviders.MSTEAMS: 120>"
    ],
    "title": "Notification system failures in messaging and digest delivery",
    "description": "Multiple notification pathways are failing including nudge notifications from messaging integrations and digest delivery tasks. Failures occur in the notification registry during provider-specific notification dispatch.",
    "tags": ["Messaging", "External System", "Notification Provider", "Task Worker"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9673104708669651,
    "cluster_avg_similarity": 0.9766964574786896
  },
  {
    "cluster_id": 164,
    "project_ids": [],
    "group_ids": [6667014357, 6672210447],
    "issue_titles": ["Group.DoesNotExist: Group matching query does not exist."],
    "title": "Event group lookup fails after group deletion",
    "description": "Events retrieved from Snuba reference group IDs that no longer exist in the database due to a race condition between group deletion and event retrieval. The lazy-loading group property triggers DoesNotExist exceptions instead of handling missing groups gracefully.",
    "tags": [
      "Database",
      "Data Integrity",
      "Django",
      "Group DoesNotExist",
      "Race Condition"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9642513272729807,
    "cluster_avg_similarity": 0.9642513272729807
  },
  {
    "cluster_id": 173,
    "project_ids": [],
    "group_ids": [6672140751, 6672140772, 7021565235],
    "issue_titles": [
      "InvalidReleaseErrorBadCharacters: invalid release: bad characters in release name"
    ],
    "title": "Semver parsing fails on invalid release characters",
    "description": "Users submitting search queries with release versions containing invalid characters (like backslashes) cause the Rust release parser to fail due to insufficient input validation in the search filter pipeline.",
    "tags": ["Input Validation", "API", "Serialization", "Rust FFI", "Search Filter"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9572849400758013,
    "cluster_avg_similarity": 0.9704944965768115
  },
  {
    "cluster_id": 178,
    "project_ids": [],
    "group_ids": [6672345079, 6797692325, 6843213166],
    "issue_titles": [
      "AvataxException: StringLengthError: \"Field 'postalCode' has an invalid length.\""
    ],
    "title": "Avalara tax calculation fails with invalid zip/state",
    "description": "Billing addresses with invalid postal code and region combinations are being sent to Avalara without validation, causing tax calculation failures during subscription operations.",
    "tags": [
      "External System",
      "Input Validation",
      "Avalara",
      "Tax Calculation",
      "Address Validation"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9577176943726587,
    "cluster_avg_similarity": 0.9639683301043599
  },
  {
    "cluster_id": 196,
    "project_ids": [],
    "group_ids": [
      6673053586, 6675782295, 6675804049, 6770410453, 6858111400, 6867998872, 6868960925,
      6869388543
    ],
    "issue_titles": [
      "ConnectTimeout: HTTPSConnectionPool(host='api.codecov.io', port=443): Max retries exceeded with url: /webhooks/sentry (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f6c14599d10>, 'Connection to api.codecov.io timed out. (connect timeout...",
      "ReadTimeout: SafeHTTPSConnectionPool(host='api.linear.app', port=443): Read timed out. (read timeout=2.0)",
      "ReadTimeout: HTTPSConnectionPool(host='api.codecov.io', port=443): Read timed out. (read timeout=10)"
    ],
    "title": "Codecov API timeout in SSL read after 10 seconds",
    "description": "HTTP POST requests to api.codecov.io are timing out during SSL read operations, taking over 3 seconds to complete within rate limit middleware spans.",
    "tags": ["Networking", "External System", "TLS", "Timeout", "Codecov"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9342923722095517,
    "cluster_avg_similarity": 0.966488381746338
  },
  {
    "cluster_id": 193,
    "project_ids": [],
    "group_ids": [
      6674717694, 6676748076, 6678805069, 6681907680, 6694640273, 6705876262, 6705876268,
      6705936414, 6709058648, 6711460490, 6712694956, 6741298159, 6757146742, 6778613639,
      6793535338, 6839369059, 6855409643, 6906635231, 6960976910, 6977939695, 7021926784
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 45 seconds exceeded by sentry.integrations.source_code_management.tasks.pr_comment_workflow",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.store.save_event",
      "ProcessingDeadlineExceeded: execution deadline of 120 seconds exceeded by sentry.issues.tasks.post_process.post_process_group",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.profiles.task.process_profile",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.rules.processing.delayed_processing",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.store.save_event_transaction",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.workflow_engine.tasks.trigger_action",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.email.send_email",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.options.sync_options_control",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.integrations.slack.tasks.send_activity_notifications_to_slack_threads",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.tasks.store.process_event",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by getsentry.tasks.run_spike_projection",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.debug_files.tasks.refresh_artifact_bundles_in_use",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.tasks.relay.build_project_config"
    ],
    "title": "Task workers timing out on memcache connections",
    "description": "Multiple task worker processes are exceeding their processing deadlines due to blocking memcache socket operations during cache lookups and writes, caused by network instability or slow memcache server responses.",
    "tags": [
      "Caching",
      "Networking",
      "Processing Deadline",
      "Memcache",
      "Socket Timeout"
    ],
    "cluster_size": 21,
    "cluster_min_similarity": 0.9240736741380786,
    "cluster_avg_similarity": 0.9528875082723267
  },
  {
    "cluster_id": 194,
    "project_ids": [],
    "group_ids": [6674831561, 6809113966, 7013115455],
    "issue_titles": ["TypeError: 'NoneType' object is not iterable"],
    "title": "Email notification fails on null query string encoding",
    "description": "The safe_urlencode utility function lacks null input handling, causing crashes during email notification generation when HTTP interface data contains null query strings from event normalization or datascrubbing.",
    "tags": ["Serialization", "Input Validation", "Email", "Django", "Null Pointer"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9675334400320923,
    "cluster_avg_similarity": 0.9708040177750138
  },
  {
    "cluster_id": 195,
    "project_ids": [],
    "group_ids": [6675539406, 6702482376],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Read timed out. (read timeout=30)"
    ],
    "title": "Snuba timeout with undefined referrer in group attachments",
    "description": "Group attachments endpoint dynamically generates referrers like 'api.group-attachments.error' that are not registered in the Referrer enum, causing Snuba queries to timeout due to improper routing and optimization.",
    "tags": [
      "API",
      "External System",
      "Configuration",
      "Snuba",
      "Timeout",
      "Referrer Validation"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9558929682323996,
    "cluster_avg_similarity": 0.9558929682323996
  },
  {
    "cluster_id": 202,
    "project_ids": [],
    "group_ids": [6677440517, 6794413207, 6844267979],
    "issue_titles": ["NotificationClassNotSetException"],
    "title": "Notification classes not registered in task worker",
    "description": "Task workers fail to send notifications because notification classes are not imported during worker initialization, causing registry lookups to fail.",
    "tags": [
      "Configuration",
      "Queueing",
      "Multiprocessing",
      "Missing Import",
      "Notification System"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9574750805202428,
    "cluster_avg_similarity": 0.9673512289697216
  },
  {
    "cluster_id": 203,
    "project_ids": [],
    "group_ids": [6677784134, 6708941920, 7017527232],
    "issue_titles": ["ValueError: not enough values to unpack (expected 2, got 1)"],
    "title": "GitHub integration fails on malformed issue key",
    "description": "An ExternalIssue record with malformed key '3778' causes GitHub integration to crash during URL generation, as it expects 'repo#issue_id' format but encounters only an issue number.",
    "tags": ["External System", "Data Integrity", "GitHub", "Input Validation"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9440992001921995,
    "cluster_avg_similarity": 0.9534970693516982
  },
  {
    "cluster_id": 207,
    "project_ids": [],
    "group_ids": [6678435494, 6689961421, 6689961440, 6793345628, 6923750134],
    "issue_titles": [
      "SubscriptionError: Cannot query apdex with a threshold parameter on the metrics dataset",
      "ApiError: status=400 body={'detail': ErrorDetail(string='Cannot query apdex with a threshold parameter on the metrics dataset', code='parse_error')}"
    ],
    "title": "Apdex function with threshold unsupported on metrics dataset",
    "description": "Alert rules using apdex(threshold) functions fail during subscription creation because the metrics dataset doesn't support apdex with threshold parameters, despite allowing rule creation.",
    "tags": ["Input Validation", "API", "Snuba", "Incompatible Metrics Query"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9337138795862563,
    "cluster_avg_similarity": 0.9569771621813832
  },
  {
    "cluster_id": 208,
    "project_ids": [],
    "group_ids": [
      6678435496, 6678435554, 6678537705, 6678537737, 6689961427, 6689961448, 6746480180,
      6746480185, 6746480186, 6746480190, 6775280372, 6775316673, 6775384336, 6775486526,
      6783126514, 6793345667, 6793375009, 6798177485, 6799524941, 6805712233, 6811977041,
      6825413890, 6825426177, 6830365844, 6849762058
    ],
    "issue_titles": [
      "SubscriptionError: se is not a tag in the metrics dataset",
      "SubscriptionError: http.url is not a tag in the metrics dataset",
      "SubscriptionError: transaction.duration is not a tag in the metrics dataset",
      "SubscriptionError: sdk.name is not a tag in the metrics dataset",
      "SubscriptionError: customerType is not a tag in the metrics dataset",
      "IncompatibleMetricsQuery: SITE_DOMAIN is not a tag in the metrics dataset",
      "SubscriptionError: user.display is not a tag in the metrics dataset",
      "SubscriptionError: url is not a tag in the metrics dataset"
    ],
    "title": "Invalid tag reference in metrics query builder",
    "description": "AlertMetricsQueryBuilder is attempting to resolve a tag key that doesn't exist in the metrics dataset, causing IncompatibleMetricsQuery errors during subscription creation and deletion tasks.",
    "tags": ["Database", "Input Validation", "Snuba", "Schema Mismatch"],
    "cluster_size": 25,
    "cluster_min_similarity": 0.9437419472078276,
    "cluster_avg_similarity": 0.9756234555652039
  },
  {
    "cluster_id": 209,
    "project_ids": [],
    "group_ids": [6678435499, 6678637607, 6689961437, 6792301323],
    "issue_titles": [
      "SubscriptionError: Metric: c:custom/checkout.failed@none could not be resolved",
      "SubscriptionError: Metric: c:custom/business_successfully_fetched@none could not be resolved"
    ],
    "title": "Metric resolution fails in AlertMetricsQueryBuilder",
    "description": "The AlertMetricsQueryBuilder cannot resolve specific metric references during query construction, raising IncompatibleMetricsQuery exceptions in both alert creation and deletion tasks.",
    "tags": ["API", "Input Validation", "Snuba", "Metric Resolution Failed"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9622888068080923,
    "cluster_avg_similarity": 0.971177849299332
  },
  {
    "cluster_id": 210,
    "project_ids": [],
    "group_ids": [6678435501, 6678570727, 6678570748, 6746480184, 6793537779],
    "issue_titles": [
      "SubscriptionError: release value application.monitoring.javascript@22.2.1 in filter not found",
      "SubscriptionError: release value application.monitoring.javascript@22.5.5 in filter not found",
      "SubscriptionError: release value com.example.vu.android@2.10.4+43 in filter not found"
    ],
    "title": "Metrics query fails on project-scoped release filter",
    "description": "Alert subscriptions fail when using release filters with project-scoped format because the metrics indexer only stores raw release versions, not the expanded project@version strings generated by parse_release().",
    "tags": ["API", "Input Validation", "Data Integrity", "Incompatible Metrics Query"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9392875958486704,
    "cluster_avg_similarity": 0.9619623612199446
  },
  {
    "cluster_id": 211,
    "project_ids": [],
    "group_ids": [6678511610, 6725560157, 6791455908],
    "issue_titles": ["SentryAppSentryError: event_not_in_servicehook"],
    "title": "Sentry App webhook fails for unsupported event types",
    "description": "Sentry Apps subscribed to some issue events are incorrectly sent webhooks for unsupported event types like 'issue.unresolved', causing validation failures when the specific event is not in their servicehook events list.",
    "tags": ["API", "Configuration", "Sentry App", "Event Filtering", "Validation Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9573904943442025,
    "cluster_avg_similarity": 0.9643314534740357
  },
  {
    "cluster_id": 212,
    "project_ids": [],
    "group_ids": [6678537740, 6678537779, 6678637575, 6678637623, 6797847220, 6798177476],
    "issue_titles": [
      "SubscriptionError: Environment: Production was not found",
      "SubscriptionError: Environment: PRD was not found"
    ],
    "title": "Metrics query failing on unknown environment filter",
    "description": "AlertMetricsQueryBuilder is throwing IncompatibleMetricsQuery exceptions when trying to resolve environment filter values that don't exist in the metrics system.",
    "tags": [
      "Input Validation",
      "Configuration",
      "Metrics",
      "Environment Filter",
      "Query Builder"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9691418657759105,
    "cluster_avg_similarity": 0.9778573499696441
  },
  {
    "cluster_id": 213,
    "project_ids": [],
    "group_ids": [6678537766, 6793468098, 6797823141],
    "issue_titles": [
      "SubscriptionError: Invalid query. Project(s) application-monitoring-springboot do not exist or are not actively selected."
    ],
    "title": "Invalid project slug search query in Snuba tasks",
    "description": "Task workers are failing when building query builders for Snuba subscriptions due to invalid project slug filters in search queries.",
    "tags": ["Input Validation", "Queueing", "API", "Invalid Search Query", "Snuba"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9584114223394279,
    "cluster_avg_similarity": 0.9689728482075514
  },
  {
    "cluster_id": 214,
    "project_ids": [],
    "group_ids": [
      6678565556, 6679003366, 6680339419, 6684972392, 6691278425, 6709185938, 6709373545,
      6721903869, 6722245237, 6725367555, 6725735955, 6731345393, 6732089288, 6801175178,
      6810055918, 7004758163, 7006590963, 7022579794
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.process_suspect_commits",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.auto_resolve_project_issues",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.tasks.code_owners_auto_sync",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.tasks.statistical_detectors.detect_function_trends",
      "ProcessingDeadlineExceeded: execution deadline of 150 seconds exceeded by sentry.integrations.source_code_management.tasks.open_pr_comment_workflow",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.merge.merge_groups",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.hybridcloud.tasks.deliver_webhooks.schedule_webhook_delivery",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by getsentry.tasks.quotas.recalculate_projected_spikes",
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by getsentry.tasks.stats.sync_outdated",
      "ProcessingDeadlineExceeded: execution deadline of 65 seconds exceeded by sentry.dynamic_sampling.tasks.recalibrate_orgs",
      "RetryError",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.check_auth",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.tasks.collect_project_platforms",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.user_report",
      "ProcessingDeadlineExceeded: execution deadline of 905 seconds exceeded by sentry.tasks.commits.fetch_commits"
    ],
    "title": "Task worker processing deadline timeout",
    "description": "Multiple task worker processes are hitting their configured processing deadlines while executing various operations, resulting in ProcessingDeadlineExceeded errors that interrupt database queries and external service calls.",
    "tags": [
      "Resource Limits",
      "Database",
      "External System",
      "Timeout",
      "Task Worker",
      "PostgreSQL"
    ],
    "cluster_size": 18,
    "cluster_min_similarity": 0.9162623358876747,
    "cluster_avg_similarity": 0.940990293064852
  },
  {
    "cluster_id": 219,
    "project_ids": [],
    "group_ids": [6680310665, 6756304596, 6806414039],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 90 seconds exceeded by sentry.tasks.process_commit_context",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.auto_enable_codecov.enable_for_org",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by getsentry.tasks.verify_github_subscriptions"
    ],
    "title": "GitHub API calls timing out in worker processes",
    "description": "Multiple background tasks are hitting processing deadlines while waiting for GitHub API responses, causing worker processes to terminate via alarm handlers.",
    "tags": [
      "External System",
      "Networking",
      "GitHub",
      "Timeout",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.958952367477511,
    "cluster_avg_similarity": 0.9656322334268519
  },
  {
    "cluster_id": 221,
    "project_ids": [],
    "group_ids": [6682915846, 6783517032],
    "issue_titles": [
      "SentryAppSentryError: missing_servicehook",
      "SentryAppSentryError: workflow_notification.missing_installation"
    ],
    "title": "Sentry app webhook failure due to missing service hook",
    "description": "Workflow notification tasks are failing when attempting to send webhooks for Sentry app integrations, with errors indicating missing service hook configuration or webhook data retrieval issues.",
    "tags": ["External System", "Configuration", "Sentry App", "Webhook Failure"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9593610736875872,
    "cluster_avg_similarity": 0.9593610736875872
  },
  {
    "cluster_id": 225,
    "project_ids": [],
    "group_ids": [
      6684018267, 6684120265, 6702927502, 6708494666, 6732056221, 6746767037, 6794594324,
      6815806036, 6872759161
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.middleware.integrations.tasks.convert_to_async_slack_response",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by getsentry.tasks.quotas.react_to_spike_protection",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by getsentry.tasks.quotas._send_reserved_quota_thresholds_notification",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.clear_region_cache",
      "ProcessingDeadlineExceeded: execution deadline of 15 seconds exceeded by sentry.data_export.tasks.merge_blobs",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2"
    ],
    "title": "Task worker RPC calls timing out with deadline exceeded",
    "description": "Task worker processes are exceeding their 30-second processing deadline while making RPC calls to remote silos, with retries consuming the available time window and causing ProcessingDeadlineExceeded errors.",
    "tags": [
      "Networking",
      "RPC",
      "Timeout",
      "Retries Exhausted",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9236332889659905,
    "cluster_avg_similarity": 0.9444499418285339
  },
  {
    "cluster_id": 226,
    "project_ids": [],
    "group_ids": [
      6684093720, 6705867517, 6731215162, 6792300053, 6796631026, 6806291306, 6869524816,
      6879888174, 6879888386, 6882704723, 6975814027
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by getsentry.tasks.quotas.deactivate_db_spike",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.integrations.tasks.sync_status_outbound",
      "HTTPError: 408 Client Error: Request Timeout for url: https://openrouter.ai/api/v1/models",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.rules.processing.delayed_processing",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.workflow_engine.tasks.trigger_action",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by getsentry.integrations.slack.tasks.new_organization_notify",
      "ProcessingDeadlineExceeded: execution deadline of 30 seconds exceeded by sentry.integrations.slack.tasks.send_activity_notifications_to_slack_threads",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.tasks.autofix.check_autofix_status",
      "ProcessingDeadlineExceeded: execution deadline of 60 seconds exceeded by sentry.tasks.code_owners_auto_sync"
    ],
    "title": "Task worker processing deadline timeouts on HTTP calls",
    "description": "Background tasks are exceeding their processing deadlines while making synchronous HTTP requests to external services (Slack API, Seer, internal silos), causing SIGALRM interruptions during socket operations.",
    "tags": [
      "Networking",
      "External System",
      "Resource Limits",
      "Timeout",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 11,
    "cluster_min_similarity": 0.9241117301469641,
    "cluster_avg_similarity": 0.9479924212866656
  },
  {
    "cluster_id": 228,
    "project_ids": [],
    "group_ids": [6684541076, 6792744388, 6794669443],
    "issue_titles": [
      "UnableToAcquireLock: Unable to acquire <Lock: 'queue_comment_task:194480649'> due to error: Could not set key: 'l:queue_comment_task:194480649'",
      "RetryException: Could not successfully execute <bound method Lock.acquire of <Lock: 'deploy-notify:94226451'>> within 9.937 seconds (98 attempts.)",
      "UnableToAcquireLock: Unable to acquire <Lock: 'create_invoices.subscription:2264513'> due to error: Could not set key: 'l:create_invoices.subscription:2264513'"
    ],
    "title": "Redis lock acquisition fails due to return type mismatch",
    "description": "Lock acquisition fails in Redis Cluster environments because the code checks for identity with True, but RedisCluster returns 'OK' string instead of boolean True for successful SET NX operations.",
    "tags": ["Caching", "Concurrency", "Redis", "Type Mismatch"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9658567202040719,
    "cluster_avg_similarity": 0.9691706018673213
  },
  {
    "cluster_id": 232,
    "project_ids": [],
    "group_ids": [
      6685373553, 6697834713, 6793075224, 6812214998, 6812647562, 6854937591, 6984737196,
      6992643012, 6993931393, 7017257930
    ],
    "issue_titles": [
      "Subscription.DoesNotExist: Subscription matching query does not exist.",
      "SubscriptionError: An unexpected database issue occurred while charging this invoice",
      "Environment.DoesNotExist: Environment matching query does not exist.",
      "Organization.DoesNotExist: Organization matching query does not exist."
    ],
    "title": "Django DoesNotExist errors in task worker processes",
    "description": "Multiple background tasks are failing when trying to retrieve database records (Organization and Subscription objects) that no longer exist, indicating data integrity issues between related models.",
    "tags": ["Database", "Data Integrity", "Django", "Queueing", "DoesNotExist"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.909908714188882,
    "cluster_avg_similarity": 0.9469486932776672
  },
  {
    "cluster_id": 233,
    "project_ids": [],
    "group_ids": [
      6686911574, 6767165452, 6804302134, 6807858695, 6810480082, 6810480105, 6905784597,
      6905891548, 6931309529, 6931314341, 6961334799
    ],
    "issue_titles": ["SnubaRPCError: code: 408"],
    "title": "Snuba RPC failures across trace and span queries",
    "description": "Multiple trace-related endpoints are encountering SnubaRPCError when querying spans and trace data through the RPC interface. The failures occur across various endpoints including trace details, logs, metadata, and Seer explorer functionality.",
    "tags": ["External System", "API", "Snuba", "RPC Error"],
    "cluster_size": 11,
    "cluster_min_similarity": 0.947719368509946,
    "cluster_avg_similarity": 0.9693311283378062
  },
  {
    "cluster_id": 234,
    "project_ids": [],
    "group_ids": [
      6688775092, 6707917945, 6710958153, 6755570545, 6837751626, 6841632111, 6870261730,
      6870267192, 6959355998, 7008251292
    ],
    "issue_titles": ["DecodeError: Error parsing message"],
    "title": "Snuba RPC error response parsing failure",
    "description": "Snuba returns HTTP 503 errors with non-protobuf response bodies that cannot be parsed as ErrorProto messages, causing DecodeError in the RPC client.",
    "tags": ["External System", "API", "Serialization", "Snuba", "RPC", "Error Parsing"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9314378096277807,
    "cluster_avg_similarity": 0.9577274814092598
  },
  {
    "cluster_id": 236,
    "project_ids": [],
    "group_ids": [6689961415, 6689961447, 6748982357, 6778300806],
    "issue_titles": [
      "SubscriptionError: transaction.duration is not a tag in the metrics dataset",
      "SubscriptionError: se is not a tag in the metrics dataset",
      "IncompatibleMetricsQuery: SITE_DOMAIN is not a tag in the metrics dataset"
    ],
    "title": "Metrics query validation rejects valid fields as tags",
    "description": "AlertMetricsQueryBuilder incorrectly treats metric fields like transaction.duration as tags when the mep-use-default-tags feature flag is enabled, causing subscription operations to fail.",
    "tags": [
      "API",
      "Configuration",
      "Input Validation",
      "Schema Validation",
      "Incompatible Metrics Query"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9427803844383084,
    "cluster_avg_similarity": 0.954136774991016
  },
  {
    "cluster_id": 240,
    "project_ids": [],
    "group_ids": [6692799304, 6693305837, 6726976190, 6793367935],
    "issue_titles": ["File.DoesNotExist: File matching query does not exist."],
    "title": "Django ORM cache access error in artifact assembly",
    "description": "Multiple task workers are failing when accessing Django model field cache during artifact bundle assembly and indexing operations.",
    "tags": ["Database", "Caching", "Django", "Task Processing"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9655036283826673,
    "cluster_avg_similarity": 0.9765600466266774
  },
  {
    "cluster_id": 242,
    "project_ids": [],
    "group_ids": [6693754863, 6721729431],
    "issue_titles": [
      "GitHubApiError: b'{\\r\\n  \"message\": \"Bad credentials\",\\r\\n  \"documentation_url\": \"https://docs.github.com/rest\",\\r\\n  \"status\": \"401\"\\r\\n}'"
    ],
    "title": "GitHub OAuth API errors during authentication flow",
    "description": "Authentication requests to GitHub API are failing during the OAuth provider login process, affecting user organization membership checks and email retrieval.",
    "tags": ["External System", "Authentication", "API", "GitHub", "OAuth"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.970583433291606,
    "cluster_avg_similarity": 0.970583433291606
  },
  {
    "cluster_id": 245,
    "project_ids": [],
    "group_ids": [6696655082, 6696659077],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 300 seconds exceeded by sentry.hybridcloud.tasks.deliver_webhooks.drain_mailbox",
      "ProcessingDeadlineExceeded: execution deadline of 180 seconds exceeded by sentry.hybridcloud.tasks.deliver_webhooks.drain_mailbox_parallel"
    ],
    "title": "Webhook delivery tasks exceeding processing deadline",
    "description": "Task worker processes delivering webhooks are hitting timeout limits during execution, causing ProcessingDeadlineExceeded errors in both sequential and parallel delivery modes.",
    "tags": ["Queueing", "External System", "Timeout", "Task Worker", "Webhook Delivery"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9568089046542207,
    "cluster_avg_similarity": 0.9568089046542207
  },
  {
    "cluster_id": 248,
    "project_ids": [],
    "group_ids": [6698006459, 6698197464, 6937547120],
    "issue_titles": [
      "Exception: HTTP 400 (admin_policy_enforced): Access to your account data is restricted by policies within your organization. Please contact the administrator of your organization for more information."
    ],
    "title": "OAuth2 identity refresh failures in auth check task",
    "description": "Background task workers are encountering exceptions when attempting to refresh OAuth2 authentication identities, causing auth check operations to fail.",
    "tags": ["Authentication", "External System", "OAuth2", "Task Worker"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9848674340336908,
    "cluster_avg_similarity": 0.9877986205611009
  },
  {
    "cluster_id": 249,
    "project_ids": [],
    "group_ids": [6698211440, 6907395781],
    "issue_titles": ["Exception: Same primary email address for multiple users"],
    "title": "OAuth identity linking fails on duplicate primary emails",
    "description": "Multiple users have the same primary email address with different case variations, causing identity linking to fail during OAuth login flow due to case-sensitivity mismatch between database query and Python comparison.",
    "tags": ["Authentication", "Data Integrity", "Django", "OAuth", "Case Sensitivity"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9772810129632715,
    "cluster_avg_similarity": 0.9772810129632715
  },
  {
    "cluster_id": 250,
    "project_ids": [],
    "group_ids": [
      6702972843, 6703015521, 6703065135, 6703445929, 6704011338, 6704838350, 6723603645
    ],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, ORGAUTHTOKEN_UPDATE_USED due to database error",
      "OutboxDatabaseError: Failed to process Outbox, AUDIT_LOG_EVENT due to database error",
      "OutboxDatabaseError: Failed to process Outbox, SUBSCRIPTION_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, USER_IP_EVENT due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORGANIZATION_INTEGRATION_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, API_TOKEN_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORG_AUTH_TOKEN_UPDATE due to database error"
    ],
    "title": "Outbox processing task fails with database errors",
    "description": "Task workers processing hybridcloud outbox shards are encountering database execution failures during batch processing operations.",
    "tags": ["Database", "Queueing", "PostgreSQL", "Task Processing"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9522669297630739,
    "cluster_avg_similarity": 0.9649412226284426
  },
  {
    "cluster_id": 252,
    "project_ids": [],
    "group_ids": [6703083452, 6704778470, 6750099076],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, ORGANIZATION_MEMBER_TEAM_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, AUTH_IDENTITY_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, PROVISION_ORGANIZATION due to database error"
    ],
    "title": "Django exception handling triggering database errors",
    "description": "Multiple request processing failures in Django middleware stack are causing database execution errors during exception handling and signal processing.",
    "tags": ["API", "Database", "Django", "PostgreSQL"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.959307080696761,
    "cluster_avg_similarity": 0.9611023905157552
  },
  {
    "cluster_id": 261,
    "project_ids": [],
    "group_ids": [6705077758, 6725840749, 6799141727, 6806749607, 6824532158, 6834481033],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification"
    ],
    "title": "Workflow notification task timeout during serialization",
    "description": "Background task exceeds 10-second deadline while serializing Group objects due to cascading database queries, Snuba requests, and integration annotation retrieval creating N+1 query patterns.",
    "tags": [
      "Queueing",
      "Database",
      "Serialization",
      "Resource Limits",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9392512236303365,
    "cluster_avg_similarity": 0.9573580054435049
  },
  {
    "cluster_id": 263,
    "project_ids": [],
    "group_ids": [
      6705342006, 6726098209, 6736162416, 6794694732, 6795623703, 6815838947, 6959131688,
      6959753217, 7000729724, 7001664563, 7001687475, 7003153053, 7013648445, 7017657981,
      7018354736, 7018502802, 7018908040, 7020888902, 7022660992
    ],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook",
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_alert_webhook_v2",
      "ProcessingDeadlineExceeded: execution deadline of 10 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.workflow_notification"
    ],
    "title": "Sentry App webhook tasks exceed processing deadlines",
    "description": "Task workers are hitting their processing deadlines while attempting to send webhooks to external Sentry App endpoints. The tasks involve expensive database queries, serialization, and RPC calls before making HTTP requests, causing them to exceed their configured timeouts.",
    "tags": [
      "External System",
      "Resource Limits",
      "Queueing",
      "ProcessingDeadlineExceeded",
      "Timeout"
    ],
    "cluster_size": 19,
    "cluster_min_similarity": 0.9101409448501845,
    "cluster_avg_similarity": 0.9487327728522269
  },
  {
    "cluster_id": 264,
    "project_ids": [],
    "group_ids": [
      6706122053, 6706758580, 6707727283, 6707727284, 6726430752, 6792401517, 6792749793,
      6793260810, 6793268617, 6794077193, 6821521843, 6856252842, 6872714672, 6873833726,
      6878201777, 6879879123, 6881015062, 6897372272, 6964501754, 6977220303, 7005450537,
      7010399757, 7023469264
    ],
    "issue_titles": [
      "Action.DoesNotExist: Action matching query does not exist.",
      "Organization.DoesNotExist: Organization matching query does not exist.",
      "Environment.DoesNotExist: Environment matching query does not exist.",
      "ExternalIssue.DoesNotExist: ExternalIssue matching query does not exist.",
      "Commit.DoesNotExist: Commit matching query does not exist.",
      "Subscription.DoesNotExist: Subscription matching query does not exist.",
      "Project.DoesNotExist: Project matching query does not exist.",
      "Group.DoesNotExist: Group matching query does not exist.",
      "Release.DoesNotExist: Release matching query does not exist.",
      "ApiGrant.DoesNotExist: ApiGrant matching query does not exist."
    ],
    "title": "DoesNotExist errors in async task processing",
    "description": "Multiple async tasks failing when attempting to fetch database records that no longer exist, likely due to race conditions between task queuing and entity lifecycle operations like deletion or merging.",
    "tags": ["Database", "Concurrency", "Django", "Record Not Found", "Task Processing"],
    "cluster_size": 23,
    "cluster_min_similarity": 0.9130042337350739,
    "cluster_avg_similarity": 0.9475393558249897
  },
  {
    "cluster_id": 269,
    "project_ids": [],
    "group_ids": [6709793450, 6709846241, 6909441101],
    "issue_titles": [
      "ApiError: status=403 body=The user does not have access to the organization."
    ],
    "title": "Slack webhook ApiError from missing org membership",
    "description": "Slack webhook actions are failing because the linked Sentry user account lacks membership in the target organization, causing access checks to fail when processing group actions.",
    "tags": ["External System", "Authorization", "API", "Slack", "Identity Linking"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9558323280730954,
    "cluster_avg_similarity": 0.9664258647122678
  },
  {
    "cluster_id": 271,
    "project_ids": [],
    "group_ids": [6711114721, 6746118296, 6878754930, 6990914635],
    "issue_titles": [
      "IntegrityError: update or delete on table \"sentry_artifactbundle\" violates foreign key constraint \"sentry_artifactbundl_artifact_bundle_id_8279332c_fk_sentry_ar\" on table \"sentry_artifactbundleindex\"",
      "IntegrityError: update or delete on table \"sentry_monitorcheckin\" violates foreign key constraint \"sentry_monitorincide_resolving_checkin_id_9b2daf6a_fk_sentry_mo\" on table \"sentry_monitorincident\"",
      "IntegrityError: update or delete on table \"sentry_incident\" violates foreign key constraint \"incident_id_refs_id_74273623\" on table \"sentry_incidentseen\"",
      "IntegrityError: update or delete on table \"workflow_engine_action\" violates foreign key constraint \"sentry_notificationm_action_id_e224a327_fk_workflow_\" on table \"sentry_notificationmessage\""
    ],
    "title": "Database commit failures across multiple components",
    "description": "Multiple Sentry components including deletion tasks, artifact assembly, API handlers, and cleanup processes are failing at the database commit stage, suggesting potential connection or transaction issues.",
    "tags": ["Database", "Concurrency", "Transaction Failure", "Connection Issues"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9513827900452801,
    "cluster_avg_similarity": 0.9624700759490157
  },
  {
    "cluster_id": 272,
    "project_ids": [],
    "group_ids": [6711283984, 6889458880, 6889470540, 6921313902],
    "issue_titles": ["IntegrationConfigurationError: Identity not found."],
    "title": "GitLab integration Identity missing for commit context",
    "description": "GitLab integration's default_auth_id references a deleted Identity record, causing commit context processing and stacktrace linking to fail. This indicates a data consistency issue where Identity cleanup did not update the OrganizationIntegration reference.",
    "tags": [
      "External System",
      "Authentication",
      "Data Integrity",
      "GitLab",
      "Identity DoesNotExist"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9513172690272859,
    "cluster_avg_similarity": 0.9632878299475308
  },
  {
    "cluster_id": 273,
    "project_ids": [],
    "group_ids": [
      6711717462, 6712227035, 6745616372, 6753978566, 6784479206, 6799173375, 6800297377,
      6820754142, 6856077790
    ],
    "issue_titles": [
      "ApiInvalidRequestError: {\"message\": \"Invalid Form Body\", \"code\": 50035, \"errors\": {\"embeds\": {\"0\": {\"description\": {\"_errors\": [{\"code\": \"BASE_TYPE_MAX_LENGTH\", \"message\": \"Must be 4096 or fewer in length.\"}]}}}}}",
      "ApiInvalidRequestError: {\"message\": \"Invalid Form Body\", \"code\": 50035, \"errors\": {\"embeds\": {\"_errors\": [{\"code\": \"MAX_EMBED_SIZE_EXCEEDED\", \"message\": \"Embed size exceeds maximum size of 6000\"}]}}}"
    ],
    "title": "Discord embed description exceeds 4096 char limit",
    "description": "Discord notifications fail when performance issues contain long SQL queries in evidence display, as the embed description field is not truncated to Discord's 4096 character limit before sending.",
    "tags": ["External System", "Input Validation", "Discord", "HTTP Error"],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9587250458904262,
    "cluster_avg_similarity": 0.9711678362666076
  },
  {
    "cluster_id": 276,
    "project_ids": [],
    "group_ids": [6712157294, 6988042707],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "PgBouncer connection failures in deletion script",
    "description": "Database deletion jobs are failing to connect to PgBouncer on port 6432, with repeated connection refusals occurring during QuerySet evaluation in the refactored delete_groups script.",
    "tags": [
      "Database",
      "External System",
      "Connection Reset",
      "PostgreSQL",
      "PgBouncer"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9593313826702368,
    "cluster_avg_similarity": 0.9593313826702368
  },
  {
    "cluster_id": 279,
    "project_ids": [],
    "group_ids": [
      6714043455, 6725380229, 6792379115, 6794440815, 6794593956, 6798243190, 6807139069,
      6855873772, 6878014820
    ],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 24 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 23 exceeds limit of 22', 'overrides': {}, 'storage_key': 'e...",
      "NoRetriesRemainingError: sentry.sentry_apps.tasks.sentry_apps.workflow_notification has consumed all of its retries",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 102 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref..."
    ],
    "title": "Snuba rate limit exceeded during webhook serialization",
    "description": "Multiple concurrent workflow notification tasks are triggering expensive Snuba queries during Group serialization for webhook payloads, exceeding the concurrent query limit of 18-22 queries on the errors_ro dataset.",
    "tags": [
      "Rate Limiting",
      "External System",
      "Queueing",
      "Snuba",
      "Retries Exhausted"
    ],
    "cluster_size": 9,
    "cluster_min_similarity": 0.9407866349083691,
    "cluster_avg_similarity": 0.9644215135716893
  },
  {
    "cluster_id": 281,
    "project_ids": [],
    "group_ids": [6714198279, 7009473332, 7009541944, 7009542437, 7009542446],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth state validation fails due to Redis race condition",
    "description": "GitHub OAuth callback state validation is failing because of race conditions in Redis session storage, where state tokens become stale or corrupted between login initiation and callback validation.",
    "tags": [
      "Authentication",
      "Concurrency",
      "Redis",
      "OAuth",
      "Race Condition",
      "State Management"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9430634457750061,
    "cluster_avg_similarity": 0.9627883010552104
  },
  {
    "cluster_id": 283,
    "project_ids": [],
    "group_ids": [6716981703, 6793866834, 6999322093],
    "issue_titles": [
      "ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}",
      "NoRetriesRemainingError: sentry.integrations.tasks.sync_status_outbound has consumed all of its retries"
    ],
    "title": "HTTP errors in outbound sync task not handled gracefully",
    "description": "The sync_status_outbound task fails to catch ApiError exceptions from external integrations like Jira, causing legitimate 404 responses (deleted/inaccessible issues) to be retried instead of handled as halt conditions.",
    "tags": ["External System", "API", "Queueing", "HTTP Error", "Retries Exhausted"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9479878925954449,
    "cluster_avg_similarity": 0.9575093274747649
  },
  {
    "cluster_id": 285,
    "project_ids": [],
    "group_ids": [6717195662, 6719007322],
    "issue_titles": ["Exception: Seer API error: 503"],
    "title": "Seer API error in autofix issue summary generation",
    "description": "The Seer API is returning error responses when generating fixability scores for issue summaries in the autofix workflow, causing the automation process to fail.",
    "tags": ["External System", "API", "Seer", "Autofix"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9766862628979174,
    "cluster_avg_similarity": 0.9766862628979174
  },
  {
    "cluster_id": 286,
    "project_ids": [],
    "group_ids": [6717262124, 6910285456, 7019766963],
    "issue_titles": [
      "ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}",
      "ApiForbiddenError: {\"errorMessages\":[\"You do not have the permission to see the specified issue.\"],\"errors\":{}}"
    ],
    "title": "Integration HTTP requests failing with error status",
    "description": "Multiple integration operations including project management events and status sync tasks are encountering HTTP errors when making requests to external systems, causing failures in both API endpoints and background tasks.",
    "tags": ["External System", "API", "HTTP", "Integration", "HTTP Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9453778386865962,
    "cluster_avg_similarity": 0.9613278782646031
  },
  {
    "cluster_id": 292,
    "project_ids": [],
    "group_ids": [6719866258, 6792141135],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='192.168.208.181', port=8080): Read timed out. (read timeout=5)"
    ],
    "title": "Task worker HTTP read timeout without retry",
    "description": "File deletion tasks are failing due to HTTP read timeouts from filestore service, with no retry mechanism configured for timeout errors despite task-level retry being available.",
    "tags": ["Networking", "External System", "HTTP", "Timeout", "Filestore"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9757893349716377,
    "cluster_avg_similarity": 0.9757893349716377
  },
  {
    "cluster_id": 296,
    "project_ids": [],
    "group_ids": [6722086808, 6722086820],
    "issue_titles": ["TypeError: unhashable type: 'list'"],
    "title": "Device class filter validation fails in metrics query",
    "description": "The device_class_converter function is encountering invalid device class values that are not present in the device_class_map during metrics query processing.",
    "tags": ["Input Validation", "API", "Metrics Query", "Device Class Filter"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.975365741915077,
    "cluster_avg_similarity": 0.975365741915077
  },
  {
    "cluster_id": 298,
    "project_ids": [],
    "group_ids": [6723347604, 6736623029, 6794697352, 6979564718, 7014965062],
    "issue_titles": [
      "UnknownOption: 'feature.projects:triage-signals-v0'",
      "UnknownOption: 'overwatch.forward-webhooks.verbose'"
    ],
    "title": "Stale feature option key lookup fails in sync_options",
    "description": "The sync_options task encounters orphaned database records for feature flags that were renamed or removed, causing UnknownOption errors when trying to look up keys that no longer exist in the current options registry.",
    "tags": [
      "Configuration",
      "Queueing",
      "Data Integrity",
      "Feature Flag Migration",
      "Stale Database Record"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9437782053849284,
    "cluster_avg_similarity": 0.9625297678656445
  },
  {
    "cluster_id": 299,
    "project_ids": [],
    "group_ids": [6723571938, 6747547432, 6884134834, 6887443647],
    "issue_titles": [
      "ApiError: {\"message\":\"404 Group Not Found\"}",
      "IntegrationError: Error Communicating with GitLab (HTTP 404): 404 Project Not Found",
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 404): VS800075: The project with id 'vstfs:///Classification/TeamProject/488a4ec0-daef-43f9-afcf-3e9eb0e32e9e' does not exist, or you do not have permission to access it.",
      "IntegrationError: Error Communicating with GitHub (HTTP 404): If this repository exists, ensure that your installation has permission to access this repository (https://github.com/settings/installations)."
    ],
    "title": "GitLab/Azure DevOps integration 404 errors from deleted projects",
    "description": "Integrations are configured with project/group IDs that no longer exist or are inaccessible, causing 404 responses when attempting to create issues or fetch project lists.",
    "tags": [
      "External System",
      "API",
      "Configuration",
      "GitLab",
      "Azure DevOps",
      "HTTP 404"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.934264217176069,
    "cluster_avg_similarity": 0.944798856366767
  },
  {
    "cluster_id": 302,
    "project_ids": [],
    "group_ids": [6724877345, 6724877767],
    "issue_titles": [
      "Subscription.DoesNotExist: Subscription matching query does not exist."
    ],
    "title": "Subscription.DoesNotExist in billing config API",
    "description": "The billing config endpoint fails when organizations exist without corresponding subscription records, indicating a data consistency issue between organizations and their required subscriptions.",
    "tags": ["Database", "Data Integrity", "API", "Django", "Missing Record"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.969502619147183,
    "cluster_avg_similarity": 0.969502619147183
  },
  {
    "cluster_id": 305,
    "project_ids": [],
    "group_ids": [6725335284, 6792752367],
    "issue_titles": ["JSONDecodeError: Expecting value: line 1 column 1 (char 0)"],
    "title": "Tempest task JSON parsing failures in worker processes",
    "description": "Multiple Tempest tasks are failing during JSON deserialization of HTTP responses, indicating malformed or corrupted response data from external service calls.",
    "tags": ["External System", "Serialization", "Deserialization", "JSON Parse Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9811118851630424,
    "cluster_avg_similarity": 0.9811118851630424
  },
  {
    "cluster_id": 306,
    "project_ids": [],
    "group_ids": [
      6725487265, 6725487266, 6805674835, 6829771050, 6830365847, 6831587006, 6835044609,
      6835568510, 6835993624
    ],
    "issue_titles": [
      "InvalidSearchQuery: Choose a single environment to filter by release stage."
    ],
    "title": "Release stage filter requires single environment context",
    "description": "Subscription update tasks fail when processing queries containing release.stage filters because the validation requires exactly one environment but none is provided during old subscription cleanup.",
    "tags": [
      "Input Validation",
      "Configuration",
      "Snuba",
      "Release Stage Filter",
      "Invalid Search Query"
    ],
    "cluster_size": 9,
    "cluster_min_similarity": 0.940370373257409,
    "cluster_avg_similarity": 0.9697962615513354
  },
  {
    "cluster_id": 307,
    "project_ids": [],
    "group_ids": [6725749886, 6746445819],
    "issue_titles": [
      "InvalidQueryError: query must have at least one expression in select"
    ],
    "title": "Empty select clause in Snuba metric cardinality query",
    "description": "The cardinality check filters out all function columns before building queries, leaving an empty select clause that fails SnQL validation. This occurs when widget queries contain only function expressions like count() and p75().",
    "tags": ["Input Validation", "Queueing", "Query Builder", "SnQL", "Empty Select"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9579443362533883,
    "cluster_avg_similarity": 0.9579443362533883
  },
  {
    "cluster_id": 309,
    "project_ids": [],
    "group_ids": [6726781123, 6803878240, 6805787930],
    "issue_titles": [
      "SentryAppSentryError: missing_servicehook",
      "SentryAppSentryError: missing_installation"
    ],
    "title": "Sentry app webhook task fails with missing servicehook",
    "description": "Sentry app webhook tasks are failing because the expected ServiceHook record is missing for existing installations, likely due to race conditions during app configuration updates or cache invalidation issues in the hybrid cloud architecture.",
    "tags": [
      "External System",
      "Configuration",
      "Caching",
      "Sentry Apps",
      "Missing Servicehook"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9510628388353847,
    "cluster_avg_similarity": 0.9593619852534264
  },
  {
    "cluster_id": 311,
    "project_ids": [],
    "group_ids": [6727719216, 6792702711],
    "issue_titles": ["TimeoutException"],
    "title": "Project config timeout from N+1 environment queries",
    "description": "Relay project configuration build is timing out due to N+1 database queries when accessing alert rule environments. Missing select_related for environment FK causes separate queries for each alert rule.",
    "tags": ["Database", "Configuration", "N+1 Query", "Timeout", "Relay"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9833023494186147,
    "cluster_avg_similarity": 0.9833023494186147
  },
  {
    "cluster_id": 313,
    "project_ids": [],
    "group_ids": [6728181594, 6792767328, 6800184233],
    "issue_titles": [
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.scheduleMessage)",
      "SlackApiError: The request to the Slack API failed. (url: https://www.slack.com/api/chat.postMessage)"
    ],
    "title": "Slack API errors in channel operations and messaging",
    "description": "Multiple Slack integration tasks are failing with SlackApiError when attempting to schedule messages for channel lookup and post messages to channels.",
    "tags": ["External System", "API", "Slack", "SlackApiError"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9532255621272243,
    "cluster_avg_similarity": 0.9626471933027924
  },
  {
    "cluster_id": 314,
    "project_ids": [],
    "group_ids": [6728270121, 6805899738],
    "issue_titles": [
      "ObjectErrorUnknown: invalid PE file",
      "ObjectErrorUnknown: invalid MachO file"
    ],
    "title": "Debug file assembly failing in symbolic processing",
    "description": "The assemble_dif task is failing when processing debug information files through the symbolic library, specifically during archive object iteration and Rust FFI calls.",
    "tags": [
      "External System",
      "Serialization",
      "Debug File Processing",
      "Symbolic Library",
      "Rust FFI"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9660471295287995,
    "cluster_avg_similarity": 0.9660471295287995
  },
  {
    "cluster_id": 315,
    "project_ids": [],
    "group_ids": [6728351772, 6796865949],
    "issue_titles": [
      "IntegrityError: insert or update on table \"sentry_artifactbundleindex\" violates foreign key constraint \"sentry_artifactbundl_artifact_bundle_id_8279332c_fk_sentry_ar\""
    ],
    "title": "Artifact bundle indexing fails on deleted bundles",
    "description": "Race condition where ArtifactBundles are deleted between backfill task scheduling and execution, causing foreign key constraint violations during index creation.",
    "tags": [
      "Database",
      "Concurrency",
      "Data Integrity",
      "Constraint Violation",
      "Background Tasks"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.96118940533218,
    "cluster_avg_similarity": 0.96118940533218
  },
  {
    "cluster_id": 317,
    "project_ids": [],
    "group_ids": [6728508401, 6800382248],
    "issue_titles": ["TypeError: 'NoneType' object cannot be interpreted as an integer"],
    "title": "Slack notification fails during timestamp processing",
    "description": "Slack activity notifications are failing when building message context, specifically during timestamp conversion for APPROX_START_TIME field in issue notifications.",
    "tags": ["External System", "API", "Slack", "Serialization"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9883915268916114,
    "cluster_avg_similarity": 0.9883915268916114
  },
  {
    "cluster_id": 318,
    "project_ids": [],
    "group_ids": [6729690402, 6839231608, 7009473564, 7010182387],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "VSTS OAuth fails - vsts_login_new provider not registered",
    "description": "OAuth pipeline attempts to use vsts_login_new provider when vsts.social-auth-migration is enabled, but this provider key was never registered in the identity manager, causing NotRegistered exceptions during token validation.",
    "tags": [
      "Authentication",
      "Configuration",
      "OAuth",
      "Provider Registration",
      "Azure DevOps"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9554977322598529,
    "cluster_avg_similarity": 0.9695644266741853
  },
  {
    "cluster_id": 322,
    "project_ids": [],
    "group_ids": [6731076286, 6798184191, 6908824900],
    "issue_titles": ["OrganizationIntegrationNotFound: missing org_integration"],
    "title": "Integration tasks failing with missing org_integration",
    "description": "Multiple integration-related background tasks (comment creation, comment updates, and commit context processing) are failing because the organization integration configuration is missing or unavailable.",
    "tags": [
      "External System",
      "Configuration",
      "Integration",
      "Missing Org Integration"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9448746794981241,
    "cluster_avg_similarity": 0.9531730398621682
  },
  {
    "cluster_id": 323,
    "project_ids": [],
    "group_ids": [6731105009, 6832091408],
    "issue_titles": [
      "NoRetriesRemainingError: sentry.tasks.integrations.update_comment has consumed all of its retries",
      "NoRetriesRemainingError: sentry.integrations.tasks.create_comment has consumed all of its retries"
    ],
    "title": "Integration tasks fail with missing org_integration",
    "description": "Integration comment sync operations are failing because the organization integration relationship is missing or not properly configured, preventing proper validation of sync permissions.",
    "tags": ["Integration", "Configuration", "Data Integrity", "Missing Relationship"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9810761360181299,
    "cluster_avg_similarity": 0.9810761360181299
  },
  {
    "cluster_id": 330,
    "project_ids": [],
    "group_ids": [6734946917, 6898208311, 6959970769],
    "issue_titles": ["ApiError"],
    "title": "Integration proxy returning 502 on API client errors",
    "description": "The control silo's integration proxy endpoint is not properly handling exceptions from third-party API calls (GitLab/GitHub), causing unhandled ApiError exceptions to bubble up and return 502/520 responses instead of proper HTTP status codes.",
    "tags": [
      "API",
      "External System",
      "Integration Proxy",
      "Exception Handling",
      "GitLab",
      "GitHub"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9559561362482908,
    "cluster_avg_similarity": 0.9587113756966423
  },
  {
    "cluster_id": 331,
    "project_ids": [],
    "group_ids": [6735138068, 6808813115, 6977602427, 7017777330],
    "issue_titles": [
      "TypeError: Interface.__init__() got multiple values for argument 'self'"
    ],
    "title": "Context interface fails with reserved Python keyword",
    "description": "Event contexts containing a 'self' key cause deserialization failure when the Interface.to_python() method unpacks data as keyword arguments, conflicting with Python's implicit self parameter.",
    "tags": ["Serialization", "Input Validation", "Python", "Reserved Keyword Conflict"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.95131508239398,
    "cluster_avg_similarity": 0.9662160844937399
  },
  {
    "cluster_id": 334,
    "project_ids": [],
    "group_ids": [6735587400, 6736085739],
    "issue_titles": [
      "TypeError: '<' not supported between instances of 'str' and 'float'"
    ],
    "title": "Trace sorting fails on mixed float/string comparison",
    "description": "The child_sort_key function returns incompatible data types when sorting trace events, causing Python to fail when comparing floats from precise timestamps with strings from transaction names.",
    "tags": ["API", "Serialization", "Data Integrity", "Type Mismatch"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9697377622986438,
    "cluster_avg_similarity": 0.9697377622986438
  },
  {
    "cluster_id": 335,
    "project_ids": [],
    "group_ids": [6735896989, 6735897451],
    "issue_titles": [
      "ApiInvalidRequestError: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid grant: authorization code is invalid\"}",
      "PipelineError: Failed to retrieve token from the upstream service."
    ],
    "title": "OAuth authorization code reuse in Vercel integration",
    "description": "Multiple requests to the Vercel OAuth callback endpoint are attempting to exchange the same single-use authorization code, causing Vercel's API to reject subsequent attempts with invalid_grant errors.",
    "tags": [
      "Authentication",
      "External System",
      "Concurrency",
      "OAuth",
      "Vercel",
      "Invalid Grant"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9852314315043561,
    "cluster_avg_similarity": 0.9852314315043561
  },
  {
    "cluster_id": 339,
    "project_ids": [],
    "group_ids": [
      6737480353, 6741533525, 6741533662, 6746194384, 6803878965, 6808644708, 6816571342,
      6915035744, 6918080031, 7013950941
    ],
    "issue_titles": ["Group.DoesNotExist: Group matching query does not exist."],
    "title": "Group lookup fails during trace serialization",
    "description": "Trace data contains issue.id references to Groups that no longer exist in the database, causing serialization to fail when attempting to fetch non-existent Group objects during error event processing.",
    "tags": ["Database", "Data Integrity", "API", "Django", "Group Lookup Failure"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9396690137054472,
    "cluster_avg_similarity": 0.9623905635384727
  },
  {
    "cluster_id": 347,
    "project_ids": [],
    "group_ids": [6744811987, 6795845689],
    "issue_titles": ["DeleteAborted: delete_groups.no_groups_found"],
    "title": "Group deletion task aborted - no groups found",
    "description": "The delete_groups_for_project task is raising DeleteAborted exceptions when no groups are found to delete for a project, indicating potential issues with task scheduling or data consistency.",
    "tags": ["Task Processing", "Data Integrity", "Deletion Task", "Delete Aborted"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.959297031631857,
    "cluster_avg_similarity": 0.959297031631857
  },
  {
    "cluster_id": 351,
    "project_ids": [],
    "group_ids": [6748936512, 6752007934, 6769375194, 6977999086],
    "issue_titles": ["ApiInvalidRequestError", "IntegrationResourceNotFoundError"],
    "title": "Integration API failures in GitHub/GitLab issue creation",
    "description": "HTTP errors occurring when Sentry attempts to communicate with GitHub and GitLab APIs during issue creation and repository configuration retrieval.",
    "tags": ["External System", "API", "GitHub", "GitLab", "HTTP Error"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9457524312215798,
    "cluster_avg_similarity": 0.9596921214318147
  },
  {
    "cluster_id": 353,
    "project_ids": [],
    "group_ids": [6749275385, 6911667706, 7016973214],
    "issue_titles": [
      "RuntimeError: cannot schedule new futures after interpreter shutdown",
      "RuntimeError: cannot schedule new futures after shutdown"
    ],
    "title": "ThreadPoolExecutor submit after interpreter shutdown",
    "description": "HTTP request threads attempt to submit tasks to ThreadPoolExecutor after Python interpreter has begun shutdown sequence, creating a race condition between request processing and server termination.",
    "tags": [
      "Concurrency",
      "Threading",
      "Shutdown Sequence",
      "Race Condition",
      "ThreadPoolExecutor"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9450143215569157,
    "cluster_avg_similarity": 0.9523246840354895
  },
  {
    "cluster_id": 357,
    "project_ids": [],
    "group_ids": [6751424943, 6794331610],
    "issue_titles": [
      "ApiError: {\"message\":\"Request body is not processable. Please check the errors.\",\"errors\":{\"message\":\"Message can not be empty.\"},\"took\":0.0,\"requestId\":\"d05f1c43-ca96-4bd6-97d1-e80c183d7a67\"}",
      "ApiError: {\"message\":\"Request body is not processable. Please check the errors.\",\"errors\":{\"message\":\"Message can not be empty.\"},\"took\":0.001,\"requestId\":\"b1d67713-333f-4f2a-8cea-504d6552e69d\"}"
    ],
    "title": "OpsGenie spike protection deactivation fails",
    "description": "Spike protection deactivation attempts to close OpsGenie alerts using the creation endpoint with incomplete payload, causing 422 errors due to missing required message field.",
    "tags": ["External System", "API", "OpsGenie", "HTTP 422", "Spike Protection"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9747573758489215,
    "cluster_avg_similarity": 0.9747573758489215
  },
  {
    "cluster_id": 363,
    "project_ids": [],
    "group_ids": [6754548453, 6758268352],
    "issue_titles": ["AttributeError: 'NoneType' object has no attribute 'id'"],
    "title": "Feature check crashes on None subscription lookup",
    "description": "Organization serialization fails when checking Seer features for organizations without subscriptions, as the code doesn't handle the None case before accessing subscription.id in BillingHistory.get_current().",
    "tags": ["API", "Serialization", "Database", "Input Validation", "AttributeError"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9703628156192328,
    "cluster_avg_similarity": 0.9703628156192328
  },
  {
    "cluster_id": 364,
    "project_ids": [],
    "group_ids": [
      6755570178, 6837752393, 6837759595, 6870110084, 6870132866, 6945973028, 6969043325
    ],
    "issue_titles": [
      "SnubaError: Failed to parse snuba error response",
      "DecodeError: Error parsing message"
    ],
    "title": "Snuba RPC timeout causing protobuf parsing failures",
    "description": "Multiple Snuba RPC endpoints are timing out and returning 504 Gateway Timeout responses with HTML/text bodies, which the error handling code incorrectly attempts to parse as protobuf ErrorProto messages.",
    "tags": [
      "External System",
      "API",
      "Timeout",
      "Serialization",
      "Snuba",
      "Gateway Timeout"
    ],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9250882751159084,
    "cluster_avg_similarity": 0.9590220538710558
  },
  {
    "cluster_id": 370,
    "project_ids": [],
    "group_ids": [
      6761349855, 6761355450, 6792433167, 6878194042, 6921408385, 7005214134, 7015845731,
      7016144644
    ],
    "issue_titles": [
      "IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist.",
      "ValueError: AlertRuleWorkflow not found when querying for AlertRuleWorkflow",
      "ValueError: IncidentGroupOpenPeriod does not exist",
      "ValueError: Alert rule detector not found when querying for AlertRuleDetector"
    ],
    "title": "Missing IncidentGroupOpenPeriod relation in metric alerts",
    "description": "Workflow engine metric alert notifications fail when querying for IncidentGroupOpenPeriod relationships that were never created during issue ingestion, causing DoesNotExist exceptions during Slack and email notification generation.",
    "tags": ["Database", "Data Integrity", "Workflow Engine", "Record Not Found"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9292557372347852,
    "cluster_avg_similarity": 0.9518164839386847
  },
  {
    "cluster_id": 375,
    "project_ids": [],
    "group_ids": [6767363302, 6810274582, 6883514139, 6902485333],
    "issue_titles": [
      "MissingSchema: Invalid URL '': No scheme supplied. Perhaps you meant https://?",
      "MissingSchema: Invalid URL '/sentry/issues?installationId=22974884-2be4-4c8b-a640-cc2826dd2205': No scheme supplied. Perhaps you meant https:///sentry/issues?installationId=22974884-2be4-4c8b-a640-cc2826dd2205?"
    ],
    "title": "Sentry App webhook URL missing causing MissingSchema",
    "description": "Internal Sentry Apps with dynamic form fields are failing when webhook_url is None or empty, causing URL construction to produce relative URLs that fail HTTP request validation.",
    "tags": ["API", "Input Validation", "Sentry Apps", "Missing Schema"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9436571142073465,
    "cluster_avg_similarity": 0.959955827069915
  },
  {
    "cluster_id": 380,
    "project_ids": [],
    "group_ids": [6774836259, 7006655649, 7006713356],
    "issue_titles": [
      "NoRetriesRemainingError: sentry.deletions.tasks.groups.delete_groups_for_project has consumed all of its retries",
      "OperationalError: canceling statement due to user request"
    ],
    "title": "PostgreSQL query cancellation in group deletion task",
    "description": "Database queries in the group deletion process are being canceled due to full table scans on unindexed columns, causing lock contention and timeouts when processing GroupHashMetadata records.",
    "tags": [
      "Database",
      "Concurrency",
      "PostgreSQL",
      "Query Cancellation",
      "Missing Index"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9513679039082958,
    "cluster_avg_similarity": 0.9642961738672152
  },
  {
    "cluster_id": 381,
    "project_ids": [],
    "group_ids": [6775118951, 6843697728],
    "issue_titles": [
      "ValueError: Expected 1 sentry app installation for action type: sentry_app, target_identifier: f7321fdc-be18-4d4a-bc85-925091bc847d, but got 0"
    ],
    "title": "Sentry app issue alert handler target identifier error",
    "description": "Workflow engine task fails when processing Sentry app issue alerts due to ValueError in target identifier resolution during rule creation.",
    "tags": ["Configuration", "Input Validation", "Workflow Engine", "Sentry App"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9653525161118205,
    "cluster_avg_similarity": 0.9653525161118205
  },
  {
    "cluster_id": 382,
    "project_ids": [],
    "group_ids": [6775280353, 6798150081],
    "issue_titles": [
      "SubscriptionError: The functions provided do not match the requested metric type"
    ],
    "title": "IncompatibleMetricsQuery in AlertMetricsQueryBuilder",
    "description": "The sum() function lacks snql_counter support when aggregating counter-type metrics like ai.total_cost, causing the query builder to raise IncompatibleMetricsQuery during subscription deletion.",
    "tags": ["API", "Configuration", "IncompatibleMetricsQuery", "Metrics Aggregation"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9697446307534625,
    "cluster_avg_similarity": 0.9697446307534625
  },
  {
    "cluster_id": 388,
    "project_ids": [],
    "group_ids": [6779047596, 6792254811, 6793990277, 6866950211],
    "issue_titles": [
      "RefreshError: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 503,\\n    \"message\": \"The service is currently unavailable.\",\\n    \"status\": \"UNAVAILABLE\"\\n  }\\n}\\n')",
      "RefreshError: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 503,\\n    \"message\": \"Authentication backend unavailable.\",\\n    \"status\": \"UNAVAILABLE\"\\n  }\\n}\\n')"
    ],
    "title": "GCP token refresh fails during profile symbolication",
    "description": "Profile symbolication tasks are failing because Google Cloud IAM service unavailability (503) causes unhandled RefreshError exceptions when acquiring access tokens for symbol sources.",
    "tags": [
      "External System",
      "Authentication",
      "Google Cloud",
      "RefreshError",
      "Profile Symbolication"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.976929968001515,
    "cluster_avg_similarity": 0.9806843477486086
  },
  {
    "cluster_id": 394,
    "project_ids": [],
    "group_ids": [
      6784499232, 6784775663, 6788281996, 6796039249, 6800377295, 6802471422, 6806805534,
      6833681577, 6936628438, 6985072459
    ],
    "issue_titles": [
      "OperationalError: canceling statement due to statement timeout",
      "OperationalError: canceling statement due to user request"
    ],
    "title": "PostgreSQL query timeout in Sentry API endpoints",
    "description": "Database queries are being canceled due to statement timeouts across multiple Sentry API endpoints, particularly affecting group index and events endpoints with complex query operations.",
    "tags": ["Database", "PostgreSQL", "API", "Timeout"],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9251265732236041,
    "cluster_avg_similarity": 0.9621503153894493
  },
  {
    "cluster_id": 395,
    "project_ids": [],
    "group_ids": [
      6784563319, 6784662423, 6784718347, 6784777741, 6784828208, 6785097543, 6785098003,
      6785111362, 6785409246, 6785601248, 6785669238, 6785768117, 6792578888, 6797684953,
      6800471686, 6808600069, 6840464452, 6897808544, 6945426256, 7024279245, 7024282576,
      7024282626, 7025006724
    ],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "PostgreSQL query failures in Sentry API handlers",
    "description": "Database query execution is failing within Sentry's PostgreSQL wrapper during API request processing and background task execution.",
    "tags": ["Database", "API", "PostgreSQL", "Query Execution"],
    "cluster_size": 23,
    "cluster_min_similarity": 0.9290544066204783,
    "cluster_avg_similarity": 0.9616962385895973
  },
  {
    "cluster_id": 399,
    "project_ids": [],
    "group_ids": [6784663532, 6788870342, 6794486651, 6804434396],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "PostgreSQL query timeouts in Django request handling",
    "description": "Multiple PostgreSQL queries are timing out during Django API request processing, particularly for activity lookups and workflow fire history aggregations. Missing database indexes on frequently queried column combinations are causing full table scans that exceed statement timeout limits.",
    "tags": ["Database", "API", "PostgreSQL", "Django", "Timeout", "Missing Index"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.947424775820645,
    "cluster_avg_similarity": 0.9586627331632583
  },
  {
    "cluster_id": 401,
    "project_ids": [],
    "group_ids": [
      6784705408, 6785665754, 6786817754, 6794696235, 6796792406, 6796792420, 6805604414,
      6806782105, 6808442238, 6812555527, 6813362692, 6842020804, 6842075781, 6843137165,
      6883978502, 6889167490, 6937471892, 7016295924
    ],
    "issue_titles": ["OperationalError: server closed the connection unexpectedly"],
    "title": "PostgreSQL connection lost in multiprocessing workers",
    "description": "Database connections inherited from parent processes become invalid in spawned taskworker child processes, causing connection failures when executing queries during billing operations.",
    "tags": [
      "Database",
      "Multiprocessing",
      "Connection Reset",
      "PostgreSQL",
      "Django ORM"
    ],
    "cluster_size": 18,
    "cluster_min_similarity": 0.92664256129265,
    "cluster_avg_similarity": 0.9631678919524982
  },
  {
    "cluster_id": 406,
    "project_ids": [],
    "group_ids": [
      6785082756, 6794807830, 6797141280, 6952385461, 6995480460, 7004710759, 7022674841
    ],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"sentry_commitauthor_organization_id_3cdc85e9f09bf3f3_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_commit_repository_id_2d25b4d8949fca93_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_preprodartifactsi_organization_id_head_siz_ee2086a2_uniq\"",
      "IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist.",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_projectcodeowners_repository_project_path__1864c01a_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_identity_idp_id_47d379630426f630_uniq\"",
      "IntegrityError: duplicate key value violates unique constraint \"sentry_commit_repository_id_key_7f948336_uniq\""
    ],
    "title": "ProjectCodeOwners race condition on code mapping ID",
    "description": "Multiple concurrent requests attempting to create ProjectCodeOwners records with the same code mapping ID are bypassing validation checks, causing unique constraint violations on repository_project_path_config field.",
    "tags": ["Database", "Concurrency", "API", "Constraint Violation", "Race Condition"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9379798685562138,
    "cluster_avg_similarity": 0.9532615773576274
  },
  {
    "cluster_id": 409,
    "project_ids": [],
    "group_ids": [6785427948, 6792940177, 6814735066, 6859339382, 7010064041],
    "issue_titles": ["OperationalError: deadlock detected"],
    "title": "PostgreSQL deadlock in tally_usage billing updates",
    "description": "Multiple concurrent billing tally processes are updating the same BillingMetricHistory and Subscription records without proper lock ordering, causing transaction deadlocks. The issue stems from multi-database transaction routing mismatches where different database connections are used within the same atomic transaction.",
    "tags": ["Database", "Concurrency", "PostgreSQL", "Deadlock", "Billing"],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9416410433841116,
    "cluster_avg_similarity": 0.9609330203970629
  },
  {
    "cluster_id": 412,
    "project_ids": [],
    "group_ids": [6788281975, 6802471392, 6802471399, 6820659301, 6878624669, 7015573042],
    "issue_titles": ["OperationalError: canceling statement due to statement timeout"],
    "title": "PostgreSQL timeout in latest release query",
    "description": "Complex window function query for resolving release:\"latest\" filters times out when processing organizations with many releases across multiple projects. The query applies expensive ranking operations before organization filtering.",
    "tags": ["Database", "API", "PostgreSQL", "Timeout", "Django"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9370827301112296,
    "cluster_avg_similarity": 0.9623965495143119
  },
  {
    "cluster_id": 415,
    "project_ids": [],
    "group_ids": [6789123542, 6817439707, 6819272625],
    "issue_titles": [
      "IntegrityError: duplicate key value violates unique constraint \"auth_user_username_key\""
    ],
    "title": "Django user registration race condition causing constraint violation",
    "description": "Multiple concurrent registration requests for the same username pass form validation simultaneously, causing the second request to fail with a unique constraint violation when attempting to save the user to the database.",
    "tags": [
      "Database",
      "Concurrency",
      "Django",
      "PostgreSQL",
      "Constraint Violation",
      "Race Condition"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9616688335209672,
    "cluster_avg_similarity": 0.9697174388324455
  },
  {
    "cluster_id": 422,
    "project_ids": [],
    "group_ids": [6792293637, 7017568711],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 905 seconds exceeded by sentry.tasks.commits.fetch_commits"
    ],
    "title": "GitLab commit fetch task exceeds 905s deadline",
    "description": "The fetch_commits task is making sequential API calls to fetch diffs for each commit in GitLab repositories, causing timeouts when processing repositories with large commit histories.",
    "tags": [
      "External System",
      "API",
      "Timeout",
      "GitLab",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9743380371349996,
    "cluster_avg_similarity": 0.9743380371349996
  },
  {
    "cluster_id": 424,
    "project_ids": [],
    "group_ids": [6792433102, 6921408379],
    "issue_titles": [
      "IncidentGroupOpenPeriod.DoesNotExist: IncidentGroupOpenPeriod matching query does not exist."
    ],
    "title": "Missing IncidentGroupOpenPeriod during alert serialization",
    "description": "Race condition where GroupOpenPeriod exists but corresponding IncidentGroupOpenPeriod record was never created, causing DoesNotExist errors when serializing metric alert notifications.",
    "tags": ["Database", "Concurrency", "Workflow Engine", "Django", "DoesNotExist"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9749781321211902,
    "cluster_avg_similarity": 0.9749781321211902
  },
  {
    "cluster_id": 427,
    "project_ids": [],
    "group_ids": [6792831416, 6794082726, 6953959701, 6978590335],
    "issue_titles": [
      "InternalServerError: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: {",
      "BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles-de/o?uploadType=multipart: <!DOCTYPE html>",
      "BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: <!DOCTYPE html>",
      "BadGateway: POST https://storage.googleapis.com/upload/storage/v1/b/sentry-replays/o?uploadType=multipart: <!DOCTYPE html>"
    ],
    "title": "GCS upload fails on HTTP 502 without retry",
    "description": "Google Cloud Storage multipart uploads are failing with HTTP 502 Bad Gateway errors during profile and replay processing. The BadGateway exception is not included in GCS_RETRYABLE_ERRORS, preventing automatic retry attempts.",
    "tags": [
      "External System",
      "Disk/Storage",
      "Google Cloud Storage",
      "Upstream Unavailable",
      "Retries Exhausted"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9513995508863334,
    "cluster_avg_similarity": 0.9644182833088647
  },
  {
    "cluster_id": 429,
    "project_ids": [],
    "group_ids": [6792981820, 6793448323, 6805727503, 6808030692],
    "issue_titles": [
      "ValueError: Not a valid response type: <!DOCTYPE html>",
      "UnsupportedResponseType: text/html; charset=utf-8"
    ],
    "title": "GitLab API returns HTML instead of JSON with 200 status",
    "description": "GitLab API endpoints are returning HTML responses with HTTP 200 status instead of expected JSON data, likely due to missing Accept headers or server-side content negotiation issues for static file paths.",
    "tags": ["External System", "API", "Serialization", "GitLab", "Content Negotiation"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9503072741689907,
    "cluster_avg_similarity": 0.9568960521510164
  },
  {
    "cluster_id": 432,
    "project_ids": [],
    "group_ids": [6793315486, 6794582189],
    "issue_titles": [
      "ApiRetryError: SafeHTTPConnectionPool(host='sentry-rpc-prod-control.us.sentry.internal', port=8999): Max retries exceeded with url: /api/0/internal/integration-proxy/ (Caused by ResponseError('too many 503 error responses'))",
      "ApiRetryError: SafeHTTPConnectionPool(host='10.2.0.67', port=8999): Max retries exceeded with url: /extensions/jira/issue/CX-5750/?xdm_e=https%3A%2F%2Fdomainpartners.atlassian.net&xdm_c=channel-sentry.io.jira__sentry-issues-glance-4387276911594019194&cp=&xdm_deprecate..."
    ],
    "title": "Region silo unavailable - HTTP retries exhausted",
    "description": "The region silo at 10.2.0.67:8999 returned 5 consecutive 503 Service Unavailable errors, causing the control silo's RegionSiloClient to exhaust all retry attempts when processing a Jira issue request.",
    "tags": [
      "Networking",
      "External System",
      "Upstream Unavailable",
      "Retries Exhausted"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9570308236328342,
    "cluster_avg_similarity": 0.9570308236328342
  },
  {
    "cluster_id": 433,
    "project_ids": [],
    "group_ids": [6793408501, 6906744483],
    "issue_titles": ["ApiError: {\"detail\":\"Internal Error\",\"errorId\":null}"],
    "title": "GitHub API 500 errors via integration proxy",
    "description": "GitHub's API is intermittently returning 500 errors for pull request file requests, causing open PR comment workflow tasks to fail when proxied through the integration-proxy service.",
    "tags": ["External System", "API", "GitHub", "Upstream Unavailable"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9583624073641308,
    "cluster_avg_similarity": 0.9583624073641308
  },
  {
    "cluster_id": 439,
    "project_ids": [],
    "group_ids": [6793574006, 6806179302, 6910371222, 6912403409, 6915144776, 6941962034],
    "issue_titles": [
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Ready for testing' that is not in the list of supported values",
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Resolved' that is not in the list of supported values",
      "IntegrationError: Error Communicating with Azure DevOps (HTTP 400): The field 'State' contains the value 'Active' that is not in the list of supported values"
    ],
    "title": "Azure DevOps sync fails with invalid work item state",
    "description": "Outbound status synchronization to Azure DevOps is failing because the configured status 'Active' is not valid for the work item type, resulting in HTTP 400 errors from the Azure DevOps API.",
    "tags": ["External System", "API", "Azure DevOps", "HTTP Error"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9505285618045818,
    "cluster_avg_similarity": 0.9666188798451838
  },
  {
    "cluster_id": 440,
    "project_ids": [],
    "group_ids": [6793812861, 6915293682],
    "issue_titles": ["SnubaRPCError: code: 500"],
    "title": "Snuba RPC calls failing across trace and events endpoints",
    "description": "Multiple API endpoints are experiencing failures when making RPC calls to Snuba for trace data and event queries, causing paginated responses to fail.",
    "tags": ["External System", "API", "Snuba", "RPC"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9691574022915127,
    "cluster_avg_similarity": 0.9691574022915127
  },
  {
    "cluster_id": 441,
    "project_ids": [],
    "group_ids": [6794134248, 7013444713],
    "issue_titles": [
      "IntegrityError: insert or update on table \"sentry_groupemailthread\" violates foreign key constraint \"sentry_groupemailthr_group_id_4b988db2_fk_sentry_gr\""
    ],
    "title": "GroupEmailThread creation fails for deleted groups",
    "description": "A race condition occurs when activity notifications are processed after their associated groups have been deleted, causing foreign key violations when trying to create GroupEmailThread records.",
    "tags": ["Database", "Concurrency", "Django", "Constraint Violation"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9701532902886322,
    "cluster_avg_similarity": 0.9701532902886322
  },
  {
    "cluster_id": 443,
    "project_ids": [],
    "group_ids": [6794396340, 6840907073, 6919117994, 6919117996],
    "issue_titles": [
      "ApiError: {\"errorMessages\":[\"Issue does not exist or you do not have permission to see it.\"],\"errors\":{}}",
      "RetryError"
    ],
    "title": "Jira integration ticket creation fails on immediate fetch",
    "description": "Issue tickets are successfully created in Jira (201 response) but immediately fail when retrieving them due to eventual consistency delays, causing a race condition between creation and fetch operations.",
    "tags": ["External System", "API", "Jira", "Race Condition"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9494851277641422,
    "cluster_avg_similarity": 0.9667428670036715
  },
  {
    "cluster_id": 446,
    "project_ids": [],
    "group_ids": [6794677909, 6812355657, 6871496144, 6926399632, 7002396288, 7016735138],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 102 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 19 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ConcurrentRateLimitAllocationPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 20 exceeds limit of 18', 'overrides': {}, 'storage_key': 'e...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 101 exceeds limit of 100', 'policy': 'referrer_guard_rail_policy', 'ref..."
    ],
    "title": "Snuba rate limit exceeded building Slack notifications",
    "description": "Multiple concurrent Slack notification workflows are independently querying Snuba for user count context during message building, causing queries to exceed the concurrent rate limit of 18 when notifications fire simultaneously.",
    "tags": [
      "Rate Limiting",
      "Database",
      "External System",
      "Concurrency",
      "Snuba",
      "Slack"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9371122666648308,
    "cluster_avg_similarity": 0.9562244590418374
  },
  {
    "cluster_id": 454,
    "project_ids": [],
    "group_ids": [6796577351, 7008868960],
    "issue_titles": [
      "TooManyRequests: POST https://storage.googleapis.com/upload/storage/v1/b/sentryio-profiles/o?uploadType=multipart: {"
    ],
    "title": "GCS profile upload fails on 429 rate limit errors",
    "description": "Profile processing tasks fail when uploading to Google Cloud Storage due to rate limiting (HTTP 429), but TooManyRequests exceptions are not included in the retryable errors list, causing immediate task failures instead of proper retry logic.",
    "tags": [
      "External System",
      "Rate Limiting",
      "Google Cloud Storage",
      "TooManyRequests"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.970824849668349,
    "cluster_avg_similarity": 0.970824849668349
  },
  {
    "cluster_id": 470,
    "project_ids": [],
    "group_ids": [6799839824, 6799839828],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, TEAM_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, PROJECT_UPDATE due to database error"
    ],
    "title": "Task processing deadline interrupts outbox draining",
    "description": "Deletion tasks trigger outbox processing via Django on_commit hooks, but the outbox SELECT FOR UPDATE query gets canceled when the 20-minute task deadline expires mid-execution.",
    "tags": [
      "Database",
      "Concurrency",
      "Queueing",
      "PostgreSQL",
      "Timeout",
      "Query Canceled"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.961963876866972,
    "cluster_avg_similarity": 0.961963876866972
  },
  {
    "cluster_id": 472,
    "project_ids": [],
    "group_ids": [6800481587, 6830730446],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 600 seconds exceeded by sentry.tasks.summaries.weekly_reports.prepare_organization_report"
    ],
    "title": "Weekly report task timeout during user data fetch",
    "description": "Weekly report generation task exceeds 600-second deadline while making RPC calls to fetch user timezone data during report delivery, after spending significant time on Snuba queries during context creation.",
    "tags": [
      "Task Processing",
      "RPC",
      "Timeout",
      "Weekly Reports",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9595062323480753,
    "cluster_avg_similarity": 0.9595062323480753
  },
  {
    "cluster_id": 475,
    "project_ids": [],
    "group_ids": [6800497870, 6976055858],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 600 seconds exceeded by sentry.tasks.summaries.weekly_reports.prepare_organization_report",
      "ProcessingDeadlineExceeded: execution deadline of 300 seconds exceeded by getsentry.tasks.send_product_trial_emails"
    ],
    "title": "CSS parsing timeout during email HTML generation",
    "description": "Email tasks are timing out after 300 seconds due to exponentially expensive CSS parsing in toronado.inline() when processing duplicate CSS selector rules in email templates.",
    "tags": ["Email", "Processing Timeout", "CSS Parsing", "Resource Limits"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9573117828524692,
    "cluster_avg_similarity": 0.9573117828524692
  },
  {
    "cluster_id": 480,
    "project_ids": [],
    "group_ids": [6802448656, 7004754177],
    "issue_titles": [
      "OutboxDatabaseError: Failed to process Outbox, TEAM_UPDATE due to database error",
      "OutboxDatabaseError: Failed to process Outbox, ORGANIZATION_MEMBER_UPDATE due to database error"
    ],
    "title": "Outbox processing fails on PostgreSQL query cancellation",
    "description": "The RegionOutbox.process_shard() method only handles LockNotAvailable errors but fails to handle QueryCanceled errors from statement timeouts or user cancellations, causing 500 errors during team creation.",
    "tags": ["Database", "API", "PostgreSQL", "Query Canceled", "Outbox Processing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9661584102026848,
    "cluster_avg_similarity": 0.9661584102026848
  },
  {
    "cluster_id": 487,
    "project_ids": [],
    "group_ids": [6803811392, 6868997925, 6869043403, 6873245716, 6907385791, 6919915425],
    "issue_titles": [
      "ApiForbiddenError: <html>",
      "ApiForbiddenError: {\"message\":\"403 Forbidden - Your account has been blocked.\"}",
      "ApiForbiddenError: {"
    ],
    "title": "HTTP errors in GitLab integration requests",
    "description": "Multiple HTTP errors occurring across GitLab integration workflows including issue creation, PR comment processing, and commit context retrieval. All failures terminate in raise_for_status() indicating upstream API issues.",
    "tags": ["External System", "API", "GitLab", "HTTP Error"],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9385394295758133,
    "cluster_avg_similarity": 0.9596808204253897
  },
  {
    "cluster_id": 491,
    "project_ids": [],
    "group_ids": [6804142086, 6804142087, 6804142097, 6804142103, 6804142109, 6804142119],
    "issue_titles": ["OperationalError: canceling statement due to lock timeout"],
    "title": "Task worker PostgreSQL lock timeout on webhook table",
    "description": "Task worker processes experience lock timeouts when reading from webhook payload replica while concurrent writes occur on primary, causing replication lag and lock contention during multiprocessing operations.",
    "tags": [
      "Database",
      "Concurrency",
      "PostgreSQL",
      "Replication Lag",
      "Lock Timeout",
      "Task Processing"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.942742230475444,
    "cluster_avg_similarity": 0.9617719672948138
  },
  {
    "cluster_id": 496,
    "project_ids": [],
    "group_ids": [
      6805577213, 6805654159, 6839574206, 6844093237, 6906725648, 6909992567, 6911693548,
      6919281209, 6928177029, 6935964384, 6941517821, 6952042219, 6969398531
    ],
    "issue_titles": [
      "ApiError",
      "IntegrationResourceNotFoundError",
      "ApiError: {\"detail\":\"Internal Error\",\"errorId\":null}",
      "ApiError: <html>",
      "RetryError"
    ],
    "title": "HTTP errors in integration requests to external SCM",
    "description": "Multiple Sentry integration tasks are failing when making HTTP requests to external source code management systems, resulting in HTTPError exceptions during commit context processing, PR comments, and notifications.",
    "tags": ["External System", "API", "HTTP", "Integration", "Source Code Management"],
    "cluster_size": 13,
    "cluster_min_similarity": 0.9186051373456374,
    "cluster_avg_similarity": 0.954111041954155
  },
  {
    "cluster_id": 500,
    "project_ids": [],
    "group_ids": [6805924070, 6810188157, 6949465777],
    "issue_titles": [
      "ApiError: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/issues/comments#create-an-issue-comment\",\"status\":\"404\"}",
      "ApiError: {\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/issues/comments#update-an-issue-comment\",\"status\":\"404\"}",
      "IntegrationError: There was an error creating a comment on the Jira issue."
    ],
    "title": "GitHub/Jira API 404 errors from stale external references",
    "description": "Background tasks for PR comments and Jira issue syncing fail with 404 errors because Sentry's database contains references to external resources that no longer exist or are inaccessible.",
    "tags": ["External System", "API", "Data Integrity", "GitHub", "Jira", "Not Found"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9511252117350114,
    "cluster_avg_similarity": 0.9559048392214188
  },
  {
    "cluster_id": 503,
    "project_ids": [],
    "group_ids": [
      6806699088, 6808849248, 6808890367, 6809744703, 6810090277, 6835141589, 6835733702,
      6885263122, 6886540659, 6886638352, 6889931382, 6941958857
    ],
    "issue_titles": [
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'megabyte', 'percentage', 'integer', 'duration', 'hour', 'byte', 'pebibyte', 'nanosecond', 'minute', 'millisecond', 'kibibyte', 'week', ...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'number', 'integer', 'kilobyte', 'nanosecond', 'minute', 'gibibyte', 'gigabyte', 'second', 'microsecond', 'currency', 'week', 'byte', 'megabyte', 'exaby...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'millisecond', 'second', 'microsecond', 'day', 'exbibyte', 'gibibyte', 'integer', 'currency', 'bit', 'pebibyte', 'gigabyte', 'exabyte', ...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'petabyte', 'microsecond', 'gibibyte', 'percentage', 'mebibyte', 'nanosecond', 'second', 'gigabyte', 'bit', 'megabyte', 'millisecond', '...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'mebibyte', 'gibibyte', 'megabyte', 'hour', 'gigabyte', 'week', 'duration', 'petabyte', 'bit', 'currency', 'day', 'number', 'integer', '...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'exbibyte', 'number', 'gigabyte', 'exabyte', 'bit', 'nanosecond', 'minute', 'percentage', 'millisecond', 'duration', 'integer', 'kibibyt...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'hour', 'mebibyte', 'microsecond', 'duration', 'percentage', 'millisecond', 'kilobyte', 'nanosecond', 'exabyte', 'gibibyte', 'megabyte', 'minute', 'tera...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'byte', 'bit', 'nanosecond', 'gibibyte', 'exbibyte', 'microsecond', 'percentage', 'tebibyte', 'pebibyte', 'number', 'kibibyte', 'kilobyte', 'minute', 'w...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'kibibyte', 'gigabyte', 'microsecond', 'pebibyte', 'petabyte', 'currency', 'bit', 'percentage', 'exbibyte', 'exabyte', 'mebibyte', 'byte...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'terabyte', 'integer', 'mebibyte', 'microsecond', 'duration', 'byte', 'minute', 'gibibyte', 'second', 'kilobyte', 'gigabyte', 'number', 'currency', 'peb...",
      "SubscriptionError: id is invalid for parameter 1 in count. Its a string type field, but it must be one of these types: {'gibibyte', 'day', 'pebibyte', 'second', 'currency', 'megabyte', 'hour', 'microsecond', 'week', 'millisecond', 'mebibyte', 'kilobyte', 'petabyte', 'tebi...",
      "SubscriptionError: transaction.duration is invalid for parameter 1 in p95. Its a string type field, but it must be one of these types: {'number', 'gibibyte', 'microsecond', 'percentage', 'second', 'terabyte', 'kibibyte', 'kilobyte', 'megabyte', 'week', 'day', 'tebibyte', ..."
    ],
    "title": "InvalidSearchQuery in EAP search resolver",
    "description": "The EAP search resolver is failing to resolve functions during timeseries query processing, causing InvalidSearchQuery exceptions in Snuba task workers.",
    "tags": ["Input Validation", "API", "EAP Search Resolver", "Invalid Search Query"],
    "cluster_size": 12,
    "cluster_min_similarity": 0.9330124259961635,
    "cluster_avg_similarity": 0.964262973629692
  },
  {
    "cluster_id": 506,
    "project_ids": [],
    "group_ids": [6808438570, 6899646185],
    "issue_titles": ["OperationalError: server closed the connection unexpectedly"],
    "title": "Django PostgreSQL reconnection fails on connection drops",
    "description": "Database connection errors during request processing are not properly handled by the auto-reconnect mechanism, causing exceptions to propagate instead of attempting reconnection when PostgreSQL closes connections unexpectedly.",
    "tags": ["Database", "Django", "PostgreSQL", "Connection Reset", "Auto-Reconnect"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9805198485849034,
    "cluster_avg_similarity": 0.9805198485849034
  },
  {
    "cluster_id": 507,
    "project_ids": [],
    "group_ids": [
      6808438571, 6808438597, 6808438599, 6808438630, 6808438637, 6899646239, 6927089687,
      7021095593
    ],
    "issue_titles": ["OperationalError: server closed the connection unexpectedly"],
    "title": "Integration middleware database connection failures",
    "description": "Multiple integration parsers (Jira and GitHub) are failing when querying organization mappings due to PostgreSQL connections being unexpectedly closed, with automatic reconnection attempts also failing.",
    "tags": [
      "Database",
      "External System",
      "PostgreSQL",
      "Connection Reset",
      "Jira",
      "GitHub"
    ],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9441369236880476,
    "cluster_avg_similarity": 0.9635051107387286
  },
  {
    "cluster_id": 512,
    "project_ids": [],
    "group_ids": [6810382376, 6815823965],
    "issue_titles": ["MarketoError: Max rate limit '100' exceeded with in '20' secs"],
    "title": "Marketo API rate limit exceeded from uncached tokens",
    "description": "MarketoClient fetches a new OAuth token for every lead submission instead of caching tokens, causing 2 API calls per lead and triggering the 100 requests per 20 seconds rate limit.",
    "tags": ["External System", "API", "Rate Limiting", "Marketo", "Caching"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9727495501841998,
    "cluster_avg_similarity": 0.9727495501841998
  },
  {
    "cluster_id": 525,
    "project_ids": [],
    "group_ids": [6830486238, 6851769528],
    "issue_titles": [
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 52 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer...",
      "RateLimitExceeded: Query on could not be run due to allocation policies, info: {'details': {'ReferrerGuardRailPolicy': {'can_run': False, 'max_threads': 0, 'explanation': {'reason': 'concurrent policy 51 exceeds limit of 50', 'policy': 'referrer_guard_rail_policy', 'refer..."
    ],
    "title": "Snuba rate limit exceeded during digest delivery",
    "description": "Multiple concurrent digest delivery tasks are making simultaneous TSDB queries with the same referrer, causing the cumulative query count to exceed Snuba's 50-query concurrent limit per referrer.",
    "tags": ["Rate Limiting", "Concurrency", "External System", "Snuba", "TSDB Queries"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9862485373316149,
    "cluster_avg_similarity": 0.9862485373316149
  },
  {
    "cluster_id": 531,
    "project_ids": [],
    "group_ids": [6835170630, 6901424352],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c482443d7f0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7eae34134dd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
    ],
    "title": "Snuba connection failures in post-process snooze validation",
    "description": "Post-processing pipeline fails when validating group snoozes with user-count conditions because the code makes unconditional Snuba queries to check user counts, with no fallback when Snuba is unreachable.",
    "tags": [
      "Networking",
      "External System",
      "Connection Reset",
      "Snuba",
      "Post Processing"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9611538127046542,
    "cluster_avg_similarity": 0.9611538127046542
  },
  {
    "cluster_id": 532,
    "project_ids": [],
    "group_ids": [6837427063, 7015717611, 7016003682, 7016549063],
    "issue_titles": ["AssertionError"],
    "title": "Slack notification fails on missing workflow_id for legacy rules",
    "description": "Legacy alert rules created before the workflow-engine-ui feature lack workflow_id in their action data, causing assertion failures when the feature flag is enabled for the organization.",
    "tags": ["External System", "Configuration", "Slack", "Assertion Failure"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9708316345457381,
    "cluster_avg_similarity": 0.974065453072309
  },
  {
    "cluster_id": 538,
    "project_ids": [],
    "group_ids": [6841916270, 6848738202, 7024907832, 7024908187],
    "issue_titles": ["Cancelled: CANCELLED"],
    "title": "Bigtable operations failing in nodestore backend",
    "description": "Event storage and retrieval operations are failing when accessing Google Cloud Bigtable through the nodestore service, affecting both error and transaction event processing.",
    "tags": [
      "External System",
      "Database",
      "Google Cloud Bigtable",
      "gRPC",
      "Retries Exhausted"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9455064225613632,
    "cluster_avg_similarity": 0.9542228073040925
  },
  {
    "cluster_id": 546,
    "project_ids": [],
    "group_ids": [6849955923, 6999733631],
    "issue_titles": [
      "TypeError: '<' not supported between instances of 'TrendBundle' and 'TrendBundle'"
    ],
    "title": "Statistical detector fails comparing TrendBundle objects",
    "description": "The limit_regressions_by_project method in statistical detectors uses heapq with TrendBundle objects that lack comparison operators, causing failures when bundles have identical scores and heap operations require tie-breaking.",
    "tags": [
      "API",
      "Data Integrity",
      "Statistical Detectors",
      "Serialization",
      "Comparison Operators"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9749564389024453,
    "cluster_avg_similarity": 0.9749564389024453
  },
  {
    "cluster_id": 547,
    "project_ids": [],
    "group_ids": [
      6851323218, 6851485112, 6852696200, 6853692146, 6853692147, 7021618233, 7022366196,
      7022895146
    ],
    "issue_titles": [
      "SubscriptionError: \"is:\" queries are not supported in this search."
    ],
    "title": "Unsupported 'is:' query syntax in search parser",
    "description": "Search queries containing 'is:' syntax are being rejected by the event search parser in Snuba timeseries requests, causing subscription creation failures.",
    "tags": ["Input Validation", "API", "Search Parser", "Invalid Search Query"],
    "cluster_size": 8,
    "cluster_min_similarity": 0.9647050640984949,
    "cluster_avg_similarity": 0.9766586054559651
  },
  {
    "cluster_id": 548,
    "project_ids": [],
    "group_ids": [6852547059, 7015139120],
    "issue_titles": [
      "QueryExecutionError: DB::Exception: Function tuple requires at least one argument.: While processing tuple(): While processing ((project_id AS _snuba_project_id) IN tuple(4506041650577408)) AND ((deleted = 0) AND ((timestamp AS _snuba_timestamp) >= toDateTime('2025-08-15T07...",
      "QueryExecutionError: Connection reset by peer (10.0.0.1:9010)"
    ],
    "title": "ClickHouse tuple() function called with empty arguments",
    "description": "Empty group_id filter from issue.id search generates invalid ClickHouse query with tuple() having no arguments, likely due to non-existent issue ID lookup.",
    "tags": ["Database", "Input Validation", "ClickHouse", "Query Execution Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9602988299977944,
    "cluster_avg_similarity": 0.9602988299977944
  },
  {
    "cluster_id": 549,
    "project_ids": [],
    "group_ids": [6853256343, 6939002409],
    "issue_titles": ["IdentityNotValid"],
    "title": "OAuth2 token refresh HTTP error in task worker",
    "description": "Task worker processes are failing to refresh OAuth2 tokens due to HTTP errors from the identity provider, causing authentication failures in background tasks.",
    "tags": ["Authentication", "External System", "OAuth2", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9812338849788645,
    "cluster_avg_similarity": 0.9812338849788645
  },
  {
    "cluster_id": 555,
    "project_ids": [],
    "group_ids": [6862198431, 6862198487],
    "issue_titles": [
      "ApiInvalidRequestError: {\"error\":{\"code\":\"bad_request\",\"message\":\"eod must be between period.start and period.end\"}}"
    ],
    "title": "Vercel API calls failing with HTTP errors",
    "description": "Task workers executing Vercel API calls are encountering HTTP errors, causing the requests library to raise HTTPError exceptions when checking response status.",
    "tags": ["External System", "API", "Vercel", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9732776121164998,
    "cluster_avg_similarity": 0.9732776121164998
  },
  {
    "cluster_id": 557,
    "project_ids": [],
    "group_ids": [6867730427, 6874853205, 6876037064],
    "issue_titles": [
      "SSLError: HTTPSConnectionPool(host='api.codecov.io', port=443): Max retries exceeded with url: /webhooks/sentry (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_DECODE_ERROR] tlsv1 alert decode error (_ssl.c:1018)')))",
      "SSLError: HTTPSConnectionPool(host='api.codecov.io', port=443): Max retries exceeded with url: /webhooks/sentry (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1018)')))"
    ],
    "title": "SSL handshake fails on Codecov webhook delivery",
    "description": "Multiple webhook deliveries to Codecov API fail during SSL handshake with EOF errors. Each webhook creates a new client instance without session persistence, causing connection reuse issues when processing webhooks in rapid succession.",
    "tags": ["External System", "Networking", "TLS", "TLS Handshake Failure", "Codecov"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9589798131632583,
    "cluster_avg_similarity": 0.9628439829612908
  },
  {
    "cluster_id": 563,
    "project_ids": [],
    "group_ids": [6868873699, 6870676475],
    "issue_titles": ["ApiForbiddenError: {"],
    "title": "GitHub commit context API rate limit as 403 error",
    "description": "GitHub's secondary rate limits return 403 Forbidden instead of 429, causing ApiForbiddenError to be thrown instead of ApiRateLimitedError, bypassing proper rate limit handling in commit context processing.",
    "tags": ["External System", "Rate Limiting", "API", "GitHub", "HTTP 403"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9687112743740326,
    "cluster_avg_similarity": 0.9687112743740326
  },
  {
    "cluster_id": 564,
    "project_ids": [],
    "group_ids": [6868949389, 7016793496],
    "issue_titles": [
      "ApiForbiddenError: {\"error\":{\"code\":\"ConversationBlockedByUser\",\"message\":\"User blocked the conversation with the bot.\"}}"
    ],
    "title": "MS Teams notification failed for blocked bot user",
    "description": "Microsoft Teams notification delivery failed because the user blocked the bot conversation, but the error handling doesn't distinguish this permanent failure from transient errors.",
    "tags": ["External System", "API", "Microsoft Teams", "Permission Denied"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.962292616438515,
    "cluster_avg_similarity": 0.962292616438515
  },
  {
    "cluster_id": 566,
    "project_ids": [],
    "group_ids": [6869016571, 7008488610],
    "issue_titles": [
      "ApiForbiddenError: {\"error\":{\"code\":\"TenantNotFound\",\"message\":\"Tenant cannot be found.\"}}"
    ],
    "title": "MS Teams API tenant mismatch in conversation lookup",
    "description": "MS Teams integration fails when creating user conversations due to improper tenant ID handling, causing the API to return TenantNotFound errors during notification delivery.",
    "tags": [
      "External System",
      "API",
      "Authentication",
      "Microsoft Teams",
      "Tenant Validation"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9608493548717465,
    "cluster_avg_similarity": 0.9608493548717465
  },
  {
    "cluster_id": 568,
    "project_ids": [],
    "group_ids": [6869438488, 6997235255],
    "issue_titles": ["ValueError: Sentry app config must contain name and value keys"],
    "title": "Sentry app config validation rejects list values",
    "description": "The SentryAppFormConfigDataBlob validation only accepts string, None, or int values, but legitimate multi-select fields like assignee_ids and labels contain list values, causing workflow action processing to fail.",
    "tags": [
      "Configuration",
      "Input Validation",
      "Workflow Engine",
      "Sentry App",
      "Type Mismatch"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9611604612114057,
    "cluster_avg_similarity": 0.9611604612114057
  },
  {
    "cluster_id": 569,
    "project_ids": [],
    "group_ids": [6869664533, 6882646160],
    "issue_titles": [
      "ApiForbiddenError: {\"message\":\"Resource not accessible by integration\",\"documentation_url\":\"https://docs.github.com/rest/issues/comments#create-an-issue-comment\",\"status\":\"403\"}"
    ],
    "title": "GitHub API rejecting PR comments with actions field",
    "description": "GitHub integration fails when creating PR comments because the 'actions' field with copilot-chat type is not supported by GitHub's REST API for issue comments, causing 403 Forbidden errors.",
    "tags": ["External System", "API", "GitHub", "HTTPError", "Unsupported Field"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9836958969821558,
    "cluster_avg_similarity": 0.9836958969821558
  },
  {
    "cluster_id": 573,
    "project_ids": [],
    "group_ids": [6870329363, 6873787846],
    "issue_titles": [
      "ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
    ],
    "title": "Webhook delivery to Codecov fails on connection reset",
    "description": "Webhook POST requests to Codecov are failing with SSL connection reset errors during the TLS handshake, preventing successful delivery of webhook payloads.",
    "tags": ["External System", "Networking", "TLS", "Codecov", "Connection Reset"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9629285766728167,
    "cluster_avg_similarity": 0.9629285766728167
  },
  {
    "cluster_id": 576,
    "project_ids": [],
    "group_ids": [6871341515, 6985404975],
    "issue_titles": [
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /functions/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78f564199a30>: Failed to establish a new connection: [Errno 111] Connection refus...",
      "SnubaError: HTTPConnectionPool(host='snuba-api', port=80): Max retries exceeded with url: /events/snql (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bc8520156d0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
    ],
    "title": "Snuba API connection refused on port 80",
    "description": "Statistical detectors and Slack notifications failing to connect to Snuba API service configured at port 80 instead of standard port 1218, causing connection refused errors.",
    "tags": [
      "External System",
      "Configuration",
      "Networking",
      "Connection Refused",
      "Snuba"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9663262693060255,
    "cluster_avg_similarity": 0.9663262693060255
  },
  {
    "cluster_id": 590,
    "project_ids": [],
    "group_ids": [6881575261, 6906573727, 7024096388],
    "issue_titles": [
      "ApiForbiddenError: <html>",
      "ApiForbiddenError: {\"message\":\"403 Forbidden - Your account has been blocked.\"}",
      "ApiError: <!DOCTYPE html >"
    ],
    "title": "GitLab OAuth token refresh fails with 403 Forbidden",
    "description": "GitLab integration fails when OAuth token refresh returns 403 'Your account has been blocked', which isn't properly handled as an invalid identity error in the OAuth2Provider.",
    "tags": ["External System", "Authentication", "API", "GitLab", "HTTP Error"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9356335438024878,
    "cluster_avg_similarity": 0.9524511446299889
  },
  {
    "cluster_id": 593,
    "project_ids": [],
    "group_ids": [6882567249, 7006667210],
    "issue_titles": [
      "ValueError: Field 'user_id' expected a number but got 'me'.",
      "ValueError: Field 'id' expected a number but got ''."
    ],
    "title": "Alert rule validation fails on 'me' target identifier",
    "description": "Django alert rule trigger action validation attempts to convert the string 'me' to an integer for user_id filtering without resolving it to the actual user ID first, causing a ValueError when int('me') is called.",
    "tags": ["API", "Input Validation", "Django", "Type Conversion Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9561916519304157,
    "cluster_avg_similarity": 0.9561916519304157
  },
  {
    "cluster_id": 596,
    "project_ids": [],
    "group_ids": [6882939029, 6886352273],
    "issue_titles": ["KeyError: <ExternalProviders.MSTEAMS: 120>"],
    "title": "MS Teams notification provider not registered",
    "description": "MS Teams notification handler fails to register during taskworker initialization due to missing module imports, causing KeyError when dispatching notifications to users with MS Teams preferences.",
    "tags": ["Configuration", "Queueing", "MS Teams", "Registry Missing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9697363886936679,
    "cluster_avg_similarity": 0.9697363886936679
  },
  {
    "cluster_id": 603,
    "project_ids": [],
    "group_ids": [6886744447, 6927466647, 6930058038, 6942141693],
    "issue_titles": [
      "HTTPError: 400 Client Error: Bad Request for url: https://api.codecov.io/sentry/internal/account/link/",
      "HTTPError: 400 Client Error: Bad Request for url: https://api.codecov.io/sentry/internal/account/unlink/"
    ],
    "title": "Codecov API rejecting account link requests",
    "description": "GitHub Codecov integration tasks are failing when attempting to link accounts, receiving HTTP 400 Bad Request responses from Codecov's API due to invalid request data format or incorrect field values.",
    "tags": ["External System", "API", "Codecov", "HTTP 400"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9605007971939832,
    "cluster_avg_similarity": 0.9694047194824416
  },
  {
    "cluster_id": 604,
    "project_ids": [],
    "group_ids": [6886762810, 6952679837, 6977860716],
    "issue_titles": ["SnubaRPCError: code: 500", "SnubaRPCError: code: 408"],
    "title": "Snuba RPC allocation policy rejecting trace queries",
    "description": "Snuba's routing strategy is rejecting trace data queries due to allocation policies, preventing autofix tasks and trace metrics requests from executing successfully.",
    "tags": ["External System", "Resource Limits", "Snuba", "RPC", "Allocation Policy"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9489632013203877,
    "cluster_avg_similarity": 0.9562578659321286
  },
  {
    "cluster_id": 607,
    "project_ids": [],
    "group_ids": [6889454671, 6918983553],
    "issue_titles": ["IntegrationConfigurationError: Identity not found."],
    "title": "GitLab integration missing default identity",
    "description": "GitLab integration operations are failing because the default identity is not configured, affecting both stacktrace linking and commit context processing.",
    "tags": ["External System", "Configuration", "GitLab", "Identity.DoesNotExist"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9648490057735272,
    "cluster_avg_similarity": 0.9648490057735272
  },
  {
    "cluster_id": 614,
    "project_ids": [],
    "group_ids": [
      6895696128, 6896297248, 6901434403, 6901439154, 6901440023, 6901566724, 6920892966
    ],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='http.url is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='transaction.duration is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='account.id is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='app_name is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='url is not a tag in the metrics dataset', code='parse_error')}"
    ],
    "title": "Metric alert chart generation fails with invalid issue filter",
    "description": "Chart generation for metric alerts fails when processing queries with multiple excluded issues due to formatting mismatch in the issue filter parsing logic. The issue short IDs are being converted to a Python list string representation instead of being properly parsed as comma-separated values.",
    "tags": ["API", "Input Validation", "Query Parsing", "Metric Alerts"],
    "cluster_size": 7,
    "cluster_min_similarity": 0.9469127590860755,
    "cluster_avg_similarity": 0.9627841077220832
  },
  {
    "cluster_id": 616,
    "project_ids": [],
    "group_ids": [6897002430, 6897503671, 6950824391, 7015335147],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='transaction.duration is not a tag in the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='Cannot query apdex with a threshold parameter on the metrics dataset', code='parse_error')}",
      "ApiError: status=400 body={'detail': ErrorDetail(string='url is not a tag in the metrics dataset', code='parse_error')}"
    ],
    "title": "Metric alert chart fails on apdex with threshold",
    "description": "Chart building for metric alert emails incorrectly maps PerformanceMetrics dataset to 'metrics' dataset, which doesn't support apdex() functions with threshold parameters like apdex(300).",
    "tags": ["API", "Configuration", "Apdex", "Dataset Mapping", "Email Notifications"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9401784613652103,
    "cluster_avg_similarity": 0.9550841300483316
  },
  {
    "cluster_id": 620,
    "project_ids": [],
    "group_ids": [6900302081, 7016155264],
    "issue_titles": [
      "IntegrationConfigurationError: GitHub App lacks permissions to create check runs. Please ensure the app has the required permissions and that the organization has accepted any updated permissions.",
      "ApiForbiddenError: {\"message\":\"Resource not accessible by integration\",\"documentation_url\":\"https://docs.github.com/rest/checks/runs#create-a-check-run\",\"status\":\"403\"}"
    ],
    "title": "GitHub App lacks permissions for status checks",
    "description": "A GitHub integration is receiving 403 'Resource not accessible by integration' errors when attempting to create check runs, indicating the GitHub App installation lacks the required 'checks' write permission or needs re-approval after permission changes.",
    "tags": ["External System", "Authorization", "API", "GitHub", "Permission Denied"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9759683617918794,
    "cluster_avg_similarity": 0.9759683617918794
  },
  {
    "cluster_id": 623,
    "project_ids": [],
    "group_ids": [6901116707, 6905848242],
    "issue_titles": ["SnubaRPCError: code: 400"],
    "title": "Snuba RPC failures in events and timeseries queries",
    "description": "API endpoints for organization events and timeseries data are encountering SnubaRPCError when making remote procedure calls to the Snuba query service.",
    "tags": ["External System", "API", "Snuba", "RPC Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.964762575057227,
    "cluster_avg_similarity": 0.964762575057227
  },
  {
    "cluster_id": 626,
    "project_ids": [],
    "group_ids": [6902559572, 6903628022, 6921588332],
    "issue_titles": [
      "ConnectionError: SafeHTTPSConnectionPool(host='your-endpoint.example.com', port=443): Max retries exceeded with url: /sentry (Caused by NewConnectionError('<sentry.net.http.SafeHTTPSConnection object at 0x79aa78464e10>: Failed to establish a new connection: [Errno -2] N...",
      "ConnectionError: SafeHTTPSConnectionPool(host='your-endpoint.example.com', port=443): Max retries exceeded with url: /sentry (Caused by NewConnectionError('<sentry.net.http.SafeHTTPSConnection object at 0x7ce5f08f6990>: Failed to establish a new connection: [Errno -2] N...",
      "ConnectionError: SafeHTTPSConnectionPool(host='your-endpoint.example.com', port=443): Max retries exceeded with url: /sentry (Caused by NewConnectionError('<sentry.net.http.SafeHTTPSConnection object at 0x7b81102f0b90>: Failed to establish a new connection: [Errno -2] N..."
    ],
    "title": "DNS resolution failure in Sentry task worker",
    "description": "Service hook processing fails due to unresolvable hostname during HTTP connection establishment. Task worker attempts DNS lookup for configured webhook URL but receives 'Name or service not known' error.",
    "tags": ["Networking", "DNS Resolution Failure", "Task Worker", "Service Hook"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9615391245301791,
    "cluster_avg_similarity": 0.966331164578774
  },
  {
    "cluster_id": 627,
    "project_ids": [],
    "group_ids": [6904636886, 7002716279],
    "issue_titles": ["TypeError: unhashable type: 'list'"],
    "title": "Alert rule validation fails on failure_count() function",
    "description": "The failure_count() function has a malformed aggregate structure that returns a complex nested list instead of a column name, causing unhashable type errors during alert rule validation.",
    "tags": ["Input Validation", "API", "Configuration", "Serialization"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9829235555439736,
    "cluster_avg_similarity": 0.9829235555439736
  },
  {
    "cluster_id": 632,
    "project_ids": [],
    "group_ids": [6906447794, 6906447795],
    "issue_titles": [
      "ExportError: Internal error. Please try again.",
      "SnubaRPCError: code: 500"
    ],
    "title": "Snuba RPC memory errors not handled as recoverable",
    "description": "Data export tasks fail permanently when ClickHouse memory limits are exceeded via RPC calls because SnubaRPCError doesn't parse error codes to convert them to recoverable exception types like QueryMemoryLimitExceeded.",
    "tags": [
      "External System",
      "API",
      "Data Export",
      "Snuba",
      "Resource Limits",
      "Error Handling"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9685138977580717,
    "cluster_avg_similarity": 0.9685138977580717
  },
  {
    "cluster_id": 634,
    "project_ids": [],
    "group_ids": [6907167829, 6907577877],
    "issue_titles": ["ApiError"],
    "title": "SCM integration file check HTTP errors",
    "description": "Stacktrace link generation is failing when checking file existence in source code management repositories, resulting in HTTP errors from upstream SCM providers.",
    "tags": ["External System", "API", "Source Code Management", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.971355689838337,
    "cluster_avg_similarity": 0.971355689838337
  },
  {
    "cluster_id": 638,
    "project_ids": [],
    "group_ids": [6909721758, 6925947524],
    "issue_titles": [
      "ValueError: AlertRuleWorkflow not found when querying for AlertRuleWorkflow"
    ],
    "title": "AlertRuleWorkflow query using non-existent workflow_id field",
    "description": "Code attempts to query AlertRuleWorkflow using workflow_id field that doesn't exist on the model, causing DoesNotExist exceptions in notification workflow actions.",
    "tags": [
      "Database",
      "Data Integrity",
      "Django",
      "Schema Migration",
      "Model Query Error"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9641963155820961,
    "cluster_avg_similarity": 0.9641963155820961
  },
  {
    "cluster_id": 644,
    "project_ids": [],
    "group_ids": [6913163064, 6916986999],
    "issue_titles": ["TypeError: Recursion limit reached"],
    "title": "Slack issue summary generation fails in Seer call",
    "description": "Issue summary generation is failing when building Slack notifications for alerts and activity notifications, with errors occurring during the Seer API call for AI-generated summaries.",
    "tags": ["External System", "API", "Slack", "Serialization"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9790142853885426,
    "cluster_avg_similarity": 0.9790142853885426
  },
  {
    "cluster_id": 651,
    "project_ids": [],
    "group_ids": [6919087350, 6919934306, 6929156762, 7006684726, 7016370589],
    "issue_titles": [
      "IntegrationConfigurationError: {\"errorMessages\":[\"Please update story points (enter 0 if 0)\",\"Kindly enter the Dev Owner\"],\"errors\":{}}",
      "ApiInvalidRequestError: {\"errorMessages\":[],\"errors\":{\"resolution\":\"Resolution is required.\"}}",
      "IntegrationConfigurationError: {\"errorMessages\":[\"You should define estimation (Story Points).\"],\"errors\":{}}",
      "IntegrationConfigurationError: {\"errorMessages\":[\"Action 101 is invalid\"],\"errors\":{}}"
    ],
    "title": "Jira status sync fails on required field validation",
    "description": "Outbound status sync tasks are failing because Jira workflows require additional fields (Story Points, Target dates, Resolution) during transitions, but the integration only sends transition IDs without checking or providing required field values.",
    "tags": [
      "External System",
      "API",
      "Input Validation",
      "Jira",
      "Constraint Violation"
    ],
    "cluster_size": 5,
    "cluster_min_similarity": 0.9587193599660989,
    "cluster_avg_similarity": 0.971329165830039
  },
  {
    "cluster_id": 654,
    "project_ids": [],
    "group_ids": [6919653814, 6919888464],
    "issue_titles": [
      "IntegrationFormError: {'parent': ['Could not find issue by id or key.']}"
    ],
    "title": "HTTP errors in post-processing rule callbacks",
    "description": "Task workers are encountering HTTP errors when executing post-processing rule callbacks, causing failures in the event processing pipeline.",
    "tags": ["API", "External System", "HTTP Error", "Post Processing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9639101307934892,
    "cluster_avg_similarity": 0.9639101307934892
  },
  {
    "cluster_id": 655,
    "project_ids": [],
    "group_ids": [6919862658, 6931488484, 6950833988],
    "issue_titles": [
      "IntegrationConfigurationError: {\"errorMessages\":[\"Fields Resolution and Regression must not be empty when closing this issue.\"],\"errors\":{}}",
      "IntegrationConfigurationError: {\"errorMessages\":[\"You should define estimation (Story Points).\"],\"errors\":{}}"
    ],
    "title": "Jira integration fails on transitions with required fields",
    "description": "Outbound status sync to Jira fails when workflow transitions have mandatory fields configured, as the integration only sends transition ID without checking for or populating required fields like justification comments.",
    "tags": ["External System", "API", "Jira", "Input Validation"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9480567495588839,
    "cluster_avg_similarity": 0.9609871229231418
  },
  {
    "cluster_id": 661,
    "project_ids": [],
    "group_ids": [6922543242, 6969839823],
    "issue_titles": ["RetryError"],
    "title": "PagerDuty workflow actions fail from rate limiting",
    "description": "Workflow engine treats rate limit errors (429) as retryable exceptions, causing tasks to retry 3 times with 5-second delays instead of backing off when PagerDuty API requests exceed rate limits.",
    "tags": ["External System", "Rate Limiting", "API", "PagerDuty", "Retries Exhausted"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9664169139089308,
    "cluster_avg_similarity": 0.9664169139089308
  },
  {
    "cluster_id": 666,
    "project_ids": [],
    "group_ids": [6926147545, 6927273924, 6940106787, 6942355371],
    "issue_titles": [
      "ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"
    ],
    "title": "API error in metric alert chart generation for notifications",
    "description": "Metric alert notifications to Slack and Discord are failing when trying to fetch chart data via internal API calls, raising ApiError from the client request.",
    "tags": ["API", "External System", "Sentry API Client", "Metric Alerts", "API Error"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9772235602199371,
    "cluster_avg_similarity": 0.9837206137461778
  },
  {
    "cluster_id": 668,
    "project_ids": [],
    "group_ids": [6928230758, 7019625543],
    "issue_titles": [
      "ValidationError: Invalid config: Additional properties are not allowed ('options' was unexpected)"
    ],
    "title": "Workflow action config validation fails during rule update",
    "description": "Issue alert rule updates are failing when migrating to the workflow engine due to invalid action configuration that doesn't pass schema validation.",
    "tags": ["API", "Input Validation", "Schema Migration", "Validation Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9720523587555389,
    "cluster_avg_similarity": 0.9720523587555389
  },
  {
    "cluster_id": 692,
    "project_ids": [],
    "group_ids": [6948437711, 7023348768],
    "issue_titles": ["KeyError: 'statusCode'"],
    "title": "Replay log summarization KeyError on statusCode field",
    "description": "The replay log summarization process is failing when attempting to access the statusCode field from payload data, indicating a data structure mismatch or missing field in the expected format.",
    "tags": ["API", "Data Integrity", "Serialization", "Sentry", "KeyError"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9827470502596255,
    "cluster_avg_similarity": 0.9827470502596255
  },
  {
    "cluster_id": 698,
    "project_ids": [],
    "group_ids": [6952625620, 7017047800],
    "issue_titles": [
      "ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"
    ],
    "title": "Discord metric alert fails on missing AlertRuleDetector",
    "description": "AlertRuleDetector lookup fails silently, causing API to query incidents with detector ID instead of alert rule ID, resulting in 404 error when rendering Discord notification charts.",
    "tags": [
      "API",
      "Data Integrity",
      "External System",
      "Discord",
      "Metric Alert",
      "Missing Record"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9681661438173886,
    "cluster_avg_similarity": 0.9681661438173886
  },
  {
    "cluster_id": 712,
    "project_ids": [],
    "group_ids": [6963221036, 6963221055, 6963221131, 7003895835, 7003973300, 7004048071],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "Django database connection failures in async context",
    "description": "Synchronous database operations are being executed within an async context across multiple request types, triggering Django's SynchronousOnlyOperation checks and causing PostgreSQL connection failures on port 6432.",
    "tags": [
      "Database",
      "Concurrency",
      "Django",
      "PostgreSQL",
      "Connection Refused",
      "Async Context"
    ],
    "cluster_size": 6,
    "cluster_min_similarity": 0.9358351792990564,
    "cluster_avg_similarity": 0.9510406350118957
  },
  {
    "cluster_id": 715,
    "project_ids": [],
    "group_ids": [6963223239, 6984577065],
    "issue_titles": [
      "OutboxFlushError: Could not flush shard category=33 (ORG_AUTH_TOKEN_UPDATE)",
      "OutboxFlushError: Could not flush shard category=0 (USER_UPDATE)"
    ],
    "title": "Hybrid cloud RPC calls failing with invalid service request",
    "description": "Cross-silo RPC calls in the hybrid cloud architecture are failing during async replication processing, indicating potential service contract mismatches or configuration issues between silos.",
    "tags": ["Hybrid Cloud", "RPC", "Async Replication", "Invalid Service Request"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9678918615262522,
    "cluster_avg_similarity": 0.9678918615262522
  },
  {
    "cluster_id": 716,
    "project_ids": [],
    "group_ids": [6964281150, 6964311697],
    "issue_titles": [
      "Cannot remove BillingSeatAssignment that doesn't exist",
      "Cannot disable BillingSeatAssignment that doesn't exist"
    ],
    "title": "BillingSeatAssignment missing for uptime detector cleanup",
    "description": "When disabling broken uptime detectors, the system attempts to remove billing seat assignments that don't exist, likely due to detectors created without seats or race conditions in billing history transitions.",
    "tags": [
      "Database",
      "Configuration",
      "Uptime Monitoring",
      "Billing",
      "Constraint Violation"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9562530837655712,
    "cluster_avg_similarity": 0.9562530837655712
  },
  {
    "cluster_id": 722,
    "project_ids": [],
    "group_ids": [6965989772, 6977978467],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=15)"
    ],
    "title": "Seer spam detection service timeout during feedback processing",
    "description": "Timeout errors when calling the Seer spam detection API during feedback event processing. The Seer service is taking longer than the 15-second timeout to respond, likely due to slow Google Gemini API calls that can take 18+ seconds.",
    "tags": ["External System", "API", "Timeout", "Seer", "Feedback Processing"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9588282672214338,
    "cluster_avg_similarity": 0.9588282672214338
  },
  {
    "cluster_id": 730,
    "project_ids": [],
    "group_ids": [6969557733, 7003688259],
    "issue_titles": [
      "ApiError: status=404 body={'detail': ErrorDetail(string='The requested resource does not exist', code='error')}"
    ],
    "title": "Slack metric alert chart fails with API 404",
    "description": "Slack notifications for metric alerts fail when trying to fetch incident data for chart generation. The API request uses an invalid project ID (-1) causing the incidents endpoint to return 404.",
    "tags": ["API", "External System", "Slack", "Invalid Parameter"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9663369346113777,
    "cluster_avg_similarity": 0.9663369346113777
  },
  {
    "cluster_id": 735,
    "project_ids": [],
    "group_ids": [6977926819, 6977946585, 6977955995],
    "issue_titles": [
      "ReadTimeoutError: HTTPConnectionPool(host='seer-web-summarization', port=80): Read timed out. (read timeout=15)"
    ],
    "title": "Seer spam detection timeout from connection pool saturation",
    "description": "Feedback spam detection requests are timing out after 15 seconds due to insufficient HTTP connection pool capacity (10 connections) for concurrent workers, causing requests to queue and exceed the timeout limit.",
    "tags": [
      "Networking",
      "External System",
      "Resource Limits",
      "Timeout",
      "Connection Pool Exhaustion",
      "Seer"
    ],
    "cluster_size": 3,
    "cluster_min_similarity": 0.978668091665147,
    "cluster_avg_similarity": 0.9819217811939366
  },
  {
    "cluster_id": 758,
    "project_ids": [],
    "group_ids": [6982119219, 6996385989],
    "issue_titles": ["Project.DoesNotExist: Project matching query does not exist."],
    "title": "SDK crash detection project not found in database",
    "description": "SDK crash monitoring fails during post-processing when attempting to report detected crashes to configured project ID 4505469596663808, which doesn't exist in the current database/region.",
    "tags": ["Database", "Configuration", "SDK Crash Detection", "Project Not Found"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9791972291959732,
    "cluster_avg_similarity": 0.9791972291959732
  },
  {
    "cluster_id": 745,
    "project_ids": [],
    "group_ids": [6983217934, 7011813887, 7014444250],
    "issue_titles": [
      "SubscriptionError: ReservedBudgetHistory not updated while recomputing reserved budget spend"
    ],
    "title": "Billing task fails on reserved budget lock timeout",
    "description": "Usage buffer flush task fails when unable to acquire reserved budget lock within timeout, causing subscription error due to lock wrapper returning None instead of handling timeout gracefully.",
    "tags": ["Concurrency", "Queueing", "Database", "Timeout", "Lock Contention"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9678544648381971,
    "cluster_avg_similarity": 0.9729331645893226
  },
  {
    "cluster_id": 763,
    "project_ids": [],
    "group_ids": [6993631144, 6998456211, 7006591206, 7013410795],
    "issue_titles": ["OperationalError: canceling statement due to user request"],
    "title": "Task deadline timeout causing database query cancellation",
    "description": "Database queries in worker tasks are being cancelled when execution exceeds the configured processing deadline (60-600 seconds), with SIGALRM signal interrupting the connection mid-query.",
    "tags": ["Database", "Queueing", "PostgreSQL", "Timeout", "Signal Handling"],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9481014808930602,
    "cluster_avg_similarity": 0.9615555917732821
  },
  {
    "cluster_id": 759,
    "project_ids": [],
    "group_ids": [6997376086, 7001416547],
    "issue_titles": ["RuntimeError: dictionary changed size during iteration"],
    "title": "Kafka producer race condition in metrics tracking",
    "description": "Multiple threads accessing the shared ConfluentProducer's metrics dictionary simultaneously, causing dictionary modification during iteration when processing monitor check-ins and replay recordings concurrently.",
    "tags": ["Concurrency", "Messaging", "Kafka", "Race Condition"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9774031655055456,
    "cluster_avg_similarity": 0.9774031655055456
  },
  {
    "cluster_id": 769,
    "project_ids": [],
    "group_ids": [7001535740, 7006047416, 7006860226, 7015264037],
    "issue_titles": [
      "ProcessingDeadlineExceeded: execution deadline of 5 seconds exceeded by sentry.sentry_apps.tasks.sentry_apps.send_resource_change_webhook"
    ],
    "title": "Webhook task timeout on Redis cluster DNS lookup",
    "description": "The send_resource_change_webhook task times out after 5 seconds when attempting to connect to Redis cluster for logging. DNS resolution of the Redis hostname hangs, causing the task deadline to be exceeded after the webhook HTTP request completes successfully.",
    "tags": [
      "Networking",
      "Caching",
      "DNS Resolution Failure",
      "Redis",
      "Processing Deadline Exceeded"
    ],
    "cluster_size": 4,
    "cluster_min_similarity": 0.9765192863425318,
    "cluster_avg_similarity": 0.9797724115105813
  },
  {
    "cluster_id": 772,
    "project_ids": [],
    "group_ids": [7001742428, 7016470239],
    "issue_titles": [
      "DataCondition.MultipleObjectsReturned: get() returned more than one DataCondition -- it returned 2!"
    ],
    "title": "Workflow action serializer fails on multi-trigger alerts",
    "description": "The alert rule serializer encounters a MultipleObjectsReturned error when trying to serialize actions that are associated with multiple triggers (e.g., warning and critical thresholds), as the query returns multiple detector conditions instead of a single one.",
    "tags": [
      "Serialization",
      "Database",
      "Workflow Engine",
      "Django",
      "MultipleObjectsReturned"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9718135725011368,
    "cluster_avg_similarity": 0.9718135725011368
  },
  {
    "cluster_id": 777,
    "project_ids": [],
    "group_ids": [7003973301, 7003973322],
    "issue_titles": [
      "OperationalError: connection to server at \"127.0.0.1\", port 6432 failed: Connection refused"
    ],
    "title": "Migration job fails accessing usage replica database",
    "description": "Dashboard migration job encounters connection failures when checking feature flags because the usage_replica database at port 6432 is unavailable, causing repeated connection refused errors during subscription lookups.",
    "tags": [
      "Database",
      "Configuration",
      "PostgreSQL",
      "Connection Reset",
      "Background Job"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9689167846345713,
    "cluster_avg_similarity": 0.9689167846345713
  },
  {
    "cluster_id": 783,
    "project_ids": [],
    "group_ids": [
      7004867863, 7004867951, 7004867957, 7004868003, 7009473367, 7009473561, 7009542153,
      7009583136, 7009583523, 7010116084
    ],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth state validation fails due to session expiration",
    "description": "VSTS OAuth flow fails when the Redis session storing the state token expires before the OAuth callback completes, causing state parameter mismatch validation errors.",
    "tags": [
      "Authentication",
      "External System",
      "Caching",
      "Session Timeout",
      "OAuth",
      "Redis",
      "VSTS"
    ],
    "cluster_size": 10,
    "cluster_min_similarity": 0.9318147627382802,
    "cluster_avg_similarity": 0.9575969248404272
  },
  {
    "cluster_id": 804,
    "project_ids": [],
    "group_ids": [7009542281, 7009583311, 7010182441],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth state validation bypassed in login flow",
    "description": "OAuth2LoginView advances to callback step without validating state parameter, causing mismatched state errors when users cancel OAuth consent or encounter callback errors.",
    "tags": ["Authentication", "OAuth", "Input Validation", "State Mismatch"],
    "cluster_size": 3,
    "cluster_min_similarity": 0.9593220360110416,
    "cluster_avg_similarity": 0.9604324220682597
  },
  {
    "cluster_id": 805,
    "project_ids": [],
    "group_ids": [7009542527, 7009583667],
    "issue_titles": ["PipelineError: An error occurred while validating your request."],
    "title": "OAuth state mismatch from corrupted callback parameters",
    "description": "Google OAuth callback is returning malformed parameters causing state validation failures. The corrupted state value suggests URL encoding issues or intermediary manipulation of OAuth callback parameters.",
    "tags": [
      "Authentication",
      "External System",
      "OAuth",
      "Google",
      "Parameter Corruption"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9594483046690255,
    "cluster_avg_similarity": 0.9594483046690255
  },
  {
    "cluster_id": 811,
    "project_ids": [],
    "group_ids": [7012239361, 7014169330],
    "issue_titles": [
      "ApiError: status=400 body={'detail': ErrorDetail(string='Your interval and date range would create too many results. Use a larger interval, or a smaller date range.', code='parse_error')}"
    ],
    "title": "Sessions API limit exceeded for crash-free alert charts",
    "description": "Chart generation for crash-free metric alerts fails when the alert's 60-minute time window is applied to extended visualization date ranges, creating too many data points for the sessions API limit.",
    "tags": [
      "API",
      "External System",
      "Rate Limiting",
      "Sessions API",
      "Metric Alerts",
      "Data Points Limit"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9727069346753893,
    "cluster_avg_similarity": 0.9727069346753893
  },
  {
    "cluster_id": 818,
    "project_ids": [],
    "group_ids": [7013927465, 7015997339],
    "issue_titles": [
      "JSONDecodeError: unexpected end of data: line 1 column 65793 (char 65792)",
      "JSONDecodeError: unexpected end of data: line 1 column 53630 (char 53629)"
    ],
    "title": "Webhook JSON truncation from multiple request.body reads",
    "description": "Django webhook handlers are making multiple calls to request.body, causing WSGI stream truncation at ~50KB and resulting in incomplete JSON parsing for large payloads from GitLab and VSTS integrations.",
    "tags": [
      "API",
      "Serialization",
      "Django",
      "Request Body Truncation",
      "Webhook Processing"
    ],
    "cluster_size": 2,
    "cluster_min_similarity": 0.955179105622461,
    "cluster_avg_similarity": 0.955179105622461
  },
  {
    "cluster_id": 842,
    "project_ids": [],
    "group_ids": [7021539800, 7021540234],
    "issue_titles": ["SubscriptionIntegrityError: Cannot start plan trial"],
    "title": "Database execution error in Sentry API handler",
    "description": "Database queries are failing during API request processing, causing exceptions to be handled and captured by Sentry's error reporting system.",
    "tags": ["Database", "API", "Django", "PostgreSQL"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9822458370758101,
    "cluster_avg_similarity": 0.9822458370758101
  },
  {
    "cluster_id": 843,
    "project_ids": [],
    "group_ids": [7021744342, 7024563637],
    "issue_titles": [
      "HTTPError: 500 Server Error: Internal Server Error for url: https://api.github.com/user",
      "HTTPError: 500 Server Error: Internal Server Error for url: https://api.github.com/user/emails"
    ],
    "title": "GitHub OAuth identity pipeline HTTP errors",
    "description": "GitHub identity provider is failing during OAuth pipeline execution when making API calls to retrieve user information and email data, resulting in HTTP errors that interrupt the authentication flow.",
    "tags": ["External System", "Authentication", "GitHub", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9791158371115763,
    "cluster_avg_similarity": 0.9791158371115763
  },
  {
    "cluster_id": 852,
    "project_ids": [],
    "group_ids": [7024062529, 7024066882],
    "issue_titles": [
      "HTTPError: 503 Server Error: Service Unavailable for url: https://app.vssps.visualstudio.com/_apis/profile/profiles/me?api-version=1.0"
    ],
    "title": "VSTS integration setup fails on user info retrieval",
    "description": "HTTP error occurs when calling VSTS API to retrieve user information during integration pipeline setup, causing the integration configuration to fail.",
    "tags": ["External System", "API", "VSTS", "HTTP Error"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.9751132657719532,
    "cluster_avg_similarity": 0.9751132657719532
  },
  {
    "cluster_id": 853,
    "project_ids": [],
    "group_ids": [7024265714, 7024599475],
    "issue_titles": [
      "ApiRateLimitedError: <!doctype html><html lang=\"en\"><head><title>HTTP Status 429  Too Many Requests</title><style type=\"text/css\">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 ...",
      "ApiRateLimitedError"
    ],
    "title": "HTTP errors in Sentry integration requests",
    "description": "Integration API calls are failing with HTTP errors, affecting both group integration management and MS Teams webhook processing.",
    "tags": ["External System", "API", "HTTP Error", "Integration"],
    "cluster_size": 2,
    "cluster_min_similarity": 0.959620424051773,
    "cluster_avg_similarity": 0.959620424051773
  }
]
